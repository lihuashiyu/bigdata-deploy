## 1. 编程语言安装

Debian/Ubuntu系列（包括：Ubuntu 20.04/22.04、Debian 11/12）使用**apt**作为包管理工具，RedHat 8/9 系列（包括：RdHat 8/9、CentoS 8/Stream、Rocky 8/9，AlmaLinux 8/9）使用**dnf**作为包管理工具，RedHat 7 系列（RdHat/CentoS 7）使用**yum**作为包管理工具，以下以 Rocky 9 为例，其它系统，需作出相应改变；

### 1.1 安装 JDK 并配置环境变量

#### 1.1.1 卸载系统自带的 open-jdk

```bash
    # ubuntu 卸载系统自带 open-jdk
    sudo apt list --installed | grep openjdk                                   # 查看安装的 openjdk 
    sudo apt remove openjdk*                                                   # 直接卸载
    sudo apt --purge remove                                                    # 卸载并且移除相关依赖
    
    # redhat 卸载系统自带 open-jdk
    rpm -qa | grep java                                                        # 查看安装的 java，或者： dnf list installed |grep java 
    rpm -qa | grep jdk                                                         # 查看安装的 jdk， 或者： dnf list installed |grep jdk
    rpm -e --nodeps java-*                                                     # 卸载 java-*，    或者： dnf -y remove java-*
    rpm -e --nodeps jdk-*                                                      # 卸载 jdk-*，     或者： dnf -y remove jdk-*
```

#### 1.1.2 JDK 下载

  从 [**Oracle 官网**](https://www.oracle.com/java/technologies/downloads/#java8) 下载 [**JDK-1.8**](https://download.oracle.com/otn/java/jdk/8u361-b09/0ae14417abb444ebb02b9815e2103550/jdk-8u361-linux-x64.tar.gz) 到本地

#### 1.1.3 解压安装

```bash
    tar -zxvf jdk-8u361-linux-x64.tar.gz -C /opt/java/                         # 解压下载的 jdk-1.8 压缩包
    cd /opt/java/                                                              # 切换到解压目录
    mv jdk1.8.0_361/ jdk-08/                                                   # 修改目录名称
```

#### 1.1.4 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用：sudo vim /etc/profile 
    
    # 添加如下内容：
        # ====================================== JDK-1.8.361 ====================================== #
        export JAVA_HOME=/opt/java/jdk-08
        export JRE_HOME=${JAVA_HOME}/jre
        export CLASSPATH=.:${CLASSPATH}:${JAVA_HOME}/lib:${JRE_HOME}/lib
        export PATH=${PATH}:${JAVA_HOME}/bin:${JRE_HOME}/bin
    
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

#### 1.1.5 测试安装

```bash
    # 刷新环境变量
    source /etc/profile                                                        # 或者： . /etc/profile
    
    # 验证安装结果
    java  -version
    javac -version
```

### 1.2 安装 Scala 并配置环境变量

#### 1.2.1 下载 scala-2.12.18 

  从 [**scala 官网**](https://scala-lang.org/) 下载 [**scala-2.12.18**](https://downloads.lightbend.com/scala/2.12.18/scala-2.12.18.tgz) 压缩包 到本地

#### 1.2.2 解压安装

```bash
    tar -zxvf scala-2.12.18.tgz -C /opt/java/                                  # 解压下载的 scala-2.12.18 压缩包
    cd /opt/java/                                                              # 切换到解压目录
    mv /opt/java/scala-2.12.18 /opt/java/scala-212/                            # 修改目录名称
```

#### 1.2.3 配置环境变量

```bash
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用：sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Scala 2.12.18 ====================================== #
        export SCALA_HOME=/opt/java/scala-212/
        export PATH=${PATH}:${SCALA_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

#### 1.2.4 验证安装

```bash
    scala -version                                                             # 查看安装的版本
    scala                                                                      # 进入交互界面
    println("hello world")                                                     # 打印数据
```

 <br>

## 2. 数据库安装


### 2.1 安装 Mysql 并配置环境变量

#### 2.1.1 卸载系统自带的 MariaDB

```bash
    # ubuntu 系列卸载系统自带 MariaDB
    sudo apt list --installed | grep -iE "maria|mysql"                         # 查看安装的 MariaDB 
    sudo apt purge maria*                                                      # 卸载 mariadb
    sudo apt remove maria*                                                     # 直接卸载 mariadb
    sudo apt purge mysql*                                                      # 卸载 mysql
    sudo apt remove mysql*                                                     # 直接卸载 mysql
    sudo dpkg -l | grep ^rc | awk '{print $2}' | sudo xargs dpkg -P            # 清理残留的数据
    sudo apt --purge remove                                                    # 卸载并且移除相关依赖
    sudo rm -rf /etc/mysql/ /var/lib/mysql                                     # 删除配置文件
    sudo apt autoremove                                                        # 自动移除残留
    sudo apt autoclean                                                         # 自动清理残留
    
    # redhat 系列卸载系统自带 MariaDB
    sudo rpm -qa | grep maria                                                  # 查看安装的 MariaDB，或者： dnf list installed | grep maria 
    sudo rpm -qa | grep mysql                                                  # 查看安装的 myusql， 或者： dnf list installed | grep mysql
    sudo rpm -e --nodeps maria-*                                               # 卸载 MariaDB，      或者： dnf -y remove maria*
    sudo rpm -e --nodeps mysql-*                                               # 卸载 Mysql，        或者： dnf -y remove mysql*
    sudo rm -rf /etc/mysql/ /etc/my.cnf /var/lib/mysql                         # 删除配置文件
    sudo rm -rf /etc/mysql/ /var/lib/mysql                                     # 删除配置文件
```

#### 2.1.2 Mysql 下载与安装

从 [**Mysql 官网**](https://www.mysql.com/) 下载 **[mysql-8.0.32](https://downloads.mysql.com/archives/get/p/23/file/mysql-8.0.32-linux-glibc2.17-x86_64-minimal.tar.xz)** 到本地

#### 2.1.3 解压安装

```bash
    sudo apt -y install libaio* libaio-dev*                                    # ubuntu 系列安装必要的系统依赖包
    
    tar -Jxvf mysql-8.0.32-linux-glibc2.12-x86_64.tar.xz -C /opt/db/           # 解压下载的 mysql-8.0.32 压缩包
    cd /opt/db/                                                                # 切换到解压目录
    mv /opt/db/mysql-8.0.32-linux-glibc2.12-x86_64/ /opt/db/mysql/             # 修改目录名称
    mkdir -p /opt/db/mysql/data/                                               # 创建 Mysql 数据存储目录
    mkdir -p /opt/db/mysql/bin-log/                                            # 创建 Mysql bin-log 数据存储目录
    mkdir -p /opt/db/mysql/tmp/                                                # 创建 Mysql 临时文件目录
    mkdir -p /opt/db/mysql/logs/                                               # 创建 Mysql 日志存储目录
    mkdir -p /etc/mysql/                                                       # 创建 Mysql 配置文件目录
    
    touch /etc/mysql/my.cnf                                                    # 创建 Mysql 配置文件，内容如 1.6.4
    cp /etc/mysql/my.cnf /opt/db/mysql/docs/my.cnf                             # 备份配置文件
    sudo chown issac:issac -R /opt/db/mysql/                                   # 将安装目录 mysql 授权给 当前用户
    chmod -R 771 /opt/db/mysql/data/                                           # 修改 数据存储目录 授权
    chmod -R 771 /opt/db/mysql/bin-log/                                        # 修改 bin-log 数据存储目录 授权
    chmod -R 777 /opt/db/mysql/tmp/                                            # 修改 临时文件目录 授权
     
    # 解决 libtinfo.so.5 动态链接库缺失问题
    sudo ln -s /usr/lib/x86_64-linux-gnu/libtinfo.so.6.2 /usr/lib/x86_64-linux-gnu/libtinfo.so.5   # ubuntu
    
    # 解决 libtinfo.so.5 动态链接库缺失问题
    sudo ln -s /usr/lib64/libtinfo.so.6.2 /usr/lib64/libtinfo.so.5             # redhat 
```

#### 2.1.4 添加环境变量

```bash
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用：sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== mysql-8.0.32 ====================================== #
        export MYSQL_HOME=/opt/db/mysql
        export PATH=${PATH}:${MYSQL_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

#### 2.1.5 修改配置文件（redhat：/etc/my.cnf；ubuntu：/etc/mysql/my.cnf）

```ini
    [client]
    port                               = 3306
    socket                             = /opt/db/mysql/tmp/mysql.sock
    default-character-set              = utf8mb4
    
    
    [mysql]
    show-warnings
    default-character-set = utf8mb4
    socket                             = /opt/db/mysql/tmp/mysql.sock
    
    
    [mysqldump]
    quick
    quote-names
    max_allowed_packet                 = 16M
    
    
    [mysqld]
    bind-address                       = 0.0.0.0
    port                               = 3306
    
    # Mysql服务的唯一编号 每个 mysql 服务 Id 需唯一
    server-id                          = 1
    
    basedir                            = /opt/db/mysql
    datadir                            = /opt/db/mysql/data
    pid-file                           = /opt/db/mysql/tmp/mysqld.pid
    socket                             = /opt/db/mysql/tmp/mysql.sock
    # 临时目录 比如 load data infile 会用到
    tmpdir                             = /opt/db/mysql/tmp
    log-error                          = /opt/db/mysql/logs/error.log
    
    character-set-server               = utf8mb4
    skip_name_resolve                  = 1
    
    lock_wait_timeout                  = 3600
    open_files_limit                   = 65535
    back_log                           = 1024
    max_connections                    = 512
    max_connect_errors                 = 1000000
    table_open_cache                   = 1024
    table_definition_cache             = 1024
    thread_stack                       = 512K
    sort_buffer_size                   = 32M
    join_buffer_size                   = 64M
    read_buffer_size                   = 128M
    read_rnd_buffer_size               = 16M
    bulk_insert_buffer_size            = 128M
    thread_cache_size                  = 768
    interactive_timeout                = 600
    wait_timeout                       = 600
    tmp_table_size                     = 64M
    max_heap_table_size                = 32M
    # query_cache_size                 = 0
    
    key_buffer_size                    = 32M
    myisam_sort_buffer_size            = 128M
    
    default-storage-engine             = INNODB
    innodb_buffer_pool_size            = 512M
    innodb_buffer_pool_instances       = 4
    # innodb_data_file_path            = ibdata1:12M:autoextend
    innodb_flush_log_at_trx_commit     = 1
    innodb_log_buffer_size             = 32M
    innodb_log_file_size               = 256M
    innodb_log_files_in_group          = 3
    innodb_max_undo_log_size           = 1G
    innodb_io_capacity                 = 400
    innodb_io_capacity_max             = 800
    innodb_open_files                  = 65535
    innodb_flush_method                = O_DIRECT
    innodb_lru_scan_depth              = 4000
    innodb_lock_wait_timeout           = 10
    innodb_rollback_on_timeout         = 1
    innodb_print_all_deadlocks         = 1
    innodb_online_alter_log_max_size   = 4G
    innodb_status_file                 = 1
    innodb_status_output               = 0
    innodb_status_output_locks         = 1
    innodb_sort_buffer_size            = 67108864
    innodb_adaptive_hash_index         = OFF
    
    # log_error_verbosity              = 3
    # slow_query_log                   = 1
    slow_query_log_file                = /opt/db/mysql/logs/slow.log
    # long_query_time                  = 0.1
    # log_queries_not_using_indexes    = 1
    # log_throttle_queries_not_using_indexes = 60
    min_examined_row_limit             = 100
    log_slow_admin_statements          = 1
    log_slow_slave_statements          = 1
    log-bin                            = /opt/db/mysql/bin-log/mysql
    binlog_format                      = ROW
    sync_binlog                        = 1
    binlog_cache_size                  = 16M
    max_binlog_cache_size              = 2G     
    max_binlog_size                    = 1G
    binlog_rows_query_log_events       = 1
    binlog_checksum                    = CRC32
    gtid_mode                          = ON
    enforce_gtid_consistency           = TRUE
    # 大小写不敏感
    lower_case_table_names             = 1
```

#### 2.1.6 修改 ${MYSQL_HOME}/support-files/mysql.server

```bash
    # 1. 将脚本中的变量 basedir 的值统一进行修改为：/opt/db/mysql；
    # 2. 将脚本中的变量 datadir 的值统一进行修改为：/opt/db/mysql/data
    # 3. 将脚本中的变量 bindir  的值统一进行修改为：/opt/db/mysql/bin
    # 4. 将脚本中的变量 sbindir 的值统一进行修改为：/opt/db/mysql/bin
    # 5. 将脚本中的变量 libexecdir 的值统一进行修改为：/opt/db/mysql/bin
    
    # 修改
    sed -i 's////g'
        :s / \/usr/local / \/opt/db / g
```

#### 2.1.7 修改 ${MYSQL_HOME}/support-files/mysqld_multi.server

```bash

    # 1. 将脚本中的变量 basedir 的值统一进行修改为：/opt/db/mysql
    # 2. 将脚本中的变量 bindir  的值统一进行修改为：/opt/db/mysql/bin
    
    # vim 在命令模式下执行如下操作
        :s / \/usr/local / \/opt/db / g
```

#### 2.1.8 编写 ${MYSQL_HOME}/bin/mysql.sh 启停脚本

```bash
    # 详见 shell 文件夹下：/bigdata-deploy/shell/database/mysql.sh
```

#### 2.1.9 初始化 Mysql 

```bash
    # 切换到 Mysql 安装路径
    cd /opt/db/mysql/ || exit
    
    # 初始化 Mysql 并查看临时密码
    nohup ${MYSQL_HOME}/bin/mysqld --initialize --console > ${MYSQL_HOME}/logs/init.log 2>&1
    grep -ni "password" ${MYSQL_HOME}/logs/init.log  
```

#### 2.1.10 启动并测试安装

```bash
    # 启动 Mysql 服务
    ${MYSQL_HOME}/support-files/mysql.server start                             # Mysql 自带脚本启动服务
    ${MYSQL_HOME}/bin/mysql.sh start                                           # 自定义脚本启动 Mysql 
    
    # 查看 Mysql 启动状况
    ${MYSQL_HOME}/support-files/mysql.server status                            # Mysql 自带脚本启动服务
    ${MYSQL_HOME}/bin/mysql.sh status                                          # 自定义脚本查询 Mysql 
    netstat -tunlp | grep 3306                                                 # 查看 Mysql 进程占用端口
    
    # 安装 Mysql 客户端
    sudo pip install mycli                                                     # 基于 Python 的 客户端
    
    # 登录 Mysql
    ${MYSQL_HOME}/bin/mysql -h master -P 3306 -u root -p Y2>yhAy>E%uV -D mysql  # Mysql 自带客户端连接 Mysql 服务
    mycli -h master -P 3306 -u root -p Y2>yhAy>E%uV -D mysql                    # 使用临时密码进行登录
```

#### 2.1.11 后续处理

```mysql
    # 修改密码策略
    set global validate_password.policy=LOW;                                   # 修改密码复杂度
    set global validate_password.length=6;                                     # 修改密码长度
    
    # 修改密码并允许远程登录
    alter user 'root'@'localhost' identified by '111111';                      # 修改密码为六个 1
    update mysql.user set host = '%' where user = 'root';                      # 使 root 能在任何 host 访问
    flush privileges;                                                          # 刷新权限，使得修改生效
    
    # 退出登录，重新登录
    quit;                                                                      # 退出登录
    mycli -h master -P 3306 -u root -p 111111 -D mysql                         # 使用修改后的密码进行登录
    
    # 创建 数据库
    create database if not exists other;                                       # 创建 other 数据库
    create database if not exists test;                                        # 创建 test  数据库
    create database if not exists hive;                                        # 创建 hive  数据库
    create database if not exists kylin;                                       # 创建 kylin 数据库
    
    # 创建 用户
    create user if not exists 'issac'@'%' identified by '111111';              # 创建 issac 用户，密码六个 1
    
    # 将创建的 数据库，权限授权给 创建的用户
    grant all privileges on other.* to 'issac'@'%';                            # 将数据库 other 的所有授权给用户 issac
    grant all privileges on test.*  to 'issac'@'%';                            # 将数据库 test  的所有授权给用户 issac
    grant all privileges on hive.*  to 'issac'@'%';                            # 将数据库 hive  的所有授权给用户 issac
    grant all privileges on kylin.* to 'issac'@'%';                            # 将数据库 kylin 的所有授权给用户 issac
    flush privileges;                                                          # 刷新权限
    
    # 退出登录，使用创建的 issac 用户重新登录，并测试
    quit;                                                                      # 退出登录，或者：\q;
    mycli -h master -P 3306 -u issac -p 111111 -D test                         # 使用修改后的密码进行登录
    show tables;                                                               # 查看 test 下的所有表
    create table if not exists test                                            # 创建 test 表
    (
        id   int          primary key,
        name varchar(64)  not null    default '',
        mark varchar(255) not null    default '未知' 
    ) engine = InnoDB;
    show create table test;
    insert into test (id, name, mark) values (101, 'issac', 'qazwsx');
    select * from test;
    quit;
```

<br>

## 3. 基础组件安装

### 3.1 安装 Maven 并配置环境变量

#### 3.1.1 Maven 下载

  从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[maven-3.6.3](https://mirrors.aliyun.com/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz)** 到本地

#### 3.1.2 解压安装

```bash
    tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/apache/                    # 解压下载的 maven-3.6.3 压缩包
    cd /opt/apache/                                                            # 切换到解压目录
    mv apache-maven-3.6.3/ maven/                                              # 修改目录名称
```

#### 3.1.3 修改 ${MAVEN_HOME}/conf/settings.xml 配置文件

```xml 
    <localRepository>/opt/apache/maven/data</localRepository>
    
    <pluginGroups></pluginGroups>
    <proxies></proxies>
    <servers></servers>
    
    <mirrors>
        <!--阿里云-->
        <mirror>
            <id>alimaven</id>
            <mirrorOf>*</mirrorOf>
            <name>阿里云公共仓库</name>
            <url>https://maven.aliyun.com/repository/public</url>
        </mirror>
    </mirrors>
    
    <repositories>
        <repository>
            <id>spring</id>
            <url>https://maven.aliyun.com/repository/spring</url>
            <releases><enabled>true</enabled></releases>
            <snapshots><enabled>true</enabled></snapshots>
        </repository>
    </repositories>
    
    <profiles>
        <profile>
            <id>jdk8</id>
            <activation>
                <activeByDefault>true</activeByDefault>
                <jdk>1.8</jdk>
            </activation>
            <properties>
                <maven.compiler.source>1.8</maven.compiler.source>
                <maven.compiler.target>1.8</maven.compiler.target>
                <maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion>
            </properties>
        </profile>
    </profiles>
```

#### 3.1.4 配置环境变量

```bash
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Maven 3.6.3 ====================================== #
        export MAVEN_HOME=/opt/apache/maven
        export PATH=${PATH}:${MAVEN_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

#### 3.1.5 测试安装

```bash
    mvn -v
```

### 3.2 安装 Gradle 并配置环境变量

#### 3.2.1 Gradle 下载
https://services.gradle.org/distributions/gradle-7.6.2-all.zip

从 [**gradle 官网**](https://gradle.org/) 下载 **[gradle-7.6.2](https://downloads.gradle.org/distributions/gradle-7.6.2-all.zip)** 到本地

#### 3.2.2 解压安装

```bash
    unzip gradle-7.6.2-all.zip -d /opt/apache/                                 # 解压下载的 gradle-7.6.2 压缩包
    cd /opt/apache/                                                            # 切换到解压目录
    mv /opt/apache/gradle-7.6.2/ /opt/apache/gradle/                           # 修改目录名称
```

#### 3.2.3 配置环境变量

```bash
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Gradle 7.6.2 ====================================== #
        export GRADLE_HOME=/opt/apache/gradle
        export PATH=${PATH}:${GRADLE_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

### 3.2.4 修改配置文件

```bash
    cd /opt/apache/gradle/                                                     # 切换到 gradle 安装目录
    touch /opt/apache/gradle/gradle.properties                                 # 创建 gradle 配置文件，内容如 3.3.5
    touch /opt/apache/gradle/init.gradle                                       # 配置 gradle 为 阿里镜像源，内容如 3.3.6
    cp /opt/apache/gradle/init.gradle /opt/apache/gradle/init.d/init.gradle    # 配置 gradle 为 阿里镜像源
```

#### 3.2.5 ${GRADLE_HOME}/gradle.properties

```properties
    systemProp.org.gradle.internal.http.socketTimeout=360000
    systemProp.org.gradle.internal.http.connectionTimeout=360000
    
    # 开启线程守护，第一次编译时开线程，之后就不会再开了
    org.gradle.daemon=true
    
    # 配置编译时的虚拟机大小
    org.gradle.jvmargs=-Xmx2048m -XX:MaxPermSize=512m -XX:+HeapDumpOnOutOfMemoryError -Dfile.encoding=UTF-8
    
    # 开启并行编译，相当于多条线程再走
    org.gradle.parallel=true
    
    # 启用新的孵化模式
    org.gradle.configureondemand=true
```

#### 3.2.6 ${GRADLE_HOME}/init.gradle

```groovy
    allprojects {
        repositories {
            def ALIYUN_PUBLIC_URL = 'https://maven.aliyun.com/repository/public/'
            def ALIYUN_REPOSITORY_URL = 'https://maven.aliyun.com/nexus/content/groups/public'
            def ALIYUN_JCENTER_URL = 'https://maven.aliyun.com/nexus/content/repositories/jcenter'
            all { ArtifactRepository repo ->
                if(repo instanceof MavenArtifactRepository){
                    def url = repo.url.toString()
                    if (url.startsWith('https://repo1.maven.org/maven2')) {
                        project.logger.lifecycle "Repository ${repo.url} replaced by $ALIYUN_REPOSITORY_URL."
                        remove repo
                    }
                    if (url.startsWith('https://jcenter.bintray.com/')) {
                        project.logger.lifecycle "Repository ${repo.url} replaced by $ALIYUN_JCENTER_URL."
                        remove repo
                    }
                }
            }
            
            maven {
                url ALIYUN_PUBLIC_URL
                url ALIYUN_REPOSITORY_URL
                url ALIYUN_JCENTER_URL
            }
            
            mavenLocal()
            mavenCentral()
        }
    }
```

#### 3.2.7 测试安装

```bash
    gradle -v
```

<br>

## 4. Hadoop 安装配置

### 4.1 配置 SSH 免密登录

```bash
    # 安装 ssh 
    sudo apt install openssh-server                                            # ubuntu 系列安装 ssh
    sudo dnf install openssh-server                                            # redhat 系列安装 ssh
    
    # 配置免密登录
    ssh-keygen -t rsa                                                          # 每个节点上，生成秘钥（连续回车 3 次）
    
    # 传输公钥
    ssh-copy-id issac@master                                                   # 在每个 slaver 节点上，将公钥复制到 master
    scp ~/.ssh/id_rsa.pub issac@master:~/.ssh/id_rsa.pub.slaver*               # 在每个 slaver 节点上，将各自的 id_rsa.pub 发给 master 节点
    
    # 合成公钥，并分发
    cat ~/.ssh/id_rsa.pub* >> ~/.ssh/authorized_keys                           # 在 master 上，将所有公钥加到用于认证的公钥文件 authorized_keys 中
    scp ~/.ssh/authorized_keys issac@slaver*:~/.ssh/                           # 将 master 上，将文件 authorized_keys 分发给每台 slaver
    
    # 修改权限
    chmod 600 ~/.ssh/authorized_keys                                           # 在所有节点上，修改每个主机 authorized_keys 的权限
    
    # 验证免密登录
    ssh slaver*                                                                # 免密登录到其它节点
```

### 4.2 下载 Hadoop-3.2.4

  从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[hadoop-3.2.4](https://mirrors.aliyun.com/apache/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz)** 到本地

### 4.3 解压安装

```bash
    tar -zxvf hadoop-3.2.4.tar.gz -C /opt/apache/                              # 解压下载的 hadoop-3.2.4 压缩包
    cd /opt/apache/                                                            # 切换到解压目录
    mv hadoop-3.2.4/ hadoop/                                                   # 修改目录名称
```

### 4.4 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Hadoop 3.2.4 ====================================== #
        export HADOOP_HOME=/opt/apache/hadoop
        export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
        # export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/*:${HADOOP_HOME}/share/hadoop/common/*:${HADOOP_HOME}/share/hadoop/hdfs:${HADOOP_HOME}/share/hadoop/hdfs/lib/*:${HADOOP_HOME}/share/hadoop/hdfs/*:${HADOOP_HOME}/share/hadoop/mapreduce/lib/*:${HADOOP_HOME}/share/hadoop/mapreduce/*:${HADOOP_HOME}/share/hadoop/yarn:${HADOOP_HOME}/share/hadoop/yarn/lib/*:${HADOOP_HOME}/share/hadoop/yarn/*
        export HADOOP_CLASSPATH=$(hadoop classpath)
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
    
    # 验证安装
    hadoop version
```

### 4.5 修改配置文件（${HADOOP_HOME}/etc/hadoop/）

#### 4.5.1 编辑 ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh 添加以下内容

```bash
    export JAVA_HOME=/opt/java/jdk-08
    export HADOOP_HOME=/opt/apache/hadoop
    # export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
```

#### 4.5.2 编辑 ${HADOOP_HOME}/etc/hadoop/yarn-env.sh 添加以下内容

```bash
    export JAVA_HOME=/opt/java/jdk-08
```

#### 4.5.3 编辑 ${HADOOP_HOME}/etc/hadoop/core-site.xml 添加以下内容

```xml
    <configuration>
        <!-- 访问 HDFS 时的 host 和 port -->
        <property>
            <name>fs.default.name</name>
            <value>hdfs://master:9000</value>
        </property>
        <!-- Hadoop 的临时文件夹根 -->
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/opt/apache/hadoop/data/tmp</value>
        </property>
        <!-- 缓冲区大小，根据服务器性能动态调整 -->
        <property>
            <name>io.file.buffer.size</name>
            <value>4096</value>
        </property>
        <!--  开启 HDFS 的垃圾桶机制，删除掉的数据可以从垃圾桶中回收，单位分钟 -->
        <property>
            <name>fs.trash.interval</name>
            <value>10080</value>
        </property>
        <!-- 开启 HDFS 支持压缩 -->
        <property>
            <name>io.compression.codecs</name>
            <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value>
        </property>
        <!-- 开启 Map 阶段文件压缩 -->
        <property>
            <name>mapreduce.map.output.compress</name>
            <value>true</value>
        </property>
        <!-- 设置 Map 阶段文件压缩编码 -->
        <property>
            <name>mapreduce.map.output.compress.codec</name>
            <value>org.apache.hadoop.io.compress.GzipCodec</value>
        </property>
        <!-- 开启 MapReduce 输出文件压缩 -->
        <property>
            <name>mapreduce.output.fileoutputformat.compress</name>
            <value>true</value>
        </property>
        <!-- 设置 MapReduce 输出文件压缩编码 -->
        <property>
            <name>mapreduce.output.fileoutputformat.compress.codec</name>
            <value>org.apache.hadoop.io.compress.GzipCodec</value>
        </property>
        
        <!-- 设置 root 账户在 Web 页面登录的代理 -->
        <property>
            <name>hadoop.proxyuser.root.hosts</name>
            <value>*</value>
        </property>
        <property>
            <name>hadoop.proxyuser.root.groups</name>
            <value>*</value>
        </property>
        
        <!-- 设置 issac 账户在 Web 页面登录的代理 -->
        <property>
            <name>hadoop.proxyuser.issac.hosts</name>
            <value>*</value>
        </property>
        <property>
            <name>hadoop.proxyuser.issac.groups</name>
            <value>*</value>
        </property>
        
        <!-- 高可用时，NameNode 访问 ZK 的地址 -->
        <property>
            <name>ha.zookeeper.quorum</name>
            <value>slaver1:2181,slaver2:2181,slaver3:2181</value>
        </property>
    </configuration>
```

#### 4.5.4 编辑 ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml 添加以下内容

```xml
    <configuration>
        <!-- 集群动态上下线 -->
        <!--
        <property>
            <name>dfs.hosts</name>
            <value>/opt/apache/hadoop/etc/hadoop/accept-host</value>
        </property>
        <property>
            <name>dfs.hosts.exclude</name>
            <value>/opt/apache/hadoop/etc/hadoop/deny-host</value>
        </property>
        -->
        <!-- 配置 NameNode 的存放位置 -->
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>/opt/apache/hadoop/data/namenode</value>
        </property>
        <!-- 定义 DataNode 数据存储的节点位置，一般先确定磁盘的挂载目录，然后多个目录用，进行分割 -->
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>/opt/apache/hadoop/data/datanode</value>
        </property>
        <!-- 配置事务编辑日志 Edits 的存储位置 -->
        <property>
            <name>dfs.namenode.edits.dir</name>
            <value>/opt/apache/hadoop/data/edits</value>
        </property>
        <!-- 配置 2NN 临时镜像存储位置 -->
        <property>
            <name>dfs.namenode.checkpoint.dir</name>
            <value>/opt/apache/hadoop/data/checkpoint</value>
        </property>
        <!-- 配置 2NN 临时编辑日志存储位置 -->
        <property>
            <name>dfs.namenode.checkpoint.edits.dir</name>
            <value>/opt/apache/hadoop/data/editpoint</value>
        </property>
	    <!-- 在 standby 和 2NN 的检查点期间，保存编辑日志命名空间的存储位置 -->
	    <!--
	    <property>
		    <name>dfs.namenode.legacy-oiv-image.dir</name>
		    <value>/opt/apache/hadoop/data/oiv</value>
	    </property>
	    -->
	    <!-- HA 中，多个 NameNode 共享的目录 -->
        <!--
		<property>
			<name>dfs.namenode.shared.edits.dir</name>
			<value>/opt/apache/hadoop/data/share</value>
		</property>	    
		-->
	    
		<!-- 副本数量 -->
        <property>
            <name>dfs.replication</name>
            <value>3</value>
        </property>
        
        <!-- NameNode 有一个工作线程池，默认值是 10 -->
        <property>
            <name>dfs.namenode.handler.count</name>
            <value>10</value>
        </property>
        <!-- 2NN 的访问路径和端口号 -->
        <property>
            <name>dfs.namenode.secondary.http-address</name>
            <value>0.0.0.0:9860</value>
        </property>
        <!-- NameNode 的外部访问路径和端口号 -->
        <property>
            <name>dfs.namenode.http-address</name>
            <value>0.0.0.0:9870</value>
        </property>
        <!-- 关闭 HDFS 的验证权限 -->
        <property>
            <name>dfs.permissions</name>
            <value>false</value>
        </property>
        <!-- HDFS 存储块的大小，4M -->
        <property>
            <name>dfs.blocksize</name>
            <value>4194304</value>
        </property>
        <!-- HDFS NameNode 最小块限制，4M -->
        <property>
            <name>dfs.namenode.fs-limits.min-block-size</name>
            <value>4194304</value>
        </property>
        <!-- 开启 HDFS WEB UI -->
        <property>
            <name>dfs.webhdfs.enabled</name>
            <value>true</value>
        </property>
        <!-- Zookeeper -->
        <property>
            <name>ha.zookeeper.quorum</name>
            <value>slaver1:2181,slaver2:2181,slaver3:2181</value>
        </property>
    </configuration>
```

#### 4.5.5 编辑 ${HADOOP_HOME}/etc/hadoop/mapred-site.xml 添加以下内容

```xml
    <configuration>
        <!-- 配置 MapReduce 在 Yarn 集群上运行(默认本地运行) -->
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
        
        <!-- 开启 MapReduce 的小任务模式：开启 uber 模式，使用 JVM 重用，默认关闭 -->
        <property>
            <name>mapreduce.job.ubertask.enable</name>
            <value>true</value>
        </property>
        
        <!-- Uber 模式中最大的 MapTask 数量，可向下修改 --> 
        <property>
            <name>mapreduce.job.ubertask.maxmaps</name>
            <value>8</value>
        </property>
        
        <!-- Uber 模式中最大的 Reduce 数量，可向下修改 -->
        <property>
            <name>mapreduce.job.ubertask.maxreduces</name>
            <value>8</value>
        </property>
        
        <!-- Uber 模式中最大的输入数据量，默认使用 dfs.blocksize 的值，可向下修改 -->
        <property>
            <name>mapreduce.job.ubertask.maxbytes</name>
            <value>4194304</value>
        </property>
        
        <!-- 环形缓冲区大小，默认 100M -->
        <property>
            <name>mapreduce.task.io.sort.mb</name>
            <value>100</value>
        </property>
        
        <!-- 环形缓冲区溢写阈值，默认 0.8 -->
        <property>
            <name>mapreduce.map.sort.spill.percent</name>
            <value>0.80</value>
        </property>
        
        <!-- Merge 合并次数，默认 10 个 -->
        <property>
            <name>mapreduce.task.io.sort.factor</name>
            <value>10</value>
        </property>
        
        <!-- MapTask 内存，默认 1g； MapTask 堆内存大小默认和该值大小一致 mapreduce.map.java.opts -->
        <property>
            <name>mapreduce.map.memory.mb</name>
            <value>-1</value>
        </property>
        
        <!-- MapTask 的 CPU 核数，默认 1 个 -->
        <property>
            <name>mapreduce.map.cpu.vcores</name>
            <value>1</value>
        </property>
        
        <!-- MapTask 异常重试次数，默认 4 次 -->
        <property>
            <name>mapreduce.map.maxattempts</name>
            <value>4</value>
        </property>
        
        <!-- 每个 Reduce 去 Map 中拉取数据的并行数，默认值是 5 -->
        <property>
            <name>mapreduce.reduce.shuffle.parallelcopies</name>
            <value>8</value>
        </property>
        
        <!-- Buffer 大小占 Reduce 可用内存的比例，默认值 0.7 -->
        <property>
            <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
            <value>0.70</value>
        </property>
        
        <!-- Buffer 中的数据达到多少比例开始写入磁盘，默认值 0.66 -->
        <property>
            <name>mapreduce.reduce.shuffle.merge.percent</name>
            <value>0.66</value>
        </property>
        
        <!-- ReduceTask 内存，默认 1g；ReduceTask 堆内存大小默认和该值大小一致 mapreduce.reduce.java.opts -->
        <property>
            <name>mapreduce.reduce.memory.mb</name>
            <value>-1</value>
        </property>
        
        <!-- ReduceTask 的 CPU 核数，默认 1 个 -->
        <property>
            <name>mapreduce.reduce.cpu.vcores</name>
            <value>1</value>
        </property>
        
        <!-- ReduceTask 失败重试次数，默认 4 次 -->
        <property>
            <name>mapreduce.reduce.maxattempts</name>
            <value>4</value>
        </property>
        
        <!-- 当 MapTask 完成的比例达到该值后才会为 ReduceTask 申请资源，默认是 0.05 -->
        <property>
            <name>mapreduce.job.reduce.slowstart.completedmaps</name>
            <value>0.05</value>
        </property>
        
        <!-- 如果程序在规定的默认 10 分钟内没有读到数据，将强制超时退出 -->
        <property>
            <name>mapreduce.task.timeout</name>
            <value>600000</value>
        </property>
        <!-- 配置 JobHistory 的访问路径和端口号，JobHistory 是执行完成的任务日志 -->
        <property>
            <name>mapreduce.jobhistory.address</name>
            <value>0.0.0.0:10020</value>
        </property>
        <!-- 配置 JobHistory 的浏览器访问路径和端口号 -->
        <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>0.0.0.0:19888</value>
        </property>
    </configuration>
```

#### 4.5.6 编辑 ${HADOOP_HOME}/etc/hadoop/yarn-site.xml 添加以下内容

```xml
    <configuration>
        <!-- 配置 ResourceManager 运行的机器地址 -->
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>master</value>
        </property>
        <!-- Yarn WEB UI 地址 -->
        <property>
            <name>yarn.resourcemanager.webapp.address</name>
            <value>0.0.0.0:8088</value>
        </property>
        <!-- 调度器地址 -->
        <property>
            <name>yarn.resourcemanager.scheduler.address</name>
            <value>master:8098</value>
        </property>
        <!-- 配置 NodeManager 上运行的附属服务为 shuffle：需要配置成 mapreduce_shfffle，才可运行 MapReduce 程序默认值 -->
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <!-- 每个节点可用最小内存, 单位 MB, 默认 1024 MB -->
        <property>
            <name>yarn.scheduler.minimum-allocation-mb</name>
            <value>512</value>
        </property>
        <!-- 每个节点可用内存, 单位 MB, 默认 8192 MB -->
        <property>
            <name>yarn.scheduler.maximum-allocation-mb</name>
            <value>8192</value>
        </property>
        <!-- 容器允许管理的物理内存大小，单位 MB， 默认 4096 MB -->
        <property>
            <name>yarn.nodemanager.resource.memory-mb</name>
            <value>12288</value>
        </property>
        <property>
            <name>yarn.nodemanager.vmem-pmem-ratio</name>
            <value>2.1</value>
        </property>
        <!-- 关闭 yarn 对物理内存的限制检查 -->
        <property>
            <name>yarn.nodemanager.pmem-check-enabled</name>
            <value>false</value>
        </property>
        <!-- 关闭 yarn 对虚拟内存的限制检查 -->
        <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
        </property>
        <!-- 可使用的 CPU Core -->
        <property>
          <name>yarn.nodemanager.resource.cpu-vcores</name>
          <value>4</value>
        </property>
        
        <!-- 开启日志聚集功能 -->
        <property>      
            <name>yarn.log-aggregation-enable</name>
            <value>true</value>
        </property>
        <!-- 设置日志聚集服务器地址 -->
        <property>
            <name>yarn.log.server.url</name>
            <value>http://0.0.0.0:19888/jobhistory/logs</value>
        </property>
        <!-- 配置聚合日志保留时间为 7 天 -->
        <property>
            <name>yarn.log-aggregation.retain-seconds</name>
            <value>604800</value>
        </property>
        
        <!-- 每个 MapReduce 初始化堆大小 -->
        <property>
            <name>mapreduce.child.java.opts</name>
            <value>-Xmx512m</value>
        </property>
        <!-- Yarn 的 ClassPath -->
        <property>
            <name>yarn.application.classpath</name>
            <value>/opt/apache/hadoop/etc/hadoop:/opt/apache/hadoop/share/hadoop/common/lib/*:/opt/apache/hadoop/share/hadoop/common/*:/opt/apache/hadoop/share/hadoop/hdfs:/opt/apache/hadoop/share/hadoop/hdfs/lib/*:/opt/apache/hadoop/share/hadoop/hdfs/*:/opt/apache/hadoop/share/hadoop/mapreduce/lib/*:/opt/apache/hadoop/share/hadoop/mapreduce/lib-examples/*:/opt/apache/hadoop/share/hadoop/mapreduce/*:/opt/apache/hadoop/share/hadoop/yarn:/opt/apache/hadoop/share/hadoop/yarn/lib/*:/opt/apache/hadoop/share/hadoop/yarn/*:/opt/apache/hadoop/share/hadoop/yarn/timelineservice/*</value>
        </property>
        
        <!-- 配置使用公平调度器 -->
        <!-- 
        <property>
            <name>yarn.resourcemanager.scheduler.class</name>
            <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>       
        </property>
         -->
        <!-- 指明公平调度器队列分配配置文件 -->
        <!-- 
        <property>
            <name>yarn.scheduler.fair.allocation.file</name>
            <value>/opt/apache/hadoop/etc/hadoop/fair-scheduler.xml</value>
        </property>
         -->
        <!-- 禁止队列间资源抢占 -->
        <property>
            <name>yarn.scheduler.fair.preemption</name>
            <value>false</value>
        </property>
        <!-- ResourceManager 处理调度器请求的线程数量，默认 50；如果提交的任务数大于 50，可以增加该值 -->
        <property>
            <name>yarn.resourcemanager.scheduler.client.thread-count</name>
            <value>50</value>
        </property>
    </configuration>
```

#### 4.5.7 编辑 ${HADOOP_HOME}/etc/hadoop/workers 添加以下内容

```python
    slaver1
    slaver2
    slaver3
```

#### 4.5.8 编辑 ${HADOOP_HOME}/etc/hadoop/capacity-scheduler.xml 添加以下内容 

```xml
    <configuration>
        <property>
            <name>yarn.scheduler.capacity.maximum-applications</name>
            <value>10000</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
            <value>0.8</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.resource-calculator</name>
            <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
        </property>
        
        <!-- 指定多队列，增加 issac 队列 -->
        <property>
            <name>yarn.scheduler.capacity.root.queues</name>
            <value>default,issac</value>
        </property>
        
        <!-- 指定队列的资源额定容量：降低 default 队列资源额定容量为 50%，默认 100% -->
        <property>
            <name>yarn.scheduler.capacity.root.default.capacity</name>
            <value>50</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.issac.capacity</name>
            <value>50</value>
        </property>
        
        <!-- 用户最多可以使用队列多少资源，1 表示 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
            <value>1</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.issac.user-limit-factor</name>
            <value>1</value>
        </property>
        
        <!-- 指定队列的资源最大容量：降低 default 队列资源最大容量为 75%，默认 100% -->
        <property>
            <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
            <value>75</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.issac.maximum-capacity</name>
            <value>75</value>
        </property>
        
        <!-- 启动队列 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.state</name>
            <value>RUNNING</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.issac.state</name>
            <value>RUNNING</value>
        </property>
        
        <!-- 哪些用户有权向队列提交作业 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
            <value>*</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.issac.acl_submit_applications</name>
            <value>*</value>
        </property>
        
        <!-- 哪些用户有权操作队列，管理员权限（查看/杀死） -->
        <property>
            <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
            <value>*</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.issac.acl_administer_queue</name>
            <value>*</value>
        </property>
        
        <!-- 哪些用户有权配置提交任务优先级 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.acl_application_max_priority</name>
            <value>*</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.issac.acl_application_max_priority</name>
            <value>*</value>
        </property>
        
        <!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout -->
        <!-- 如果 application 指定了超时时间，则提交到该队列的 application 能够指定的最大超时时间不能超过该值 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.maximum-application-lifetime
            </name>
            <value>-1</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.issac.maximum-application-lifetime</name>
            <value>-1</value>
        </property>
        
        <!-- 如果 application 没指定超时时间，则用 default-application-lifetime 作为默认值 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.default-application-lifetime</name>
            <value>-1</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.issac.default-application-lifetime</name>
            <value>-1</value>
        </property>
        
        
        <property>
            <name>yarn.scheduler.capacity.node-locality-delay</name>
            <value>40</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.rack-locality-additional-delay</name>
            <value>-1</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.queue-mappings</name>
            <value></value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
            <value>false</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments</name>
            <value>1</value>
        </property>
        
        
        <property>
            <name>yarn.scheduler.capacity.application.fail-fast</name>
            <value>false</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.workflow-priority-mappings</name>
            <value></value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.workflow-priority-mappings-override.enable</name>
            <value>false</value>
        </property>
    </configuration>
```

#### 4.5.9 编辑 ${HADOOP_HOME}/etc/hadoop/fair-scheduler.xml 添加以下内容 capacity-scheduler

```xml
    <allocations>
        <!-- 单个队列中 Application Master 占用资源的最大比例,取值 0-1 ，企业一般配置 0.1 -->
        <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>
        
        <!-- 单个队列最大资源的默认值 issac default -->
        <queueMaxResourcesDefault>4096mb,4vcores</queueMaxResourcesDefault>
        
        <!-- 增加一个队列 test -->
        <queue name="test">
            <!-- 队列最小资源 -->
            <minResources>2048mb,2vcores</minResources>
            
            <!-- 队列最大资源 -->
            <maxResources>4096mb,4vcores</maxResources>
            
            <!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 -->
            <maxRunningApps>4</maxRunningApps>
            
            <!-- 队列中Application Master占用资源的最大比例 -->
            <maxAMShare>0.5</maxAMShare>
            
            <!-- 该队列资源权重,默认值为1.0 -->
            <weight>1.0</weight>
            
            <!-- 队列内部的资源分配策略 -->
            <schedulingPolicy>fair</schedulingPolicy>
        </queue>
        
        <!-- 增加一个队列    -->
        <queue name="issac" type="parent">
            <!-- 队列最小资源 -->
            <minResources>2048mb,2vcores</minResources>
            
            <!-- 队列最大资源 -->
            <maxResources>4096mb,4vcores</maxResources>
            
            <!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 -->
            <maxRunningApps>4</maxRunningApps>
            
            <!-- 队列中Application Master占用资源的最大比例 -->
            <maxAMShare>0.5</maxAMShare>
            
            <!-- 该队列资源权重,默认值为1.0 -->
            <weight>1.0</weight>
            
            <!-- 队列内部的资源分配策略 -->
            <schedulingPolicy>fair</schedulingPolicy>
        </queue>
        
        <!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 -->
        <queuePlacementPolicy>
            <!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false表示：如果指定队列不存在,不允许自动创建-->
            <rule name="specified" create="false"/>
            
            <!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 -->
            <rule name="nestedUserQueue" create="true">
                <rule name="primaryGroup" create="false"/>
            </rule>
            
            <!-- 最后一个规则必须为reject或者default。Reject表示拒绝创建提交失败，default表示把任务提交到default队列 -->
            <rule name="reject"/>
        </queuePlacementPolicy>
    </allocations>
```

#### 4.5.10 创建数据和日志存储路径

```bash
    cd /opt/apache/hadoop/                                                     # 切换到安装路径
    mkdir -p ${HADOOP_HOME}/data                                               # 创建 数据存储 目录
    mkdir -p ${HADOOP_HOME}/logs                                               # 创建 日志存储 目录
    
    cd /opt/apache/hadoop/data                                                 # 切换到数据存储路径
    mkdir -p ${HADOOP_HOME}/data/tmp                                           # 创建 临时存储 路径
    mkdir -p ${HADOOP_HOME}/data/namenode                                      # 创建 NameNode 数据存储路径
    mkdir -p ${HADOOP_HOME}/data/datanode                                      # 创建 DataNode 数据存储路径
    mkdir -p ${HADOOP_HOME}/data/edit                                          # 创建 编辑日志 存储路径
    mkdir -p ${HADOOP_HOME}/data/metapoint                                     # 创建 元数据信息检查点 存储路径
    mkdir -p ${HADOOP_HOME}/data/editpoint                                     # 创建 编辑日志检查点 存储路径
```

### 4.6 编写 hadoop 启停脚本

```bash
    cd /opt/apache/hadoop/bin/                                                 # 切换到 hadoop 安装目录的 bin
    touch ${HADOOP_HOME}/bin/hadoop.sh                                         # 脚本内容，详见：/bigdata-deploy/shell/apache/hadoop.sh
    chmod +x ${HADOOP_HOME}/bin/hadoop.sh                                      # 添加可执行权限
```

### 4.7 分发到其它节点

#### 4.7.1 安装同步命令

```bash
    # 安装同步命令
    sudo dnf install rsync                                                     # redhat
    sudo apt install rsync                                                     # ubuntu
    
    mkdir -p /home/issac/shell                                                 # 创建 shell 存放目录
    
    # 添加 shell 到环境变量（redhat：~/.bash_profile，ubuntu：~/.bashrc）
    echo "# ============================== issac-1.0.0 ============================== #" >> ~/.bash_profile  # 添加 shell 到环境变量（redhat）
    echo "export ISSAC_HOME=/home/issac" >> ~/.bash_profile                    # 创建 ISSAC_HOME 环境变量
    echo "PATH=${PATH}:${ISSAC_HOME}/shell" >> ~/.bash_profile                 # 添加 ISSAC_HOME 到环境变量
    source ~/.bash_profile                                                     # 使环境变量生效
```

#### 4.7.2 集群同步脚本 ${ISSAC_HOME}/shell/xync.sh 

```bash
    vim ${ISSAC_HOME}/shell/xync.sh                                            # 创建同步脚本，详见：/bigdata-deploy/shell/other/xync.sh
    chmod +x ${ISSAC_HOME}/shell/xync.sh                                       # 添加可执行权限
```

#### 4.7.3 查看集群命令 ${ISSAC_HOME}/shell/xcall.sh

```bash
    vim ${ISSAC_HOME}/shell/xync.sh                                            # 创建同步脚本，详见：/bigdata-deploy/shell/other/xcall.sh
    chmod +x ${ISSAC_HOME}/shell/xync.sh                                       # 添加可执行权限
```

#### 4.7.4 同步 Hadoop 安装路径  

```bash
    cd /opt/apache/                                                            # 切换到 hadoop 安装父路径
    ~/shell/xync.sh hadoop                                                     # 同步 hadoop 到其它节点
````

### 4.8 格式化 NameNode

```bash
    cd /opt/apache/hadoop/                                                     # 进入 hadoop 安装目录
    ${HADOOP_HOME}/bin/hadoop namenode -format > ${HADOOP_HOME}/logs/format.log 2>&1 &   # 格式化 namenode
    grep -ni "successfully formatted" ${HADOOP_HOME}/logs/format.log           # 查看格式化结果
```

### 2.8 启动集群并测试

```bash
    cd /opt/apache/hadoop/                                                     # 进入 hadoop 安装目录
    
    # 单独启动各个进程
    ${HADOOP_HOME}/sbin/hadoop-daemons.sh start namenode                       # 启动 NameNode 守护进程
    ${HADOOP_HOME}/sbin/hadoop-daemons.sh start datanode                       # 启动 DataNode 守护进程
    ${HADOOP_HOME}/sbin/hadoop-daemons.sh start SecondaryNameNode              # 启动 SecondaryNameNode 守护进程
    ${HADOOP_HOME}/sbin/yarn-daemon.sh start resourcemanager                   # 启动 ResourceManager 守护进程
    ${HADOOP_HOME}/sbin/yarn-daemon.sh start nodemanager                       # 启动 NodeManager 守护进程
    
    # 分别启动集群启动 hdfs yarn
    ${HADOOP_HOME}/sbin/start-dfs.sh                                           # 启动 hdfs 集群
    ${HADOOP_HOME}/sbin/start-yarn.sh                                          # 启动 yarn 集群
    
    # 全部启动 hdfs yarn
    ${HADOOP_HOME}/sbin/start-all.sh                                           # 启动 hdfs、yarn 集群
    
    # 历史服务器
    ${HADOOP_HOME}/sbin/mr-jobhistory-daemon.sh start historyserver            # 启动 HistoryServer 集群
    
    # 编写的脚本启动
    ${HADOOP_HOME}/bin/hadoop.sh start                                         # 启动 hdfs、yarn、HistoryServer 并验证启动结果
    
    jps -l                                                                     # 查看 hadoop 启动的 jvm 进程
    http://master:9870                                                         # 浏览器访问 NameNode
    http://master:9860                                                         # 浏览器访问 2NN
    http://master:8088/cluster                                                 # 浏览器访问 Yarn
    http://master:19888/jobhistory                                             # 浏览器访问 历史服务器
    
    # 计算 pi 
    ${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar pi 10 10
    
    # 计算
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /hadoop/test/wc/input               # 创建 hdfs 数据输入目录
    ${HADOOP_HOME}/bin/hadoop fs -ls /hadoop/test/wc/input                     # 查看创建的目录
    ${HADOOP_HOME}/bin/hadoop fs -put ${HADOOP_HOME}/*.txt /hadoop/test/wc/input    # 上传文件到 hdfs
    ${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar wordcount /hadoop/test/wc/input /hadoop/test/wc/output
    ${HADOOP_HOME}/bin/hadoop fs -cat /hadoop/test/wc/output/part-r-00000  # 查看统计结果
```



## 5. Spark 安装配置

### 5.1 下载 Spark-3.2.3 源码

  从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[spark-3.2.3 源码](https://mirrors.aliyun.com/apache/spark/spark-3.2.3/spark-3.2.3.tgz)** 和不含有 hadoop 的 [**spark-3.2.3**](https://mirrors.aliyun.com/apache/spark/spark-3.2.3/spark-3.2.3-bin-without-hadoop.tgz) 到本地

### 5.2 编译 Spark（spark-3.2.3 和 hadoop-3.2.4 在兼容性上存在问题，需要修改源码重新编译）

#### 5.2.1 解压 spark 源码

```bash
    sudo dnf install -y git                                                    # 安装 git 用于编译 spark 时执行补丁
    mkdir -p ~/coding/src/                                                     # 创建目录存放代码
    
    git clone https://github.com/apache/spark.git                              # 使用 git 下载源码
```

#### 5.2.2 修改源码前，编译

```bash
    cd ~/coding/java/src  || exit                                              # 进入 spark 源码目录
    git branch                                                                 # 查看本地分支
    git branch -r                                                              # 查看远程分支，-r 表示 remote
    git branch -a                                                              # 查看所有分支
    git checkout v3.2.3                                                        # 切换到 3.2.3 分支
    
    # 编译 spark-3.2.3
    ./dev/make-distribution.sh --name build --tgz -Phive-3.1 -Phive-thriftserver -Phadoop-3.2 -Phadoop-provided -Pyarn -Pscala-2.12 -Dhadoop.version=3.2.4 -DskipTests
```

#### 5.2.3 应用补丁文件，并进行编译

```bash
    cd ~/coding/java/src  || exit                                              # 进入 spark 源码目录
    touch issac-spark.patch                                                    # 创建 issac-spark 补丁，内容详见：/bigdata-deploy/patch/issac-spark.patch
    
    git apply --check ./issac-spark.patch                                      # 检查 patch 是否可用
    git apply ./issac-spark.patch                                              # 应用补丁，不包含 commit 内容
    git am ./issac-spark.patch                                                 # 应用补丁，包含 commit 内容
    
    # 再次编译
    ./dev/make-distribution.sh --name build --tgz -Phive-3.1 -Phive-thriftserver -Phadoop-3.2 -Phadoop-provided -Pyarn -Pscala-2.12 -Dhadoop.version=3.2.4 -DskipTests
```

### 5.3 解压安装 Spark

```bash
    cd ~/coding/java/src/spark/ || exit                                        # 切换到解压目录
    tar -zxf spark-3.2.3-bin-build.tgz -C /opt/apache/                         # 解压编译后的压缩包
    mv /opt/apache/spark-3.2.3-bin-build/ /opt/apache/spark                    # 修改目录名称
    mkdir -p /opt/apache/spark/logs/                                           # 创建日志目录
```

### 5.4 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Spark 3.2.3 ====================================== #
        export SPARK_HOME=/opt/apache/spark
        export PATH=$PATH:${SPARK_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 5.5 修改配置文件

#### 5.5.1 添加如下配置文件

```bash
    cd /opt/apache/spark/                                                      # 切换到 spark 安装目录
    touch ${SPARK_HOME}/conf/spark-env.sh
    touch ${SPARK_HOME}/conf/spark-defaults.conf
    touch ${SPARK_HOME}/spark/conf/workers
    cp ${SPARK_HOME}/conf/log4j.properties.template ${SPARK_HOME}/conf/log4j.properties
```

#### 5.5.2 修改 ${SPARK_HOME}/conf/spark-env.sh 

```bash
    # 添加如下内容
    export JAVA_HOME=/opt/java/jdk-08
    export SCALA_HOME=/opt/java/scala-212
    export HADOOP_HOME=/opt/apache/hadoop
    export SPARK_HOME=/opt/apache/spark
    
    export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    export YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    # export SPARK_MASTER_HOST=master
    export SPARK_CONF_DIR=${SPARK_HOME}/conf
    
    export SPARK_HISTORY_OPTS="
        -Dspark.history.ui.port=18080 
        -Dspark.history.fs.logDirectory=hdfs://master:9000/spark/logs 
        -Dspark.history.retainedApplications=30"
    
    export SPARK_DIST_CLASSPATH=$(${HADOOP_HOME}/bin/hadoop classpath)
```

#### 5.5.3 修改 ${SPARK_HOME}/conf/spark-defaults.conf 

```bash
    # 添加如下内容
    spark.master                      yarn
    spark.eventLog.enabled            true
    spark.eventLog.dir                hdfs://master:9000/spark/logs
    spark.yarn.jars                   hdfs://master:9000/spark/jars/*
    spark.history.fs.logDirectory     hdfs://master:9000/spark/history
    
    spark.serializer                  org.apache.spark.serializer.KryoSerializer
    spark.driver.memory               6g
    spark.executor.memory             8g
    
    spark.yarn.historyServer.address  http://0.0.0.0:18080
    spark.history.ui.port             18080
```

#### 5.5.4 修改 ${SPARK_HOME}/conf/works

```bash
    slaver1
    slaver2
    slaver3
```

### 5.6 上传 jar 到 hdfs

```bash
    # 在 hdfs 上创建 spark 需要的目录
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/                             # hdfs 上的 spark 工作目录
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/jars                         # 不带 hadoop 的 spark 依赖
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/logs                         # spark 日志存放目录
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/history                      # spark 历史服务器目录
    
    # 上传不带 hadoop 的 spark 依赖
    tar -zxvf spark-3.2.3-bin-without-hadoop.tgz                               # 解压下载的不带 hadoop 的 spark 安装包
    ${HADOOP_HOME}/bin/hadoop fs -put spark-3.2.3-bin-without-hadoop/jars/* /spark/jars/ # 上传文件到 hdfs
```

### 5.7 编写 spark 启停脚本

```bash
    cd /opt/apache/spark/bin/                                                  # 切换到 spark 安装目录的 bin
    touch ${SPARK_HOME}/bin/spark.sh                                           # 脚本内容，内容详见：/bigdata-deploy/shell/spark.sh
    chmod +x ${SPARK_HOME}/bin/spark.sh                                        # 添加可执行权限
```

### 5.8 同步 spark 安装路径，分发到其他节点 

```bash
    cd /opt/apache/                                                            # 切换到 spark 安装父路径
    ~/shell/xync.sh spark                                                      # 同步 spark 到其它节点
````

### 5.9 启动集群并测试

#### 5.9.1 启动集群

```bash
    # 单独启动
    ${SPARK_HOME}/sbin/start-master.sh                                         # 单独启动 master 主节点
    ${SPARK_HOME}/sbin/start-workers.sh spark://master:7077                    # 启动所有的 slave（worker） 节点
    ${SPARK_HOME}/sbin/start-worker.sh  spark://master:7077                    # 启动单台的 slave（worker） 节点
    ${SPARK_HOME}/sbin/start-history-server.sh                                 # 启动历史服务器
    
    # 全部启动
    ${SPARK_HOME}/sbin/start-all.sh                                            # 启动 master 和 worker 节点
    
    # 自定义脚本启动
    ${SPARK_HOME}/bin/spark.sh start                                           # 启动 master、worker、history-server
```

#### 5.9.2 集群测试

```bash
    jps -l                                                                     # 查看 spark 启动的 jvm 进程
    http://master:8080                                                         # 浏览器访问 SparkMaster
    http://master:4040                                                         # 浏览器访问 SparkUI
    http://master:18080                                                        # 浏览器访问 历史服务器
    
    # 计算 pi 
    ${SPARK_HOME}/bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[*] ${SPARK_HOME}/examples/jars/spark-examples_2.12-3.2.3.jar 100
    ${SPARK_HOME}/bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://master:7077 --deploy-mode cluster ${SPARK_HOME}/examples/jars/spark-examples_2.12-3.2.3.jar 100
    ${SPARK_HOME}/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --num-executors 3 --executor-cores 2 ${SPARK_HOME}/examples/jars/spark-examples_2.12-3.2.3.jar
    ${SPARK_HOME}/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client  --driver-memory 1G --executor-memory 1G --num-executors 3 --executor-cores 2 ${SPARK_HOME}/examples/jars/spark-examples_2.12-3.2.3.jar
    
    # 计算 wc
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/test/wc/input                # 创建 hdfs 数据输入目录
    ${HADOOP_HOME}/bin/hadoop fs -ls /hadoop/test/wc/input                     # 查看创建的目录
    ${HADOOP_HOME}/bin/hadoop fs -put ${SPARK_HOME}/RELEASE /spark/test/wc/input    # 上传文件到 hdfs
    ${SPARK_HOME}/bin/spark-submit --class com.example.spark.ScalaWordCount --master yarn --deploy-mode client --driver-memory 1G --executor-memory 1G --total-executor-cores 2 ~/coding/java/jar/wc-1.0.jar hdfs:///spark/test/input
```



## 6. Zookeeper 安装

### 6.1 zookeeper-3.6.4 下载

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[zookeeper-3.6.4](https://mirrors.aliyun.com/apache/zookeeper/zookeeper-3.6.4/apache-zookeeper-3.6.4-bin.tar.gz)** 安装包到本地

### 6.2 解压安装 zookeeper

```bash
    tar -zxvf apache-zookeeper-3.6.4-bin.tar.gz -C /opt/apache/                # 解压编译后的压缩包
    mv /opt/apache/apache-zookeeper-3.6.4-bin /opt/apache/zookeeper            # 修改目录名称
    mkdir -p /opt/apache/zookeeper/data/                                       # 创建数据存储目录
    mkdir -p /opt/apache/zookeeper/logs/                                       # 创建日志目录
```

### 6.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Zookeeper 3.6.4 ====================================== #
        export ZOOKEEPER_HOME=/opt/apache/zookeeper
        export PATH=${PATH}:${ZOOKEEPER_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 6.4 修改配置文件

#### 6.4.1 创建配置文件

```bash
    cd ${ZOOKEEPER_HOME}/conf/ || exit                                         # 切换到 zookeeper 配置文件目录
    touch /opt/apache/zookeeper/conf/zoo.cfg                                   # zookeeper 主要配置文件
    cp ${ZOOKEEPER_HOME}/conf/log4j.properties.template ${ZOOKEEPER_HOME}/conf/log4j.properties    # 日志配置文件
    mkdir -p ${ZOOKEEPER_HOME}/data                                            # 创建 ZK 数据存储目录
    mkdir -p ${ZOOKEEPER_HOME}/logs                                            # 创建 ZK 日志存储目录
```

#### 6.4.2 修改 ${ZOOKEEPER_HOME}/conf/zoo.cfg 

```bash
    # 用来调节心跳和超时, 默认的会话超时时间是两倍的 tickTime
    tickTime=2000
    
    # 用于配置允许 followers 连接并同步到 leader 的最大时间
    initLimit=10
    
    # 配置leader 和 followers 间进行心跳检测的最大延迟时间
    syncLimit=5
    
    # 存储内存数据库快照目录, 并且除非指定其它目录, 否则数据库更新的事务日志也将会存储在该目录下
    dataDir=/opt/apache/zookeeper/data
    
    # 配置 dataLogDir 参数来指定 ZooKeeper 事务日志的存储目录
    dataLogDir=/opt/apache/zookeeper/logs
    
    # 服务器监听客户端连接的端口, 也即客户端尝试连接的端口, 默认值是 2181 
    clientPort=2181
    
    # 不然会出现端口被占用的情况，因为默认是和 Apache.Tomcat 使用的 8080 端口
    admin.serverPort=8180
    
    # 限制单个客户端与单台服务器之前的并发连接数量, 可以通过 IP 地址来区分不同的客户端，它用来防止某种类型的 DoS 攻击, 将其设置为 0 将完全移除并发连接数的限制
    # maxClientCnxns=60
    
    # ZooKeeper 自动清理时需要保留的数据文件快照的数量和对应的事务日志文件, 默认值是 3
    # autopurge.snapRetainCount=3
    
    # 和 autopurge.snapRetainCount 配套使用, 用于配置 ZooKeeper 自动清理文件的频率，默认值是 1, 即默认开启自动清理功能, 设置为 0 则表示禁用自动清理功能。
    # autopurge.purgeInterval=1
    
    
    ## Metrics Providers
    # https://prometheus.io Metrics Exporter
    #metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
    #metricsProvider.httpPort=7000
    #metricsProvider.exportJvmInfo=true
    
    # 服务器唯一标识，各个节点中，自己的节点要修改为：server.*=0.0.0.0:2888:3888
    server.1=slaver1:2888:3888
    server.2=slaver2:2888:3888
    server.3=slaver3:2888:3888
```

#### 6.4.3 创建 ${ZOOKEEPER_HOME}/data/myid

```bash
    # 创建每个节点独有的版本号
    echo "1" >> ${ZOOKEEPER_HOME}/data/myid                                    # slaver1
    echo "2" >> ${ZOOKEEPER_HOME}/data/myid                                    # slaver2
    echo "3" >> ${ZOOKEEPER_HOME}/data/myid                                    # slaver3
```

### 6.6 编写 zookeeper 启停脚本

```bash
    cd /opt/apache/zookeeper/ || exit                                          # 切换到 zookeeper 安装目录的 bin
    touch ${ZOOKEEPER_HOME}/bin/zookeeper.sh                                   # 脚本内容，内容详见：/bigdata-deploy/shell/zookeeper.sh
    touch ${ISSAC_HOME}/shell/zookeeper-cluster.sh                             # 脚本内容，内容详见：/bigdata-deploy/other/zookeeper-cluster.sh
    chmod +x ${ZOOKEEPER_HOME}/bin/zookeeper.sh                                # 添加可执行权限
    chmod +x ${ISSAC_HOME}/shell/zookeeper-cluster.sh                          # 添加可执行权限
```

### 6.7 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 zookeeper 安装父路径
    ~/shell/xync.sh zookeeper                                                  # 同步 zookeeper 到其它节点
````

### 6.8 启动 Zookeeper 并验证

```bash
    # 启动 zookeeper 集群
   ${ZOOKEEPER_HOME}/bin/zkServer.sh start                                     # 分别在 slaver1、slaver2、slaver3 执行 
   ${ZOOKEEPER_HOME}/bin/zookeeper.sh start                                    # 分别在 slaver1、slaver2、slaver3 执行 
   ${ISSAC_HOME}/shell/zookeeper-cluster.sh start                              # 一次性启动集群
   
   # 查看各节点状态
   jps -l 
   ${ZOOKEEPER_HOME}/bin/zkServer.sh status                                    # 在每个节点执行查看
   ${ISSAC_HOME}/shell/xcall.sh ${ZOOKEEPER_HOME}/bin/zkServer.sh status       # 在主节点 slaver 执行查看
```



## 7. Kafka 安装配置

### 7.1 下载 kafka-3.2.3 安装包

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[kafka-3.2.3](https://mirrors.aliyun.com/apache/kafka/3.2.3/kafka_2.12-3.2.3.tgz)** 安装包到本地

### 7.2 解压安装 kafka

```bash
    tar -zxvf kafka_2.12-3.2.3.tgz -C /opt/apache/                             # 解压 压缩包
    mv /opt/apache/kafka_2.12-3.2.3 /opt/apache/kafka                          # 修改目录名称
    mkdir -p /opt/apache/kafka/data/                                           # 创建数据存储目录
    mkdir -p /opt/apache/kafka/logs/                                           # 创建日志目录
```

### 7.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Kafka 3.2.3 ====================================== #
        export KAFKA_HOME=/opt/apache/kafka
        export PATH=${PATH}:${KAFKA_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 7.4 修改配置文件

#### 7.4.1 修改 ${KAFKA_HOME}/config/zookeeper.properties

```properties
    # kafka 的数据在 zookeeper 中存储位置
    dataDir=/opt/apache/zookeeper/data
    
    # zookeeper 的端口号
    clientPort=2181
    
    # disable the per-ip limit on the number of connections since this is a non-production config
    maxClientCnxns=0
    
    # Disable the adminserver by default to avoid port conflicts.
    # Set the port to something non-conflicting if choosing to enable this
    admin.enableServer=false
    # admin.serverPort=808
```

#### 7.4.2 修改 ${KAFKA_HOME}/config/producer.properties

```properties
    bootstrap.servers=slaver1:9092,slaver2:9092,slaver3:9092
    
    # specify the compression codec for all data generated: none, gzip, snappy, lz4, zstd
    compression.type=gzip
```

#### 7.4.3 修改 ${KAFKA_HOME}/config/server.properties

```properties
    ############################# Server Basics #############################
    # 配合 broker 的 id：对于每个 broker.id 来说，必须设置为唯一的整数，且从 0 开始（注意：每台机器的 id 不同）
    broker.id=0
    # 删除 topic 功能
    delete.topic.enable=true
    
    ############################# Socket Server Settings #############################
    # 每个节点需要配置自己的参数
    listeners=PLAINTEXT://slaver*:9092
    advertised.listeners=PLAINTEXT://slaver*:9092
    # listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
    
    num.network.threads=3
    num.io.threads=8
    socket.send.buffer.bytes=102400
    socket.receive.buffer.bytes=102400
    socket.request.max.bytes=104857600
    
    ############################# Log Basics #############################
    # 数据日志文件存储路径（用逗号分隔的目录列表）
    log.dirs=/opt/apache/kafka/data
    num.partitions=3
    num.recovery.threads.per.data.dir=1
    
    ############################# Internal Topic Settings  #############################
    offsets.topic.replication.factor=3
    transaction.state.log.replication.factor=3
    transaction.state.log.min.isr=1
    
    ############################# Log Flush Policy #############################
    # log.flush.interval.messages=10000
    # log.flush.interval.ms=1000
    
    ############################# Log Retention Policy #############################
    log.retention.hours=168
    log.segment.bytes=1073741824
    log.retention.check.interval.ms=300000
    
    ############################# Zookeeper #############################
    # 配置连接 zookeeper 的集群地址（用逗号分隔的目录列表）
    zookeeper.connect=slaver1:2181/kafka,slaver2:2181/kafka,slaver3:2181/kafka
    
    # 连接 zookeeper 超时
    zookeeper.connection.timeout.ms=20000
    
    group.initial.rebalance.delay.ms=0
```

#### 7.4.4 修改 ${KAFKA_HOME}/config/consumer.properties

```properties
    bootstrap.servers=slaver1:9092,slaver2:9092,slaver3:9092
    group.id=test-consumer-group
    # auto.offset.reset=
```

### 7.5 安装 kafka 的可视化工具 efak 

#### 7.5.1 下载 efak-3.0.1 安装包

从 [**kafka-eagle 官网**](http://www.kafka-eagle.org/) 找到下载按钮，并 **[点击按钮](https://github.com/smartloli/kafka-eagle-bin/blob/master/efak-web-3.0.2-bin.tar.gz)** 下载安装包到本地

#### 7.5.2 解压安装 efak

```bash
    tar -zxvf kafka-eagle-bin-3.0.1.tgz                                        # 解压 压缩包
    cd kafka_2.12-3.2.3/                                                       # 进入到解压路径
    tar -zxvf efak-web-3.0.1-bin.tar.gz -C ${KAFKA_HOME}/                      # 解压 压缩包
    mv ${KAFKA_HOME}/efak-web-3.0.1 ${KAFKA_HOME}/efak                         # 修改目录名称
```

#### 7.5.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== efak 3.0.1 ====================================== #
        export KE_HOME=/opt/apache/kafka/efak
        export PATH=${PATH}:${KE_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
    
    vim ${ZOOKEEPER_HOME}/bin/zkServer.sh                                      # 修改 Zookeeper 脚本，用于支持 3.5+ 版本
        ZOOMAIN="-Dzookeeper.4lw.commands.whitelist=* ${ZOOMAIN}"              # 在 if [ "x$SERVER_JVMFLAGS" != "x" ] 之前，大约 74 行添加
        
    vim ${KAFKA_HOME}/bin/kafka-run-class.sh                                   # 修改 Kafka 脚本，用于支持 JMX
        JMX_PORT=9988                                                          # 在注释后，脚本开始前添加
```

### 7.5.4 创建 efak 的 mysql 数据库

```mysql
    mycli -h master -P 3306 -u root -p 111111 -D mysql                         # 使用修改后的密码进行登录
    create database if not exists ke;                                          # 创建 efak 数据库
    grant all privileges on ke.* to 'issac'@'%';                               # 将数据库 ke 的所有授权给用户 issac
    flush privileges;                                                          # 刷新权限
```


#### 7.5.5 修改配置文件 ${KE_HOME}/conf/system-config.properties

```properties
    cluster1.zk.list=slaver1:2181/kafka,slaver2:2181/kafka,slaver3:2181/kafka  # 7 行
    efak.webui.port=8048                                                       # 31 行
    efak.worknode.port=8085                                                    # 39 行
    cluster1.efak.offset.storage=kafka                                         # 54 行
    
    efak.distributed.enable=true                                               # 启用分布式模式
    efak.cluster.mode.status=master                                            # 在master节点上设置角色为 master，其他节点设置为 slave
    efak.worknode.master.host=master                                           # 设置 master 节点的主机地址
    efak.worknode.port=8085                                                    # 设置一个可用的端口供 WorkNodeServer 使用
    
    # 元数据存储路径（125-128 行）
    efak.driver=com.mysql.cj.jdbc.Driver
    efak.url=jdbc:mysql://master:3306/ke?useUnicode=true&characterEncoding=UTF-8&createDatabaseIfNotExist=true&allowPublicKeyRetrieval=true&useSSL=false&zeroDateTimeBehavior=convertToNull
    efak.username=issac
    efak.password=111111
```

#### 7.5.6 修改配置文件 ${KE_HOME}/conf/works

```bash
    echo "slaver1" >> ${KE_HOME}/conf/works 
    echo "slaver2" >> ${KE_HOME}/conf/works 
    echo "slaver3" >> ${KE_HOME}/conf/works 
```

### 7.6 编写 kafka 启停和 kafka 集群启停脚本： ${KAFKA_HOME}/bin/kafka.sh

```bash
    cd /opt/apache/kafka/bin || exit                                           # 切换到 kafka 安装目录的 bin 目录
    touch ${KAFKA_HOME}/bin/kafka.sh                                           # 脚本内容，内容详见：/bigdata-deploy/apache/kafka.sh
    chmod +x ${KAFKA_HOME}/bin/kafka.sh                                        # 添加可执行权限
    
    touch ${EFAK_HOME}/bin/efak.sh                                             # 脚本内容，内容详见：/bigdata-deploy/apache/efak.sh
    chmod +x ${EFAK_HOME}/bin/efak.sh                                          # 添加可执行权限
    
    touch ${ISSAC_HOME}/shell/kafka-cluster.sh                                 # 脚本内容，内容详见：/bigdata-deploy/other/kafka-cluster.sh
    chmod +x ${ISSAC_HOME}/shell/kafka-cluster.sh                              # 添加可执行权限
```

### 7.7 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 kafka 安装父路径
    ~/shell/xync.sh kafka                                                      # 同步 kafka 到其它节点
````

### 7.8 启动 kafka 和 efak 

```bash
    ${KAFKA_HOME}/bin/kafka-server-start.sh -daemon ${KAFKA_HOME}/config/server.properties > /dev/null 2>&1 &     # 每个节点单独启动 
    ${KAFKA_HOME}/bin/kafka.sh start                                           # 每个节点单独启动 
    ${ISSAC_HOME}/shell/kafka-cluster.sh start                                 # 集群启动
    ${KE_HOME}/bin/ke.sh cluster start                                         # 启动 EFAK
    
    jps -l                                                                     # 查看 kafka 启动的 jvm 进程
    http://master:8048/                                                        # EFAK UI
```


### 7.9 kafka 测试

```bash
    # 查看当前服务器中的所有 topic
    ${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server slaver1:9092,slaver2:9092,slaver3:9092 --list
    
    # 创建 topic
    ${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server slaver1:9092,slaver2:9092,slaver3:9092 --create --topic test --replication-factor 3 --partitions 3
    
    # 发送消息
    ${KAFKA_HOME}/bin/kafka-console-producer.sh --broker-list slaver1:9092,slaver2:9092,slaver3:9092 --topic test
    
    # 1.5 消费消息
    ${KAFKA_HOME}/bin/kafka-console-consumer.sh --bootstrap-server slaver1:9092,slaver2:9092,slaver3:9092 --from-beginning --topic test
```



## 8. Hive 安装配置

### 8.1 下载 Hive-3.1.3 源码

使用 git 从 [**github**](https://github.com/) 下载 **[Hive-3.1.3](https://github.com/apache/hive.git)** 源码到安装有图形化界面的 linux 系统

### 8.2 编译 Hive（hive-3.1.3 和 hadoop-3.2.4、spark-3.2.3 在兼容性上存在问题，需要修改源码重新编译）

#### 8.2.1 clone hive-3.1.3 源码，测试编译

```bash
    # 下载源码，并编译
    git clone https://github.com/apache/hive.git                               # clone 源码
    cd hive || exit                                                            # 进入解压路

    # 切换分支
    git branch                                                                 # 查看本地分支
    git branch -r                                                              # 查看远程分支，-r 表示 remote
    git branch -a                                                              # 查看所有分支
    git checkout rel/release-3.1.3                                             # 切换到 3.1.3 分支
    
    mvn clean -DskipTests package -Pdist                                       # 跳过测试，对 hive 进行编译打包
```

### 8.3 修改源码

```bash
    touch issac-hive.patch                                                     # 创建补丁，补丁内容详见：/bigdata-deploy/patch/issac-hive.patch
    
    git apply --check ./issac-hive.patch                                       # 检查 patch 是否可用
    git apply ./issac-hive.patch                                               # 应用补丁，不包含 commit 内容
    git am ./issac-hive.patch                                                  # 应用补丁，包含 commit 内容
    
    mvn clean -DskipTests package -Pdist                                       # 跳过测试，对 hive 进行编译打包
```

### 8.4 解压安装

```bash
    cd packaging/target || exit                                                # 进入编译后的打包路径
    tar -zxvf apache-hive-3.1.3-bin.tar.gz -C /opt/apache/                     # 解压编译后的压缩包
    mv /opt/apache/apache-hive-3.1.3-bin /opt/apache/hive                      # 修改目录名称
    mkdir -p /opt/apache/hive/logs/                                            # 创建日志目录
```

### 8.5 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Hive 3.1.3 ====================================== #
        export HIVE_HOME=/opt/apache/hive
        export PATH=${PATH}:${HIVE_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 8.6 修改配置文件

#### 8.6.1 创建配置文件

```bash
    cd /opt/apache/hive || exit                                                # 切换到 hive 安装目录
    touch ${HIVE_HOME}/conf/hive-env.sh                                        # hive 环境参数配置
    touch ${HIVE_HOME}/conf/hive-site.xml                                      # hive 主要配置文件
    touch ${HIVE_HOME}/conf/beeline-site.xml                                   # beeline 客户端配置文件
    mv ${HIVE_HOME}/conf/hive-log4j2.properties.template ${HIVE_HOME}/conf/hive-log4j2.properties
    
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /hive                               # 创建 HDFS 上 Hive 的根目录
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /hive/data                          # 创建 HDFS 上 Hive 的 数据存储 目录
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /hive/tmp                           # 创建 HDFS 上 Hive 的 临时存储 目录
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /hive/logs                          # 创建 HDFS 上 Hive 的 日志存储 目录
    
    ${HADOOP_HOME}/bin/hadoop fs -chmod -R 777 /hive/data                      # 修改 Hive 的 数据存储 目录权限
    ${HADOOP_HOME}/bin/hadoop fs -chmod -R 777 /hive/tmp                       # 修改 Hive 的 临时存储 目录权限
    ${HADOOP_HOME}/bin/hadoop fs -chmod -R 777 /hive/logs                      # 修改 Hive 的 日志存储 目录权限
```

#### 8.6.2 修改 ${HIVE_HOME}/conf/hive-env.sh

```bash
    export HADOOP_HEAPSIZE=4096
    
    # Hadoop 安装路径
    export HADOOP_HOME=/opt/apache/hadoop
    
    # Hive 配置文件路径
    export HIVE_CONF_DIR=/opt/apache/hive/conf
    
    # Hive jar 包路径
    export HIVE_AUX_JARS_PATH=/opt/apache/hive/lib
```

#### 8.6.3 修改 ${HIVE_HOME}/conf/hive-site.xml

```xml
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <!-- 记录 Hive 中的元数据信息在 mysql 中 -->
        <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://master:3306/hive?serverTimezone=UTC&amp;createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;allowPublicKeyRetrieval=true</value>
            <description>连接数据库用户名称</description>
        </property>
        <!-- jdbc mysql驱动 -->
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>com.mysql.cj.jdbc.Driver</value>
            <description>连接数据库驱动</description>
        </property>
        <!-- mysql 的用户名和密码 -->
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>issac</value>
            <description>连接数据库用户名称</description>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>111111</value>
            <description>连接数据库用户密码</description>
        </property>
        
        <!-- Hive 元数据存储版本的验证 -->
        <property>
            <name>hive.metastore.schema.verification</name>
            <value>false</value>
        </property>
        
        <!-- 元数据读取不到 -->
        <property>
            <name>metastore.storage.schema.reader.impl</name>
            <value>org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader</value>
        </property>
         <!-- 元数据存储授权 -->
        <property>
            <name>hive.metastore.event.db.notification.api.auth</name>
            <value>false</value>
        </property>
        
        <!-- 自动创建相关数据 -->
        <property>
            <name>datanucleus.fixedDatastore</name>
            <value>false</value>
        </property>
        <property>
            <name>datanucleus.readOnlyDatastore</name>
            <value>false</value>
        </property>
        <property>
            <name>datanucleus.schema.autoCreateAll</name>
            <value>true</value>
        </property>
        <property>
            <name>datanucleus.autoCreateSchema</name>
            <value>true</value>
        </property>
        <property>
            <name>datanucleus.autoCreateTables</name>
            <value>true</value>
        </property>
        <property>
            <name>datanucleus.autoCreateColumns</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.metastore.local</name>
            <value>true</value>
        </property>
        <!-- 显示表的列名 -->
        <property>
            <name>hive.cli.print.header</name>
            <value>true</value>
            <description>客户端显示当前查询表的头信息</description>
        </property>
        <!-- 显示数据库名称 -->
        <property>
            <name>hive.cli.print.current.db</name>
            <value>true</value>
            <description>客户端显示当前数据库名称信息</description>
        </property>
        <!-- hdfs 位置 -->
        <property>
            <name>hive.metastore.warehouse.dir</name>
            <value>/hive/data</value>
            <description>hdfs 上 hive 数据存放位置</description>
        </property>
        
        <!-- Hive 作业的 HDFS 根目录位置 -->
        <property>
            <name>hive.exec.scratchdir</name>
            <value>/hive/tmp</value>
        </property>
        
        <!-- Hive 作业的 HDFS 根目录创建写权限 -->
        <property>
            <name>hive.scratch.dir.permission</name>
            <value>777</value>
        </property>
        
        <!-- HDFS 日志目录 -->
        <property>
            <name>hive.querylog.location</name>
            <value>/hive/logs</value>
        </property>
        
        <!-- 设置 主节点元数据 metastore 服务的节点信息 -->
        <property>
            <name>hive.metastore.uris</name>
            <value>thrift://master:9083</value>
        </property>
        
        <!-- 客户端远程连接的端口 -->
        <property>
            <name>hive.server2.thrift.port</name>
            <value>10000</value>
        </property>
        <property>
            <name>hive.server2.thrift.bind.host</name>
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>hive.server2.webui.host</name>
            <value>0.0.0.0</value>
        </property>
        
        <!-- hive 服务的页面的端口 -->
        <property>
            <name>hive.server2.webui.port</name>
            <value>10002</value>
        </property>
        
        <property> 
            <name>hive.server2.long.polling.timeout</name>
            <value>5000</value>
        </property>
        
        <property>
            <name>hive.server2.enable.doAs</name>
            <value>true</value>
        </property>
        
        <property>
            <name>datanucleus.autoCreateSchema</name>
            <value>false</value>
        </property>
        
        <property>
            <name>datanucleus.fixedDatastore</name>
            <value>true</value>
        </property>
        
        <property>
            <name>hive.server2.thrift.client.user</name>
            <value>issac</value>
            <description>Username to use against thrift client</description>
        </property>
        <property>
            <name>hive.server2.thrift.client.password</name>
            <value>111111</value>
            <description>Password to use against thrift client</description>
        </property>
        
        <!-- Spark 依赖位置（注意：端口号 9000 必须和 namenode 的端口号一致） -->
        <property>
            <name>spark.yarn.jars</name>
            <value>hdfs://master:9000/spark/jars/*</value>
        </property>
        
        <!-- Hive 默认执行引擎 -->
        <property>
            <name>hive.execution.engine</name>
            <value>spark</value>
        </property>
        <property>
            <name>hive.enable.spark.execution.engine</name>
            <value>true</value>
        </property>
        
        <!-- 连接超时问题 -->
        <property>
            <name>hive.spark.client.connect.timeout</name>
            <value>900000</value>
        </property>
        <property>
            <name>hive.spark.client.server.connect.timeout</name>
            <value>900000</value>
        </property>
        <property>
            <name>hive.fetch.task.conversion</name>
            <value>more</value>
            <description>
                0. none    : disable hive.fetch.task.conversion
                1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
                2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)
            </description>
        </property>
        
        <!-- Hive 默认的文件存储格式 -->
        <!--
        <property>
            <name>hive.default.fileformat</name>
            <value>parquet</value>
        </property>
        <property>
            <name>hive.default.fileformat.managed</name>
            <value>parquet</value>
        </property>
        -->
        
        <!-- Hive 在执行时使用压缩 -->
        <property>
            <name>hive.exec.compress.intermediate</name>
            <value>true</value>
        </property>
        
        <!-- Hive 在执行时的压缩格式 -->
        <property>
            <name>hive.intermediate.compression.codec</name>
            <value>org.apache.hadoop.io.compress.GzipCodec</value>
        </property>
        
        <!-- Hive 在压缩时，按块进行压缩 -->
        <property>
            <name>hive.intermediate.compression.type</name>
            <value>BLOCK</value>
        </property>
        
        <!-- Hive 在执行结束，输出到 HDFS 的时候使用压缩 -->
        <property>
            <name>hive.exec.compress.output</name>
            <value>true</value>
        </property>
    </configuration>
```

#### 8.6.4 修改 ${HIVE_HOME}/conf/beeline-site.xml

```xml
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>beeline.hs2.jdbc.url.tcpUrl</name>
            <value>jdbc:hive2://master:10000/default;user=issac;password=111111</value>
        </property>
        <property>
            <name>beeline.hs2.jdbc.url.httpUrl</name>
            <value>jdbc:hive2://master:10000/default;user=issac;password=111111;transportMode=http;httpPath=cliservice</value>
        </property>
        <property>
            <name>beeline.hs2.jdbc.url.default</name>
            <value>tcpUrl</value>
        </property>
    </configuration>
```

#### 8.6.5 修改 ${HIVE_HOME}/conf/hive-log4j2.properties

```properties
    property.hive.log.dir = /opt/apache/hive/logs                              # 修改日志存储路径
```

#### 8.6.6 复制 ${HIVE_HOME}/conf/hive-site.xml、${SPARK_HOME}/conf/spark-defaults.conf

```bash
    cp ${HIVE_HOME}/conf/hive-site.xml ${SPARK_HOME}/conf/                     # 复制 hive 配置到 spark 配置目录，用于搭建 hive on spark 
    cp ${SPARK_HOME}/conf/spark-defaults.conf ${HIVE_HOME}/conf/               # 复制 spark 配置到 hive 配置目录，用于搭建 hive on spark
```

### 8.7 初始化元数据

### 8.7.1 Mysql 连接数据库驱动下载  

在 **[Maven 仓库](https://mvnrepository.com/)** 搜索并下载 **[Mysql jdbc 驱动](https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.32/mysql-connector-java-8.0.32.jar)** 到本地

### 8.7.2 复制数据库驱动到 hive 依赖

```bash
    cp mysql-connector-java-8.0.32.jar ${HIVE_HOME}/lib/
```

#### 8.7.3 启动 Mysql 创建相关数据库

```bash
    /opt/mysql/bin/mysql.sh start                                              # 启动数据库
    mycli -h master -P 3306 -u root -p 111111 -D mysql                         # 连接数据库
    
    create database if not exists hive;                                        # 创建 hive  数据库
    grant all privileges on hive.*  to 'issac'@'%';                            # 将数据库 hive 的所有权限给用户 issac
```

#### 8.7.4 初始化元数据

```bash
    cd /opt/apache/hive/ || exit                                               # 切换到 hive 安装目录
    ${HIVE_HOME}/bin/schematool -dbType mysql -initSchema -verbose             # 初始化元数据
```

#### 8.7.5 解决 hive 中文乱码

```mysql
    -- 登录 mysql，切换到 hive 数据库
    use hive;
    
    -- 修改字段注释字符集
    alter table columns_v2       modify column comment      varchar(2048)  character set utf8mb4;
    
    -- 修改表注释字符集
    alter table table_params     modify column param_value  varchar(4096)  character set utf8mb4;
    
    -- 修改分区参数，支持分区建用中文表示
    alter table partition_params modify column param_value  varchar(4096)  character set utf8mb4;
    alter table partition_keys   modify column pkey_comment varchar(4096)  character set utf8mb4;
    
    -- 修改索引名注释，支持中文表示
    alter table index_params     modify column param_value  varchar(4096)  character set utf8mb4;
    
    -- 修改视图，支持视图中文
    alter table tbls modify column view_expanded_text       mediumtext     character set utf8mb4;
    alter table tbls modify column view_original_text       mediumtext     character set utf8mb4;
    
    -- 刷新权限
    flush privileges;
```

### 8.8 Hive 启停脚本：${HIVE_HOME}/bin/hive.sh 

```bash
    cd /opt/apache/hive/bin/                                                   # 切换到 hive 安装目录的 bin
    touch ${HIVE_HOME}/bin/hive.sh                                             # 脚本内容，详见：/bigdata-deploy/shell/apache/hive.sh
    chmod +x ${HIVE_HOME}/bin/hive.sh                                          # 添加可执行权限
```

### 8.9 同步 hive 安装路径，分发到其他节点 

```bash
    cd /opt/apache/                                                            # 切换到 hive 安装父路径
    ~/shell/xync.sh hive                                                       # 同步 hive 到其它节点
````

### 8.10 启动 Hive 

```bash
    # 切换到 hive 安装目录
    cd /opt/apache/hive/ || exit
    
    # 单个服务一次启动，分别启动 metastore 和 hiveserver2 
    nohup ${HIVE_HOME}/bin/hive --service metastore >> ${HIVE_HOME}/logs/${LOG_FILE} 2>&1 &
    nohup ${HIVE_HOME}/bin/hiveserver2              >> ${HIVE_HOME}/logs/${LOG_FILE} 2>&1 &
    
    # 自定义脚本启动
    ${HIVE_HOME}/bin/hive.sh start
    
    # 查看 hive 启动状态
    jps -l 
    http://master:10002/
```

### 8.11 测试 Hive

```mysql
    -- 使用 datagrip 连接上 hive 
    
    show databases;                                                            -- 查看所有数据库
    create database if not exists test;                                        -- 创建 test 数据库
    use test;                                                                  -- 切换到 test 数据库
    create table if not exists student                                         -- 创建 test 数据库
    (
        id     int           comment '主键 ID',
        name   varchar(64)   comment '姓名',
        age    int           comment '年龄',
        gender int           comment '性别：-1，未知；0，女；1：男',
        hight  float         comment '身高：厘米',
        wight  float         comment '体重：千克',
        email  varchar(128)  comment '电子邮件',
        remark varchar(1024) comment '备注'
    ) comment '学生测试表';

    set mapreduce.map.java.opts='-Xmx4096m';                                   -- 设置 map 堆内存
    set mapreduce.reduce.java.opts='-Xms4096m';                                -- 设置 reduce 堆内存
    
    -- 测试 mr 引擎
    set hive.execution.engine=mr;
    insert into student (id, name, age, gender, hight, wight, email, remark) values (1, '张三', 33, 1, 172.1, 48.9, 'zhangsan@qq.com', '学生');
    
    -- 测试 spark 引擎
    set hive.execution.engine=spark;
    insert into student (id, name, age, gender, hight, wight, email, remark) values (2, '李四', 23, 0, 165.1, 53.9, 'lisi@qq.com', '学生');
    insert into student (id, name, age, gender, hight, wight, email, remark) 
        values (3, '王五', 28, 1, 168.3, 52.7, 'wangwu@qq.com', '学生'), 
               (4, '赵六', 22, 0, 161.3, 46.7, 'zhaoliu@qq.com', '教师');
    
    -- 测试执行计划
    explain formatted select * from student;
    select * from student limit 10;
```

## 9. HBase 安装

### 9.1 HBase-2.4.15 下载

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[hbase-2.4.16](https://mirrors.aliyun.com/apache/hbase/2.4.16/hbase-2.4.16-bin.tar.gz)** 安装包到本地

### 9.2 解压安装 hbase

```bash
    tar -zxvf hbase-2.4.16-bin.tar.gz -C /opt/apache/                          # 解压 压缩包
    mv /opt/apache/hbase-2.4.16-bin /opt/apache/hbase                          # 修改目录名称
    mkdir -p /opt/apache/hbase/data/                                           # 创建数据存储目录
    mkdir -p /opt/apache/hbase/logs/                                           # 创建日志目录
```

### 9.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Hbase 2.4.16 ====================================== #
        export HBASE_HOME=/opt/apache/hbase
        export PATH=${PATH}:${HBASE_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 9.4 配置文件修改

#### 9.4.1 修改 ${HBASE_HOME}/conf/hbase-env.sh

```bash
    export JAVA_HOME=/opt/java/jdk-08
    export HBASE_HEAPSIZE=4G
    export HBASE_OFFHEAPSIZE=4G
    export HBASE_LOG_DIR=${HBASE_HOME}/logs
    export HBASE_PID_DIR=/opt/apache/hbase/data
    export HBASE_MANAGES_ZK=false
```

#### 9.4.2 修改 ${HBASE_HOME}/conf/hbase-site.xml

```xml
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    
    <!-- See also https://hbase.apache.org/book.html#standalone_dist -->
    <configuration>
        <!-- 指定 HBase 在 HDFS 上存储的路径 -->
        <property>
            <name>hbase.rootdir</name>
            <value>hdfs://master:9000/hbase</value>
        </property>
        <!-- 指定 HBase 是否分布式运行 -->
        <property>
            <name>hbase.cluster.distributed</name>
            <value>true</value>
        </property>
        <!-- 在分布式的情况下一定要设置，不然容易出现 Hmaster 起不来的情况 -->
        <property>
            <name>hbase.unsafe.stream.capability.enforce</name>
            <value>false</value>
        </property>
        <!-- 指定 zookeeper 的地址，多个用 "," 分割 -->
        <property>
            <name>hbase.zookeeper.quorum</name>
            <value>slaver1:2181,slaver2:2181,slaver3:2181</value>
        </property>
        <!-- 指定在 zookeeper 的数据存储路径，与 zookeeper 配置相同 -->
        <property>
            <name>hbase.zookeeper.property.dataDir</name>
            <value>/opt/apache/zookeeper/data</value>
        </property>
        <!-- 连接 zookeeper 的端口号 -->
        <property>
            <name>hbase.zookeeper.property.clientPort</name>
            <value>2181</value>
        </property>
        <!-- 指定 HBase 管理页面 -->
        <property>
            <name>hbase.master.info.port</name>
            <value>16010</value>
        </property>
        
        <!-- 建立二级索引，将业务需要的数据联立建立索引，方便查询 -->
        <property>
            <name>hbase.regionserver.wal.codec</name>
            <value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
        </property>
        
        <!-- 如果使用了 hbase 中的自定义 namespace，不仅仅使用 default -->
        <!-- 那么在 phoenix 中与之对应的是 schema 的概念，但是默认并没有开启，需要在增加以下配置项 -->
        <property>
            <name>phoenix.schema.isNamespaceMappingEnabled</name>
            <value>true</value>
        </property>
        
        <property>
            <name>phoenix.schema.mapSystemTablesToNamespace</name>
            <value>true</value>
        </property>
        
        <!-- 用户可以创建临时或永久的用户自定义函数 -->
        <!-- 这些用户自定义函数可以像内置的 create、upsert、delete 一样被调用 -->
        <property>
            <name>phoenix.functions.allowUserDefinedFunctions</name>
            <value>true</value>
        </property>
    </configuration>
```

#### 9.4.3 修改 ${HBASE_HOME}/conf/regionservers

```bash
    cd /opt/apache/hbase/ || exit                                              # 切换的 hbase 安装路径
    echo "slaver1" >  ${HBASE_HOME}/conf/regionservers                         # 将主机 slaver1 添加到 RegionServer
    echo "slaver2" >> ${HBASE_HOME}/conf/regionservers                         # 将主机 slaver2 添加到 RegionServer
    echo "slaver3" >> ${HBASE_HOME}/conf/regionservers                         # 将主机 slaver3 添加到 RegionServer
```

#### 9.4.4 添加 ${HBASE_HOME}/conf/backup-masters

```bash
    # 添加以下内容，用于 HMaster 高可用（可忽略）
    echo "backend" > ${HBASE_HOME}/conf/backup-masters
```

#### 9.4.5 修改日志存储目录 ${HBASE_HOME}/conf/log4j.properties

```bash
    # 修改日志存储路径
    hbase.log.dir=/opt/apache/hbase/logs                                       # 修改第 20 行
```

#### 9.4.6 复制 hadoop 的配置文件

```bash
    # 解决与 hadoop 的 guava 依赖
    rm ${HBASE_HOME}/lib/guava-11.0.2.jar                                      # 删除 flume 旧版本的 guava
    cp -fr ${HADOOP_HOME}/share/hadoop/common/lib/guava-27.0-jre.jar ${HBASE_HOME}/lib/  # 和 hadoop 保持一致
    
    cp -fr  ${HADOOP_HOME}/etc/hadoop/core-site.xml  ${HBASE_HOME}/conf/
    cp -fr  ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml  ${HBASE_HOME}/conf/
```

### 9.5 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 hbase 安装父路径
    ~/shell/xync.sh hbase                                                      # 同步 hbase 到其它节点
````

### 9.6 编写 hbase 启停脚本：${HBASE_HOME}/bin/hbase.sh

```bash
    cd /opt/apache/hbase/bin/                                                  # 切换到 hbase 安装目录的 bin
    touch ${HBASE_HOME}/bin/hbase.sh                                           # 脚本内容，详见：/bigdata-deploy/shell/apache/hbase.sh
    chmod +x ${HBASE_HOME}/bin/hbase.sh                                        # 添加可执行权限
```

### 9.7 启动 HBase

```bash
    ${ZOOKEEPER_HOME}/bin/zookeeper.sh start                                   # 启动 zookeeper
    ${HADOOP_HOME}/bin/hadoop.sh start                                         # 启动 hadoop
    
    cd /opt/apache/hbase/ || exit                                              # 切换的 hbase 安装路径
    
    # 每个节点单独启动
    ${HBASE_HOME}/bin/hbase-daemon.sh start master                             # 主节点启动 HMaster
    ${HBASE_HOME}/bin/hbase-daemon.sh start regionserver                       # 每个 slaver 节点启动 HRegionServer
    
    # 启动集群的组件
    ${HBASE_HOME}/bin/hbase-daemons.sh start master                            # 启动集群的 HMaster
    ${HBASE_HOME}/bin/hbase-daemons.sh start regionserver                      # 启动集群所有的 HRegionServer
    
    # 启动集群所有组件
    ${HBASE_HOME}/bin/start-habse.sh                                           # 启动集群的 HMaster 和所有的 HRegionServer
    ${HBASE_HOME}/bin/hbase.sh start                                           # 启动集群的 HMaster 和所有的 HRegionServer，并检查启动状况
```

### 9.8 查看 HBase 启动结果并测试

```bash
    # 查看各节点状态
    jps -l 
    ${HBASE_HOME}/bin/hbase.sh status                                          # 在每个节点执行查看
    ${ISSAC_HOME}/shell/xcall.sh ${HBASE_HOME}/bin/hbase.sh status             # 在主节点 master 执行查看
    
    http://master:16010/master-status                                          # HMaster 状态
    http://master:16030/rs-status                                              # HRegionServer 状态
```


### 9.9 hbase shell 基本操作

```bash
    ${HBASE_HOME}/bin/hbase shell                                              # 进入 hbase 命令行
    
    list_namespace                                                             # 列出所有命名空间
    create_namespace 'test'                                                    # 新建命名空间
    drop_namespace   'test'                                                    # 删除命名空间，必须为空
    alter_namespace  'test', {METHOD => 'set', 'PROPERTY_NAME' => 'PROPERTY_VALUE'}      # 修改表空间
    
    list                                                                       # 列出所有表
    list_namespace_tables 'test'                                               # 列出指定命名空间下的所有表
    create 'test:test', 'cf1'                                                  # 新建一个命名空间 test 表名 test，列族为 cf1 的表
    describe 'test:test'                                                       # 查看表内容
    drop 'test:test'                                                           # 删除表
    
    scan 'test:test'                                                           # 查看所有的表数据
    scan 'test:test', {LIMIT=>5}                                               # 查看前 5 行数据
    
    create 'student','info','address'
    
    put 'student','1','info:age','22'
    put 'student','1','info:name','zhao'
    put 'student','1','info:class','2'
    put 'student','1','address:city','shanghai'
    put 'student','1','address:area','pudong'
    
    put 'student','2','info:age','21'
    put 'student','2','info:name','yang'
    put 'student','2','info:class','1'    
    put 'student','2','address:city','beijing'
    put 'student','2','address:area','CBD'
    
    scan 'student'
```


## 10. Phoenix 安装

### 10.1 Phoenix 下载

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[phoenix-5.1.3](https://mirrors.aliyun.com/apache/phoenix/phoenix-5.1.3/phoenix-hbase-2.4-5.1.3-bin.tar.gz)** 安装包到本地


### 10.2 解压安装 phoenix

```bash
    tar -zxvf phoenix-hbase-2.4-5.1.3-bin.tar.gz -C /opt/apache/               # 解压 压缩包
    mv /opt/apache/phoenix-hbase-2.4-5.1.3-bin /opt/apache/phoenix             # 修改目录名称
    mkdir -p /opt/apache/phoenix/logs                                          # 创建 日志存储 路径
```

### 10.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Phoenix 5.1.3 ====================================== #
        export PHOENIX_HOME=/opt/apache/phoenix
        export PATH=${PATH}:${PHOENIX_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 10.4 配置文件修改

#### 10.4.1 复制驱动并修改配置

```bash
    cd /opt/apache/phoenix/ || exit                                            # 切换的 phoenix 安装路径
    cp ${PHOENIX_HOME}/phoenix-server-hbase-2.4-5.1.3.jar ${HBASE_HOME}/lib/   # 复制 驱动
    vim ${HBASE_HOME}/conf/hbase-site.xml                                      # hbase 的配置文件添加 8.4.2 内容
    cp ${HBASE_HOME}/conf/hbase-site.xml ${PHOENIX_HOME}/bin/                  # 复制 hbase 配置文件到 phoenix 
    cp ${HADOOP_HOME}/etc/hadoop/core-site.xml ${PHOENIX_HOME}/bin/            # 复制 hadoop 的 core 配置文件到 phoenix 
    cp ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml ${PHOENIX_HOME}/bin/            # 复制 hadoop 的 hdfs 配置文件到 phoenix    
```

#### 10.4.2 修改 ${HBASE_HOME}/conf/hbase-site.xml   

````xml
    <!-- 如果使用了 hbase 中的自定义 namespace，不仅仅使用 default -->
    <!-- 那么在 phoenix 中与之对应的是 schema 的概念，但是默认并没有开启，需要在增加以下配置项 -->
    <property>
        <name>phoenix.schema.isNamespaceMappingEnabled</name>
        <value>true</value>
    </property>
    
    <property>
        <name>phoenix.schema.mapSystemTablesToNamespace</name>
        <value>true</value>
    </property>
    
    <!-- 用户可以创建临时或永久的用户自定义函数 -->
    <property>
        <name>phoenix.functions.allowUserDefinedFunctions</name>
        <value>true</value>
    </property>
````

### 10.5 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 phoenix 安装父路径
    ~/shell/xync.sh phoenix                                                    # 同步 phoenix 到其它节点
    ~/shell/xync.sh hbase/conf/hbase-site.xml                                  # 同步 hbase 配置到其它节点
````

### 10.6 编写 phoenix 启停脚本：${PHOENIX_HOME}/bin/phoenix.sh

```bash
    cd /opt/apache/phoenix/bin/                                                # 切换到 hive 安装目录的 bin
    touch ${PHOENIX_HOME}/bin/phoenix.sh                                       # 脚本内容，详见：/bigdata-deploy/shell/apache/phoenix.sh
    chmod +x ${PHOENIX_HOME}/bin/phoenix.sh                                    # 添加可执行权限
```

### 10.7 启动 HBase
```bash
    ${ZOOKEEPER_HOME}/bin/zookeeper.sh start                                   # 启动 zookeeper
    ${HADOOP_HOME}/bin/hadoop.sh start                                         # 启动 hadoop
    ${HBASE_HOME}/bin/hbase.sh start                                           # 启动 hbase
    
    cd /opt/apache/phoenix/ || exit                                            # 切换的 phoenix 安装路径
    ${PHOENIX_HOME}/bin/sqlline.py slaver1,slaver2,slaver3:2181                # 启动 sqlline 客户端
    nohup ${PHOENIX_HOME}/bin/queryserver.py >> ${PHOENIX_HOME}/logs/queryserver.log 2>&1 &   # 后台启动查询
    jps -l                                                                     # 查看 phoenix 和 客户端 状态
```

### 10.8 测试 phoenix

```mysql
    create schema if not exists test;                                          -- 创建 schema（数据库），就是 hbase 中的 namespace
    use test;                                                                  -- 切换到 test schema
    !tables                                                                    -- 查看所有表格
    
    create table if not exists student                                         -- 创建 test 表
    (
        id     bigint      primary key,
        name   varchar(64),
        age    bigint,
        gender bigint'
    );
    
    !describe student;                                                         -- 查看表的结构
    upsert into student (id, name, age, gender) values(1001, '张三', 25, 0);   -- 添加 
    upsert into student (id, name, age, gender) values(1002, '李四', 36, 1);   -- 添加 
    upsert into student (id, name, age, gender) values(1001, '王五', 27, 1);   -- 修改 
    select * from student;                                                     -- 查询    
    delete from student where id = 1001;                                       -- 删除
    
    select * from SYSTEM.CATALOG;                                              -- phoenix 中的表信息都存在 SYSTEM.CATALOG 表
    !exit                                                                      -- 退出 sqlline 客户端，或者：!quit
```



## 11. Flink 安装

### 11.1 flink-1.15.3 下载

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[flink-1.15.3](https://mirrors.aliyun.com/apache/flink/flink-1.15.3/flink-1.15.3-bin-scala_2.12.tgz)** 安装包到本地

### 11.2 解压安装 flink

```bash
    tar -zxvf flink-1.15.3-bin-scala_2.12.tgz -C /opt/apache/                  # 解压 压缩包
    mv /opt/apache/flink-1.15.3-bin-scala_2.12 /opt/apache/flink               # 修改目录名称
    mkdir -p /opt/apache/flume/logs/                                           # 创建日志目录
```

### 11.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== flink 1.15.3 ====================================== #
        export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop/
        export FLINK_HOME=/opt/apache/flink
        export PATH=${PATH}:${FLINK_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 11.4 配置文件修改

#### 11.4.1 修改 ${FLINK_HOME}/conf/flink-conf.yaml

```bash
    # ======================================= 基础 配置 ======================================= #
    # jobManager 的IP地址
    jobmanager.rpc.address: master
    
    # JobManager 的端口号
    jobmanager.rpc.port: 6123
    
    # JobManager JVM heap 内存大小
    jobmanager.bind-host: 0.0.0.0
    jobmanager.memory.process.size: 2048m
     
    # TaskManager JVM heap 内存大小
    taskmanager.host: master
    taskmanager.bind-host: 0.0.0.0
    taskmanager.memory.flink.size: 2048m
    taskmanager.memory.process.size: 2048m
    
    # 每个 TaskManager 提供的任务 slots 数量大小
    taskmanager.numberOfTaskSlots: 4
     
    # 程序默认并行计算的个数
    parallelism.default: 4
     
    # 文件系统来源
    fs.default-scheme: hdfs://master:9000
    
    # ====================================== 高可用性配置 ======================================= #
    # 高可用性配置，可以选择 'NONE' 或者 'zookeeper'.
    # high-availability: zookeeper
     
    # 文件系统路径，让 Flink 在高可用性设置中持久保存元数据
    # high-availability.storageDir: hdfs:///flink/meta/
     
    # zookeeper 集群中仲裁者的机器 ip 和 port 端口号
    # high-availability.zookeeper.quorum: slaver1:2181,slaver2:2181,slaver3:2181
     
    # 默认是 open，如果 zookeeper security 启用了该值会更改成 creator
    # high-availability.zookeeper.client.acl: open
    
    # ===================================== 容错和检查点 配置 ===================================== #
    # 用于存储和检查点状态
    state.backend: hashmap
     
    # 存储检查点的数据文件和元数据的默认目录
    state.checkpoints.dir: hdfs://master:9000/flink/checkpoints
     
    # savepoints 的默认目标目录(可选)
    state.savepoints.dir: hdfs://master:9000/flink/savepoints
     
    # 用于启用/禁用增量 checkpoints 的标志
    state.backend.incremental: false
    
    jobmanager.execution.failover-strategy: region
    
    # ==================================== WEB UI 前端配置 ==================================== #
    # Web 的运行时监视器端口
    rest.port: 8081
    
    # 基于 Web 的运行时监视器侦听的地址.
    rest.address: master
    
    rest.bind-port: 8081
    rest.bind-address: 0.0.0.0
    
    # 是否从基于 Web 的 jobmanager 启用作业提交
    # web.submit.enable: false
    # web.cancel.enable: false
    
    # ======================================== 高级配置 ======================================= #
    # io.tmp.dirs: /opt/apache/flink/data/tmp
    
    # 是否应在 TaskManager 启动时预先分配 TaskManager 管理的内存
    # taskmanager.memory.preallocate: false
     
    # 类加载解析顺序，是先检查用户代码 jar（“child-first”）还是应用程序类路径（“parent-first”）。 默认设置指示首先从用户代码 jar 加载类
    # classloader.resolve-order: child-first
      
    # 用于网络缓冲区的 JVM 内存的分数:决定 TaskManager 可以同时拥有多少流数据交换通道以及通道缓冲的程度 
    # 如果作业被拒绝或者您收到系统没有足够缓冲区的警告，请增加此值或下面的最小/最大值
    #     taskmanager.network.memory.min 和 taskmanager.network.memory.max 可能会覆盖此分数
    # taskmanager.network.memory.fraction: 0.1
    # taskmanager.network.memory.min: 67108864
    # taskmanager.network.memory.max: 1073741824
    
    # ======================================= 集群安全配置 ====================================== #
    # 指示是否从 Kerberos ticket 缓存中读取
    # security.kerberos.login.use-ticket-cache: true
     
    # 包含用户凭据的 Kerberos 密钥表文件的绝对路径
    # security.kerberos.login.keytab: /path/to/kerberos/keytab
     
    # 与 keytab 关联的 Kerberos 主体名称
    # security.kerberos.login.principal: flink-user
     
    # 以逗号分隔的登录上下文列表，用于提供 Kerberos 凭据（例如，`Client，KafkaClient`使用凭证进行 ZooKeeper 身份验证和 Kafka 身份验证）
    # security.kerberos.login.contexts: Client,KafkaClient
    
    # ================================== Zookeeper 集群安全配置 ================================= #
    # 覆盖以下配置以提供自定义 ZK 服务名称
    # zookeeper.sasl.service-name: zookeeper
     
    # 该配置必须匹配 "security.kerberos.login.contexts" 中的列表（含有一个）
    # zookeeper.sasl.login-context-name: Client
    
    # ===================================== 历史服务器配置 ====================================== #
    historyserver.web.address: 0.0.0.0
    historyserver.web.port: 8082
    historyserver.archive.fs.dir: hdfs:///flink/history/
    historyserver.archive.fs.refresh-interval: 10000
```

#### 11.4.2 修改 ${FLINK_HOME}/conf/masters

```bash
    master:8081
```

#### 11.4.3 修改 ${FLINK_HOME}/conf/workers

```bash
    slaver1
    slaver2
    slaver3
```


### 11.5 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 flink 安装父路径
    ~/shell/xync.sh flink                                                      # 同步 flink 到其它节点
````

### 11.6 编写 flink 启停脚本：${FLINK_HOME}/bin/flink.sh

```bash
    cd /opt/apache/flink/bin/                                                  # 切换到 flink 安装目录的 bin
    touch ${FLINK_HOME}/bin/flink.sh                                           # 脚本内容，详见：/bigdata-deploy/shell/apache/flink.sh
    chmod +x ${FLINK_HOME}/bin/flink.sh                                        # 添加可执行权限
```

### 11.7 启动 flink

```bash
    ${ZOOKEEPER_HOME}/bin/zookeeper.sh start                                   # 启动 zookeeper
    ${HADOOP_HOME}/bin/hadoop.sh start                                         # 启动 hadoop
    
    cd /opt/apache/flink/ || exit                                              # 切换的 flink 安装路径
    
    # 启动集群
    ${FLINK_HOME}/bin/start-cluster.sh                                         # 启动flink 集群
    ${FLINK_HOME}/bin/flink.sh start                                           # 启动flink 集群，并检查启动状况
```

### 9.8 查看 flink 启动结果并测试

```bash
    # 查看各节点状态
    jps -l 
    ${FLINK_HOME}/bin/hbase.sh status                                          # 在每个节点执行查看
    ${ISSAC_HOME}/shell/xcall.sh "jps -l"                                      # 在主节点 master 执行查看
    
    http://master:8083                                                         # 查看 flink WEB UI
    
    ${FLINK_HOME}/bin/flink run ${FLINK_HOME}/examples/batch/WordCount.jar \
                            --input hdfs://master:9000/hadoop/test/wc/input \
                            --output hdfs://master:9000/hadoop/test/wc/flink
```



## 12. Flume 安装

### 12.1 Flume-1.11.0 下载

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[flume-1.11.0](https://mirrors.aliyun.com/apache/flume/1.11.0/apache-flume-1.11.0-bin.tar.gz)** 安装包到本地

### 12.2 解压安装 flume

```bash
    tar -zxvf apache-flume-1.11.0-bin.tar.gz -C /opt/apache/                   # 解压 压缩包
    mv /opt/apache/apache-flume-1.11.0-bin /opt/apache/flume                   # 修改目录名称
    mkdir -p /opt/apache/flume/logs/                                           # 创建日志目录
```

### 12.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== flume 1.11.0 ====================================== #
        export FLUME_HOME=/opt/apache/flume
        export PATH=${PATH}:${FLUME_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 12.4 配置文件修改

#### 12.4.1 创建 ${FLUME_HOME}/conf/flume-env.sh 

```bash
    # 添加如下内容：
    export JAVA_HOME=/opt/java/jdk-08
    export JAVA_OPTS="-Xms256m -Xmx512m -Dcom.sun.management.jmxremote"
    # FLUME_CLASSPATH=""
```

#### 12.4.2 创建 ${FLUME_HOME}/bin/flume-ng

```bash
    # 修改 229 行（若在 ${FLUME_HOME}/conf/flume-env.sh 修改过该参数，此处需忽略）
    JAVA_OPTS="-Xms256m -Xmx512m"
```

#### 12.4.3 修改 ${FLUME_HOME}/bin/log4j2.xml

```xml
    <!-- 修改 21 行日志存储目录 -->
    <Property name="LOG_DIR">/opt/apache/flume/logs</Property>
```

#### 12.4.4 配置与 hadoop 兼容性问题

```bash
    # 解决与 hadoop 的 guava 依赖
    rm ${FLUME_HOME}/lib/guava-11.0.2.jar                                      # 删除 flume 旧版本的 guava
    cp -fr ${HADOOP_HOME}/share/hadoop/common/lib/guava-27.0-jre.jar ${FLUME_HOME}/lib/  # 和 hadoop 保持一致
    
    # 复制 hadoop 的 jar 用于支持 HDFS 的读写（flume-1.9.0 之后版本无需复制 jar）
    cp -fr ${HADOOP_HOME}/share/hadoop/common/*.jar     ${FLUME_HOME}/lib/
    cp -fr ${HADOOP_HOME}/share/hadoop/common/lib/*.jar ${FLUME_HOME}/lib/
    cp -fr ${HADOOP_HOME}/share/hadoop/hdfs/*.jar       ${FLUME_HOME}/lib/
    cp -fr ${HADOOP_HOME}/share/hadoop/hdfs/lib/*.jar   ${FLUME_HOME}/lib/
    
    # 复制 hadoop 的 配置文件 用于支持 HDFS 的读写（使用时，可在配置文件中指定，非必须配置）
    cp -fr ${HADOOP_HOME}/etc/hadoop/core-site.xml      ${FLUME_HOME}/conf/
    cp -fr ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml      ${FLUME_HOME}/conf/
```

#### 12.4.5 创建 ${FLUME_HOME}/conf/flume-conf.properties

```properties
    # 添加如下内容：
    ############################## agent 中各组件的名字 ##############################
    ## agent 中的 source 组件
    a1.sources = r1
    ## agent 中的下沉组件 sink
    a1.sinks = k1
    ## agent 内部的数据传输通道 channel，用于从 source 将数据传递到 sink
    a1.channels = c1
     
    ########################### 描述和配置 source 组件：r1 ############################
    ## netcat 用于监听一个端口的
    a1.sources.r1.type = netcat
    ## 配置的绑定地址,这个机器的 hostname 是 master，所以下面也可以配置成 master
    a1.sources.r1.bind = master
    ## 配置的绑定端口
    a1.sources.r1.port = 44444
     
    # 描述和配置sink组件：k1
    a1.sinks.k1.type = logger
     
    ######################## 描述和配置 channel 组件，此处使用内存 ########################
    ## 缓存到内存中，如果是文件，可以使用 file 类型
    a1.channels.c1.type = memory
    ## 使用的空间
    a1.channels.c1.capacity = 1000
    ## 事务使用的空间
    a1.channels.c1.transactionCapacity = 100
    
    ########################## source channel sink之间的连接关系 ##########################
    a1.sources.r1.channels = c1
    a1.sinks.k1.channel = c1
```

### 12.5 测试
```bash
    sudo dnf install -y telnet                                                 # 安装 telnet
    
    ${FLUME_HOME}/bin/flume-ng version                                         # 查看 flume 版本号
    
    # 启动 flume 进行测试
    ${FLUME_HOME}/bin/flume-ng agent -c conf -f ${FLUME_HOME}/conf/flume-conf.properties -n a1 -Dflume.root.logger=INFO,console
    
    telnet master 44444                                                        # 新建终端，并启动 telnet，然后使用键盘进行输入
    less ${FLUME_HOME}/logs/flume.log                                          # 查看日志中是否输出相关信息
    
    ~/shell/xync.sh /opt/apache/flume                                          # 测试完毕，并成功后，进行分发
```


## 13. Doris 安装

### 13.1 Doris-1.1.5 下载

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[doris-fe-1.1.15](https://mirrors.aliyun.com/apache/doris/1.1/1.1.5-rc02/apache-doris-fe-1.1.5-bin.tar.gz)** 和 **[doris-be-1.1.15](https://mirrors.aliyun.com/apache/doris/1.1/1.1.5-rc02/apache-doris-be-1.1.5-bin-x86_64.tar.gz)**  安装包到本地

### 13.2 解压安装 flume

```bash
    mkdir -p /opt/apache/doris                                                 # 创建 doris 安装目录
    
    tar -zxvf apache-doris-fe-1.1.5-bin.tar.gz        -C /opt/apache/doris     # 解压 fe 压缩包
    tar -zxvf apache-doris-be-1.1.5-bin-x86_64.tar.gz -C /opt/apache/doris     # 解压 be 压缩包
    mv apache-doris-fe-1.1.5-bin        /opt/apache/doris/fe                   # 修改 fe 目录名称
    mv apache-doris-be-1.1.5-bin-x86_64 /opt/apache/doris/be                   # 修改 be 目录名称
    
    mkdir -p /opt/apache/doris/fe/data                                         # 创建日志目录
    mkdir -p /opt/apache/doris/be/data                                         # 创建日志目录
```

### 13.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== dpris 1.1.5 ====================================== #
        export DORIS_FE_HOME=/opt/apache/doris/fe
        export DORIS_BE_HOME=/opt/apache/doris/be
        export PATH=${PATH}:${DORIS_FE_HOME}/bin:${DORIS_BE_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 13.4 配置文件修改

#### 13.4.1 创建 ${DORIS_FE_HOME}/conf/fe.conf

```bash
    LOG_DIR = /opt/apache/doris/fe/log
    
    JAVA_OPTS="-Xmx4096m -XX:+UseMembar -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=7 -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:-CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction=80 -XX:SoftRefLRUPolicyMSPerMB=0 -Xloggc:$DORIS_FE_HOME/log/fe.gc.log.$DATE"
    
    JAVA_OPTS_FOR_JDK_9="-Xmx4096m -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=7 -XX:+CMSClassUnloadingEnabled -XX:-CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction=80 -XX:SoftRefLRUPolicyMSPerMB=0 -Xlog:gc*:$DORIS_FE_HOME/log/fe.gc.log.$DATE:time"
    
    # 添加支持 https
    enable_http_server_v2 = true
    dynamic_partition_enable = true
    
    priority_networks = 192.168.0.0/16
    
    meta_dir = /opt/apache/doris/fe/data
    
    dynamic_partition_enable = true                                            # 增加动态分区配置
    lower_case_table_names = 1                                                 # 表名大小写不敏感
```

#### 13.4.2 创建 ${DORIS_BE_HOME}/conf/be.conf

```bash
    PPROF_TMPDIR = /opt/apache/doris/be/log
    
    priority_networks = 192.168.0.0/16
    
    storage_root_path = /opt/apache/doris/be/data
    
    sys_log_dir = /opt/apache/doris/be/log
    
    max_tablet_version_num = 5000
    max_compaction_threads = 16
    compaction_task_num_per_disk = 8
    cumulative_size_based_promotion_size_mbytes = 2048
    row_step_for_compaction_merge_log = 1
    total_permits_for_compaction_score = 50000
    
    # 表明大小写不敏感
    lower_case_table_names = 1
```

### 13.5 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 doris 安装父路径
    ~/shell/xync.sh doris                                                      # 同步 doris 到其它节点
````
