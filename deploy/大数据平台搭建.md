## 1. Java、Maven、Gradle、Mysql 安装配置

### 1.1 安装 Java 并配置环境变量

#### 1.1.1 卸载系统自带的 OpenJdk

```bash
    # ubuntu 卸载系统自带 open-jdk
    sudo apt list --installed | grep openjdk                                   # 查看安装的 openJDK 
    sudo apt remove openjdk*                                                   # 直接卸载
    sudo apt-get --purge remove                                                # 卸载并且移除相关依赖
    
    # redhat 卸载系统自带 open-jdk
    rpm -qa | grep java                                                        # 查看安装的 java，或者： yum list installed |grep java 
    rpm -qa | grep jdk                                                         # 查看安装的 jdk， 或者： yum list installed |grep jdk
    rpm -e --nodeps java-*                                                     # 卸载 java-*，    或者： yum -y remove java-*
    rpm -e --nodeps jdk-*                                                      # 卸载 jdk-*，     或者： yum -y remove jdk-*
    
    # redhat 8 系列默认会安装 openjdk,redhat 9 系列默认不会安装任何相关 jdk
    
```

#### 1.1.2 Java 下载

  从 [**Oracle 官网**](https://www.oracle.com/java/technologies/downloads/#java8) 下载 [**JDK 1.8**](https://download.oracle.com/otn/java/jdk/8u361-b09/0ae14417abb444ebb02b9815e2103550/jdk-8u361-linux-x64.tar.gz) 到本地

#### 1.1.3 解压安装

```bash
    tar -zxvf jdk-8u361-linux-x64.tar.gz -C /opt/java/                         # 解压下载的 JDK-1.8 压缩包
    cd /opt/java/                                                              # 切换到解压目录
    mv jdk1.8.0_361/ jdk-08/                                                   # 修改目录名称
```

#### 1.1.4 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ====================================== JDK-1.8.351 ====================================== #
            export JAVA_HOME=/opt/java/jdk-08
            export JRE_HOME=${JAVA_HOME}/jre
            export CLASSPATH=.:${CLASSPATH}:${JAVA_HOME}/lib:${JRE_HOME}/lib
            export PATH=${PATH}:${JAVA_HOME}/bin:${JRE_HOME}/bin
    
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

#### 1.1.5 测试安装

```bash
    # 刷新环境变量
    source /etc/profile                                                        # 或者： . /etc/profile
    
    # 验证安装结果
    java  -version
    javac -version
```

### 1.2 安装 Scala 并配置环境变量

#### 1.2.1 下载 scala-2.12.17 

  从 [**scala 官网**](https://scala-lang.org/) 下载 [**scala-2.12.17**](https://downloads.lightbend.com/scala/2.12.17/scala-2.12.17.tgz) 压缩包 到本地

#### 1.2.2 解压安装

```bash
    tar -zxvf scala-2.12.17.tgz -C /opt/java/                                  # 解压下载的 scala-2.12.17 压缩包
    cd /opt/java/                                                              # 切换到解压目录
    mv /opt/java/scala-2.12.17 /opt/java/scala-212/                            # 修改目录名称
```

#### 1.2.3 配置环境变量

```bash
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Scala 2.12.17 ====================================== #
        export SCALA_HOME=/opt/java/scala-212/
        export PATH=${PATH}:${SCALA_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

#### 1.2.4 验证安装

```bash
    scala -version                                                             # 查看安装的版本
    scala                                                                      # 进入交互界面
    println("hello world")                                                     # 打印数据
```

### 1.3 安装 Python 并配置环境变量


### 1.4 安装 Maven 并配置环境变量

#### 1.4.1 Maven 下载

  从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[maven-3.6.3](https://mirrors.aliyun.com/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz)** 到本地

#### 1.4.2 解压安装

```bash
    tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/apache/                    # 解压下载的 maven-3.6.3 压缩包
    cd /opt/apache/                                                            # 切换到解压目录
    mv apache-maven-3.6.3/ maven/                                              # 修改目录名称
```

#### 1.4.3 修改 ${MAVEN_HOME}/conf/settings.xml 配置文件

```xml 
    <localRepository>/opt/apache/maven/data</localRepository>
    
    <pluginGroups></pluginGroups>
    <proxies></proxies>
    <servers></servers>
    
    <mirrors>
        <!--阿里云-->
        <mirror>
            <id>alimaven</id>
            <mirrorOf>*</mirrorOf>
            <name>阿里云公共仓库</name>
            <url>https://maven.aliyun.com/repository/public</url>
        </mirror>
    </mirrors>
    
    <repositories>
        <repository>
            <id>spring</id>
            <url>https://maven.aliyun.com/repository/spring</url>
            <releases><enabled>true</enabled></releases>
            <snapshots><enabled>true</enabled></snapshots>
        </repository>
    </repositories>
    
    <profiles>
        <profile>
            <id>jdk8</id>
            <activation>
                <activeByDefault>true</activeByDefault>
                <jdk>1.8</jdk>
            </activation>
            <properties>
                <maven.compiler.source>1.8</maven.compiler.source>
                <maven.compiler.target>1.8</maven.compiler.target>
                <maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion>
            </properties>
        </profile>
    </profiles>
```

#### 1.4.4 配置环境变量

```bash
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Maven 3.6.3 ====================================== #
        export MAVEN_HOME=/opt/apache/maven
        export PATH=${PATH}:${MAVEN_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

#### 1.4.5 测试安装

```bash
    mvn -v
```

### 1.5 安装 Gradle 并配置环境变量

#### 1.5.1 Gradle 下载

从 [**gradle 官网**](https://gradle.org/) 下载 **[gradle-7.4.2](https://downloads.gradle-dn.com/distributions/gradle-7.4.2-bin.zip)** 到本地

#### 1.5.2 解压安装

```bash
    unzip gradle-7.4.2-bin.zip -d /opt/apache/                                 # 解压下载的 gradle-7.4.2 压缩包
    cd /opt/apache/                                                            # 切换到解压目录
    mv /opt/apache/gradle-7.4.2/ /opt/apache/gradle/                           # 修改目录名称
```

#### 1.5.3 配置环境变量

```bash
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Gradle 7.4.2 ====================================== #
        export GRADLE_HOME=/opt/apache/gradle
        export PATH=${PATH}:${GRADLE_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

### 1.5.4 修改配置文件

```bash
    cd /opt/apache/gradle/                                                     # 切换到 gradle 安装目录
    touch /opt/apache/gradle/gradle.properties                                 # 创建 gradle 配置文件，内容如 1.5.5
    touch /opt/apache/gradle/init.gradle                                       # 配置 gradle 为 阿里镜像源，内容如 1.5.6
    cp /opt/apache/gradle/init.gradle /opt/apache/gradle/init.d/init.gradle    # 配置 gradle 为 阿里镜像源
```

#### 1.5.5 ${GRADLE_HOME}/gradle.properties

```properties
    systemProp.org.gradle.internal.http.socketTimeout=360000
    systemProp.org.gradle.internal.http.connectionTimeout=360000
    
    # 开启线程守护，第一次编译时开线程，之后就不会再开了
    org.gradle.daemon=true
    
    # 配置编译时的虚拟机大小
    org.gradle.jvmargs=-Xmx2048m -XX:MaxPermSize=512m -XX:+HeapDumpOnOutOfMemoryError -Dfile.encoding=UTF-8
    
    # 开启并行编译，相当于多条线程再走
    org.gradle.parallel=true
    
    # 启用新的孵化模式
    org.gradle.configureondemand=true
```

#### 1.5.6 ${GRADLE_HOME}/init.gradle

```groovy
    allprojects {
        repositories {
            def ALIYUN_PUBLIC_URL = 'https://maven.aliyun.com/repository/public/'
            def ALIYUN_REPOSITORY_URL = 'https://maven.aliyun.com/nexus/content/groups/public'
            def ALIYUN_JCENTER_URL = 'https://maven.aliyun.com/nexus/content/repositories/jcenter'
            all { ArtifactRepository repo ->
                if(repo instanceof MavenArtifactRepository){
                    def url = repo.url.toString()
                    if (url.startsWith('https://repo1.maven.org/maven2')) {
                        project.logger.lifecycle "Repository ${repo.url} replaced by $ALIYUN_REPOSITORY_URL."
                        remove repo
                    }
                    if (url.startsWith('https://jcenter.bintray.com/')) {
                        project.logger.lifecycle "Repository ${repo.url} replaced by $ALIYUN_JCENTER_URL."
                        remove repo
                    }
                }
            }
            
            maven {
                url ALIYUN_PUBLIC_URL
                url ALIYUN_REPOSITORY_URL
                url ALIYUN_JCENTER_URL
            }
            
            mavenLocal()
            mavenCentral()
        }
    }
```

#### 1.5.7 测试安装

```bash
    gradle -v
```

### 1.6 安装 Mysql 并配置环境变量

#### 1.6.1 卸载系统自带的 MariaDB

```bash
    # ubuntu 卸载系统自带 MariaDB
    sudo apt list --installed | grep -iE "maria|mysql"                         # 查看安装的 MariaDB 
    sudo apt purge maria*                                                      # 卸载 mariadb
    sudo apt remove maria*                                                     # 直接卸载 mariadb
    sudo apt purge mysql*                                                      # 卸载 mysql
    sudo apt remove mysql*                                                     # 直接卸载 mysql
    sudo dpkg -l | grep ^rc | awk '{print $2}' | sudo xargs dpkg -P            # 清理残留的数据
    sudo apt-get --purge remove                                                # 卸载并且移除相关依赖
    sudo rm -rf /etc/mysql/ /var/lib/mysql                                     # 删除配置文件
    sudo apt autoremove                                                        # 自动移除残留
    sudo apt autoclean                                                         # 自动清理残留
    
    # redhat 卸载系统自带 MariaDB
    sudo rpm -qa | grep maria                                                  # 查看安装的 MariaDB，或者： yum list installed | grep maria 
    sudo rpm -qa | grep mysql                                                  # 查看安装的 myusql， 或者： yum list installed | grep mysql
    sudo rpm -e --nodeps maria-*                                               # 卸载 MariaDB，      或者： yum -y remove maria*
    sudo rpm -e --nodeps mysql-*                                               # 卸载 Mysql，        或者： yum -y remove mysql*
    sudo rm -rf /etc/mysql/ /etc/my.cnf /var/lib/mysql                         # 删除配置文件
    sudo rm -rf /etc/mysql/ /var/lib/mysql                                     # 删除配置文件
```

#### 1.6.2 Mysql 下载与安装

从 [**Mysql 官网**](https://www.mysql.com/) 下载 **[mysql-8.0.31](https://downloads.mysql.com/archives/get/p/23/file/mysql-8.0.31-linux-glibc2.12-x86_64.tar.xz)** 到本地

#### 1.6.3 解压安装

```bash
    sudo apt -y install libaio* libaio-dev*                                    # 安装必要的系统依赖包
    
    tar -Jxvf mysql-8.0.31-linux-glibc2.12-x86_64.tar.xz -C /opt/db/           # 解压下载的 mysql-8.0.31 压缩包
    cd /opt/db/                                                                # 切换到解压目录
    mv /opt/db/mysql-8.0.31-linux-glibc2.12-x86_64/ /opt/db/mysql/             # 修改目录名称
    mkdir -p /opt/db/mysql/data/                                               # 创建 Mysql 数据存储目录
    mkdir -p /opt/db/mysql/bin-log/                                            # 创建 Mysql bin-log 数据存储目录
    mkdir -p /opt/db/mysql/tmp/                                                # 创建 Mysql 临时文件目录
    mkdir -p /opt/db/mysql/logs/                                               # 创建 Mysql 日志存储目录
    mkdir -p /etc/mysql/                                                       # 创建 Mysql 配置文件目录
    
    touch /etc/mysql/my.cnf                                                    # 创建 Mysql 配置文件，内容如 1.6.4
    cp /etc/mysql/my.cnf /opt/db/mysql/docs/my.cnf                             # 备份配置文件
    sudo chown issac:issac -R /opt/db/mysql/                                   # 将安装目录 mysql 授权给 当前用户
    chmod -R 771 /opt/db/mysql/data/                                           # 修改 数据存储目录 授权
    chmod -R 771 /opt/db/mysql/bin-log/                                        # 修改 bin-log 数据存储目录 授权
    chmod -R 777 /opt/db/mysql/tmp/                                            # 修改 临时文件目录 授权
     
    # 解决 libtinfo.so.5 动态链接库缺失问题
    sudo ln -s /usr/lib/x86_64-linux-gnu/libtinfo.so.6.2 /usr/lib/x86_64-linux-gnu/libtinfo.so.5   # ubuntu
    
    # 解决 libtinfo.so.5 动态链接库缺失问题
    sudo ln -s /usr/lib64/libtinfo.so.6.2 /usr/lib64/libtinfo.so.5             # redhat 
```

#### 1.6.4 修改配置文件（redhat：/etc/my.cnf；ubuntu：/etc/my.cnf ）

```ini
    [client]
    port                    = 3306
    socket                  = /opt/db/mysql/tmp/mysql.sock
    default-character-set   = utf8mb4
    
    
    [mysql]
    show-warnings
    default-character-set = utf8mb4
    socket                   = /opt/db/mysql/tmp/mysql.sock
    
    
    [mysqldump]
    quick
    quote-names
    max_allowed_packet       = 16M
    
    
    [mysqld]
    bind-address             = 0.0.0.0
    port                     = 3306
    user                     = issac
    
    # Mysql服务的唯一编号 每个 mysql 服务 Id 需唯一
    server-id                = 1
    
    basedir                  = /opt/db/mysql
    datadir                  = /opt/db/mysql/data
    pid-file                 = /opt/db/mysql/tmp/mysqld.pid
    socket                   = /opt/db/mysql/tmp/mysql.sock
    # 临时目录 比如 load data infile 会用到
    tmpdir                   = /opt/db/mysql/tmp
    log-error                = /opt/db/mysql/logs/error.log
    
    character-set-server     = utf8mb4
    skip_name_resolve        = 1
    
    lock_wait_timeout        = 3600
    open_files_limit         = 65535
    back_log                 = 1024
    max_connections          = 512
    max_connect_errors       = 1000000
    table_open_cache         = 1024
    table_definition_cache   = 1024
    thread_stack             = 512K
    sort_buffer_size         = 32M
    join_buffer_size         = 64M
    read_buffer_size         = 128M
    read_rnd_buffer_size     = 16M
    bulk_insert_buffer_size  = 128M
    thread_cache_size        = 768
    interactive_timeout      = 600
    wait_timeout             = 600
    tmp_table_size           = 64M
    max_heap_table_size      = 32M
    # query_cache_size       = 0
    
    key_buffer_size          = 32M
    myisam_sort_buffer_size  = 128M
    
    default-storage-engine             = INNODB
    innodb_buffer_pool_size            = 512M
    innodb_buffer_pool_instances       = 4
    # innodb_data_file_path            = ibdata1:12M:autoextend
    innodb_flush_log_at_trx_commit     = 1
    innodb_log_buffer_size             = 32M
    innodb_log_file_size               = 256M
    innodb_log_files_in_group          = 3
    innodb_max_undo_log_size           = 1G
    innodb_io_capacity                 = 400
    innodb_io_capacity_max             = 800
    innodb_open_files                  = 65535
    innodb_flush_method                = O_DIRECT
    innodb_lru_scan_depth              = 4000
    innodb_lock_wait_timeout           = 10
    innodb_rollback_on_timeout         = 1
    innodb_print_all_deadlocks         = 1
    innodb_online_alter_log_max_size   = 4G
    innodb_status_file                 = 1
    innodb_status_output               = 0
    innodb_status_output_locks         = 1
    innodb_sort_buffer_size            = 67108864
    innodb_adaptive_hash_index         = OFF
    
    # log_error_verbosity              = 3
    # slow_query_log                   = 1
    slow_query_log_file                = /opt/db/mysql/logs/slow.log
    # long_query_time                  = 0.1
    # log_queries_not_using_indexes    = 1
    # log_throttle_queries_not_using_indexes = 60
    min_examined_row_limit             = 100
    log_slow_admin_statements          = 1
    log_slow_slave_statements          = 1
    log-bin                            = /opt/db/mysql/bin-log/mysql
    binlog_format                      = ROW
    sync_binlog                        = 1
    binlog_cache_size                  = 16M
    max_binlog_cache_size              = 2G     
    max_binlog_size                    = 1G
    binlog_rows_query_log_events       = 1
    binlog_checksum                    = CRC32
    gtid_mode                          = ON
    enforce_gtid_consistency           = TRUE
    # 大小写不敏感
    lower_case_table_names             = 1
```

#### 1.6.5 修改 ${MYSQL_HOME}/support-files/mysql.server

```bashpro shell script 
    #!/bin/sh
    
    
    basedir=/opt/db/mysql
    datadir=/opt/db/mysql/data
    
    # Default value, in seconds, afterwhich the script should timeout waiting
    # for server start. 
    # Value here is overriden by value in my.cnf. 
    # 0 means don't wait at all
    # Negative numbers mean to wait indefinitely
    service_startup_timeout=900
    
    # Lock directory for RedHat / SuSE.
    lockdir='/var/lock/subsys'
    lock_file_path="$lockdir/mysql"
    
    # The following variables are only set for letting mysql.server find things.
    
    # Set some defaults
    mysqld_pid_file_path=
    if test -z "$basedir"
    then
      basedir=/opt/db/mysql
      bindir=/opt/db/mysql/bin
      if test -z "$datadir"
      then
        datadir=/opt/db/mysql/data
      fi
      sbindir=/opt/db/mysql/bin
      libexecdir=/opt/db/mysql/bin
    else
      bindir="$basedir/bin"
      if test -z "$datadir"
      then
        datadir="$basedir/data"
      fi
      sbindir="$basedir/sbin"
      libexecdir="$basedir/libexec"
    fi
    
    # datadir_set is used to determine if datadir was set (and so should be
    # *not* set inside of the --basedir= handler.)
    datadir_set=
    
    #
    # Use LSB init script functions for printing messages, if possible
    #
    lsb_functions="/lib/lsb/init-functions"
    if test -f $lsb_functions ; then
      . $lsb_functions
    else
      log_success_msg()
      {
        echo " SUCCESS! $@"
      }
      log_failure_msg()
      {
        echo " ERROR! $@"
      }
    fi
    
    PATH="/sbin:/usr/sbin:/bin:/usr/bin:$basedir/bin"
    export PATH
    
    mode=$1    # start or stop
    
    [ $# -ge 1 ] && shift
    
    
    other_args="$*"   # uncommon, but needed when called from an RPM upgrade action
               # Expected: "--skip-networking --skip-grant-tables"
               # They are not checked here, intentionally, as it is the resposibility
               # of the "spec" file author to give correct arguments only.
    
    case `echo "testing\c"`,`echo -n testing` in
        *c*,-n*) echo_n=   echo_c=     ;;
        *c*,*)   echo_n=-n echo_c=     ;;
        *)       echo_n=   echo_c='\c' ;;
    esac
    
    parse_server_arguments() {
      for arg do
        case "$arg" in
          --basedir=*)  basedir=`echo "$arg" | sed -e 's/^[^=]*=//'`
                        bindir="$basedir/bin"
                if test -z "$datadir_set"; then
                  datadir="$basedir/data"
                fi
                sbindir="$basedir/sbin"
                libexecdir="$basedir/libexec"
            ;;
          --datadir=*)  datadir=`echo "$arg" | sed -e 's/^[^=]*=//'`
                datadir_set=1
        ;;
          --pid-file=*) mysqld_pid_file_path=`echo "$arg" | sed -e 's/^[^=]*=//'` ;;
          --service-startup-timeout=*) service_startup_timeout=`echo "$arg" | sed -e 's/^[^=]*=//'` ;;
        esac
      done
    }
    
    wait_for_pid () {
      verb="$1"           # created | removed
      pid="$2"            # process ID of the program operating on the pid-file
      pid_file_path="$3" # path to the PID file.
    
      i=0
      avoid_race_condition="by checking again"
    
      while test $i -ne $service_startup_timeout ; do
    
        case "$verb" in
          'created')
            # wait for a PID-file to pop into existence.
            test -s "$pid_file_path" && i='' && break
            ;;
          'removed')
            # wait for this PID-file to disappear
            test ! -s "$pid_file_path" && i='' && break
            ;;
          *)
            echo "wait_for_pid () usage: wait_for_pid created|removed pid pid_file_path"
            exit 1
            ;;
        esac
    
        # if server isn't running, then pid-file will never be updated
        if test -n "$pid"; then
          if kill -0 "$pid" 2>/dev/null; then
            :  # the server still runs
          else
            # The server may have exited between the last pid-file check and now.  
            if test -n "$avoid_race_condition"; then
              avoid_race_condition=""
              continue  # Check again.
            fi
    
            # there's nothing that will affect the file.
            log_failure_msg "The server quit without updating PID file ($pid_file_path)."
            return 1  # not waiting any more.
          fi
        fi
    
        echo $echo_n ".$echo_c"
        i=`expr $i + 1`
        sleep 1
    
      done
    
      if test -z "$i" ; then
        log_success_msg
        return 0
      else
        log_failure_msg
        return 1
      fi
    }
    
    # Get arguments from the my.cnf file,
    # the only group, which is read from now on is [mysqld]
    if test -x "$bindir/my_print_defaults";  then
      print_defaults="$bindir/my_print_defaults"
    else
      # Try to find basedir in /etc/my.cnf
      conf=/etc/my.cnf
      print_defaults=
      if test -r $conf
      then
        subpat='^[^=]*basedir[^=]*=\(.*\)$'
        dirs=`sed -e "/$subpat/!d" -e 's//\1/' $conf`
        for d in $dirs
        do
          d=`echo $d | sed -e 's/[ 	]//g'`
          if test -x "$d/bin/my_print_defaults"
          then
            print_defaults="$d/bin/my_print_defaults"
            break
          fi
        done
      fi
    
      # Hope it's in the PATH ... but I doubt it
      test -z "$print_defaults" && print_defaults="my_print_defaults"
    fi
    
    #
    # Read defaults file from 'basedir'.   If there is no defaults file there
    # check if it's in the old (depricated) place (datadir) and read it from there
    #
    
    extra_args=""
    if test -r "$basedir/my.cnf"
    then
      extra_args="-e $basedir/my.cnf"
    fi
    
    parse_server_arguments `$print_defaults $extra_args mysqld server mysql_server mysql.server`
    
    #
    # Set pid file if not given
    #
    if test -z "$mysqld_pid_file_path"
    then
      mysqld_pid_file_path=$datadir/`hostname`.pid
    else
      case "$mysqld_pid_file_path" in
        /* ) ;;
        * )  mysqld_pid_file_path="$datadir/$mysqld_pid_file_path" ;;
      esac
    fi
    
    case "$mode" in
      'start')
        # Start daemon
    
        # Safeguard (relative paths, core dumps..)
        cd $basedir
    
        echo $echo_n "Starting MySQL"
        if test -x $bindir/mysqld_safe
        then
          # Give extra arguments to mysqld with the my.cnf file. This script
          # may be overwritten at next upgrade.
          $bindir/mysqld_safe --datadir="$datadir" --pid-file="$mysqld_pid_file_path" $other_args >/dev/null &
          wait_for_pid created "$!" "$mysqld_pid_file_path"; return_value=$?
    
          # Make lock for RedHat / SuSE
          if test -w "$lockdir"
          then
            touch "$lock_file_path"
          fi
    
          exit $return_value
        else
          log_failure_msg "Couldn't find MySQL server ($bindir/mysqld_safe)"
        fi
        ;;
    
      'stop')
        # Stop daemon. We use a signal here to avoid having to know the
        # root password.
    
        if test -s "$mysqld_pid_file_path"
        then
          # signal mysqld_safe that it needs to stop
          touch "$mysqld_pid_file_path.shutdown"
    
          mysqld_pid=`cat "$mysqld_pid_file_path"`
    
          if (kill -0 $mysqld_pid 2>/dev/null)
          then
            echo $echo_n "Shutting down MySQL"
            kill $mysqld_pid
            # mysqld should remove the pid file when it exits, so wait for it.
            wait_for_pid removed "$mysqld_pid" "$mysqld_pid_file_path"; return_value=$?
          else
            log_failure_msg "MySQL server process #$mysqld_pid is not running!"
            rm "$mysqld_pid_file_path"
          fi
    
          # Delete lock for RedHat / SuSE
          if test -f "$lock_file_path"
          then
            rm -f "$lock_file_path"
          fi
          exit $return_value
        else
          log_failure_msg "MySQL server PID file could not be found!"
        fi
        ;;
    
      'restart')
        # Stop the service and regardless of whether it was
        # running or not, start it again.
        if $0 stop  $other_args; then
          $0 start $other_args
        else
          log_failure_msg "Failed to stop running server, so refusing to try to start."
          exit 1
        fi
        ;;
    
      'reload'|'force-reload')
        if test -s "$mysqld_pid_file_path" ; then
          read mysqld_pid <  "$mysqld_pid_file_path"
          kill -HUP $mysqld_pid && log_success_msg "Reloading service MySQL"
          touch "$mysqld_pid_file_path"
        else
          log_failure_msg "MySQL PID file could not be found!"
          exit 1
        fi
        ;;
      'status')
        # First, check to see if pid file exists
        if test -s "$mysqld_pid_file_path" ; then 
          read mysqld_pid < "$mysqld_pid_file_path"
          if kill -0 $mysqld_pid 2>/dev/null ; then 
            log_success_msg "MySQL running ($mysqld_pid)"
            exit 0
          else
            log_failure_msg "MySQL is not running, but PID file exists"
            exit 1
          fi
        else
          # Try to find appropriate mysqld process
          mysqld_pid=`pidof $libexecdir/mysqld`
    
          # test if multiple pids exist
          pid_count=`echo $mysqld_pid | wc -w`
          if test $pid_count -gt 1 ; then
            log_failure_msg "Multiple MySQL running but PID file could not be found ($mysqld_pid)"
            exit 5
          elif test -z $mysqld_pid ; then 
            if test -f "$lock_file_path" ; then 
              log_failure_msg "MySQL is not running, but lock file ($lock_file_path) exists"
              exit 2
            fi 
            log_failure_msg "MySQL is not running"
            exit 3
          else
            log_failure_msg "MySQL is running but PID file could not be found"
            exit 4
          fi
        fi
        ;;
        *)
          # usage
          basename=`basename "$0"`
          echo "Usage: $basename  {start|stop|restart|reload|force-reload|status}  [ MySQL server options ]"
          exit 1
        ;;
    esac
    
    exit 0
```

#### 1.6.6 修改 ${MYSQL_HOME}/support-files/mysqld_multi.server

```bashpro shell script
    #!/bin/sh
    
    basedir=/opt/db/mysql
    bindir=/opt/db/mysql/bin
    
    if test -x $bindir/mysqld_multi
    then
      mysqld_multi="$bindir/mysqld_multi";
    else
      echo "Can't execute $bindir/mysqld_multi from dir $basedir";
      exit;
    fi
    
    case "$1" in
        'start' )
            "$mysqld_multi" start $2
            ;;
        'stop' )
            "$mysqld_multi" stop $2
            ;;
        'report' )
            "$mysqld_multi" report $2
            ;;
        'restart' )
            "$mysqld_multi" stop $2
            "$mysqld_multi" start $2
            ;;
        *)
            echo "Usage: $0 {start|stop|report|restart}" >&2
            ;;
    esac
```

#### 1.6.7 编写 ${MYSQL_HOME}/bin/mysql.sh 启停脚本

```bashpro shell script
    #!/usr/bin/env bash
    # shellcheck disable=SC2009
    
    
    SERVICE_DIR=$(cd "$(dirname "$0")/../" || exit; pwd)
    SERVICE_NAME=Mysql
    MYSQL_SAFE=mysqld_safe
    MYSQLD=mysqld
    
    SERVICE_PORT=3306
    RUN=running
    STOP=is
    
    
    printf "\n=========================================================================\n"
    #    匹配输入参数
    case "$1" in
        # 1. 运行程序：running
        start)
            # 1.1 统计正在运行程序的 pid 的个数
            mysql_status=$("${SERVICE_DIR}/support-files/mysql.server" status | awk '{print $3}')
            mysql_pid=$(ps -aux | grep -i ${SERVICE_NAME} | grep -vi "$0" | grep -v grep | awk '{print $2}')
            
            # 1.2 若 Mysql 运行状态为停止，则运行程序，否则打印程序正在运行
            if [ "${mysql_status}" = "${STOP}" ] && [ ! "${mysql_pid}" ]; then
                echo "    程序（${SERVICE_NAME}）正在加载中 ......"
                "${SERVICE_DIR}/support-files/mysql.server" start > /dev/null 2>&1
                sleep 1
                echo "    程序（${SERVICE_NAME}）启动验证中 ...... "
                sleep 2
    
                # 1.3 判断程序 Mysqld_Safe 启动是否成功
                safe_pid=$(ps -aux | grep -i ${MYSQL_SAFE} |grep -vi "$0" | grep -v grep | awk '{print $2}')
                if [ ! "${safe_pid}" ]; then
                    echo "    程序（mysqld_safe）启动失败 ...... "
                fi
    
                # 1.4 判断程序 Mysqld 启动是否成功
                mysqld_pid=$(ps -aux | grep -i ${MYSQLD} |grep -vi "$0" | grep -v grep | awk '{print $2}')
                if [ ! "${mysqld_pid}" ]; then
                    echo "    程序（mysqld）启动失败 ...... "
                fi
    
                # 1.5 判断所有程序启动是否成功
                pid_count=$(ps -aux | grep -i ${SERVICE_NAME} |grep -vi "$0" | grep -v grep | awk '{print $2}' | wc -l)
                new_status=$("${SERVICE_DIR}/support-files/mysql.server" status | awk '{print $3}')
                if [ "${pid_count}" -ge 2 ] && [ "${new_status}" = "${RUN}" ]; then
                    echo "    程序（${SERVICE_NAME}）启动成功 ...... "
                else
                    echo "    程序（${SERVICE_NAME}）启动失败 ...... "
                fi
            else
                echo "    程序（${SERVICE_NAME}）正在运行当中 ...... "
            fi
        ;;
    
        # 2. 停止
        stop)
            pid_count=$(ps -aux | grep -i ${SERVICE_NAME} |grep -vi "$0" | grep -v grep | awk '{print $2}' | wc -l)
            new_status=$("${SERVICE_DIR}/support-files/mysql.server" status | awk '{print $3}')
            # 2.1 判断程序状态
            if [ "${pid_count}" -eq 0 ] && [ "${new_status}" = "${STOP}" ]; then
                echo "    程序（${SERVICE_NAME}）的进程不存在，程序没有运行 ...... "
            # 2.2 杀死进程，关闭程序
            elif [ "${pid_count}" -eq 2 ]; then
                echo "    程序（${SERVICE_NAME}）正在停止 ......"
                "${SERVICE_DIR}/support-files/mysql.server" stop > /dev/null 2>&1
                sleep 2
                echo "    程序（${SERVICE_NAME}）停止验证中 ......"
                sleep 3    
                
            # 2.3 若还未关闭，则强制杀死进程，关闭程序
                pid_count=$(ps -aux | grep -i ${SERVICE_NAME} |grep -vi "$0" | grep -v grep | awk '{print $2}' | wc -l)
                if [ "${pid_count}" -ge 1 ]; then 
                    "${SERVICE_DIR}/support-files/mysql.server" stop > /dev/null 2>&1
                    # temp=$(ps -aux | grep -i ${SERVICE_NAME} | grep -vi "$0" | grep -v grep | awk '{print $2}' | xargs kill -9)
                fi
    
                echo "    程序（${SERVICE_NAME}）已经停止成功 ......"
            else
                echo "    程序（${SERVICE_NAME}）运行出现问题 ......"
            fi
            ;;
    
        # 3. 状态查询
        status)
            # 3.1 查看正在运行程序的 pid
            pid_count=$(ps -aux | grep -i ${SERVICE_NAME} | grep -vi "$0" | grep -v grep | awk '{print $2}' | wc -l)
            new_status=$("${SERVICE_DIR}/support-files/mysql.server" status | awk '{print $3}')
            
            # 3.2 判断 Mysql 运行状态
            if [ "${pid_count}" -eq 0 ] && [ "${new_status}" = "${STOP}" ]; then
                echo "    程序（${SERVICE_NAME}）已经停止 ...... "
            elif [ "${pid_count}" -eq 2 ] && [ "${new_status}" = "${RUN}" ]; then
                echo "    程序（${SERVICE_NAME}）正在运行中 ...... "
            else
                echo "    程序（${SERVICE_NAME}）运行出现问题 ...... "
            fi
        ;;
    
        # 4. 重启程序
        restart)
            # 3.1 重启 Mysql
            echo "    程序（${SERVICE_NAME}）正在重启 ...... "
            "${SERVICE_DIR}/support-files/mysql.server" restart > /dev/null 2>&1
            sleep 3
                
            # 3.2 判断 Mysql 运行状态
            pid_count=$(ps -aux | grep -i ${SERVICE_NAME} | grep -vi "$0" | grep -v grep | awk '{print $2}' | wc -l)
            new_status=$("${SERVICE_DIR}/support-files/mysql.server" status | awk '{print $3}')
    
            if [ "${pid_count}" -eq 0 ] && [ "${new_status}" = "${STOP}" ]; then
                echo "    程序（${SERVICE_NAME}）重启失败 ...... "
            elif [ "${pid_count}" -eq 2 ] && [ "${new_status}" = "${RUN}" ]; then
                echo "    程序（${SERVICE_NAME}）重启成功 ...... "
            else
                echo "    程序（${SERVICE_NAME}）运行出现问题 ...... "
            fi
            ;;
    
        # 5. 重启加载配置文件
        reload)
            # 3.1 判断 Mysql 状态
            pid_count=$(ps -aux | grep -i ${SERVICE_NAME} | grep -vi "$0" | grep -v grep | awk '{print $2}' | wc -l)
            new_status=$("${SERVICE_DIR}/support-files/mysql.server" status | awk '{print $3}')
    
            if [ "${pid_count}" -eq 2 ] && [ "${new_status}" = "${RUN}" ]; then
                echo "    程序（${SERVICE_NAME}）正在重新加载配置文件 ...... "
                "${SERVICE_DIR}/support-files/mysql.server" reload > /dev/null 2>&1
                sleep 3
    
                pid_count=$(ps -aux | grep -i ${SERVICE_NAME} | grep -vi "$0" | grep -v grep | awk '{print $2}' | wc -l)
                new_status=$("${SERVICE_DIR}/support-files/mysql.server" status | awk '{print $3}')
    
                if [ "${pid_count}" -eq 2 ] && [ "${new_status}" = "${RUN}" ]; then
                    echo "    程序（${SERVICE_NAME}）重新加载配置文件成功 ...... "
                else
                    echo "    程序（${SERVICE_NAME}）重新加载配置文件启失败 ...... "
                fi
            else
                echo "    程序（${SERVICE_NAME}）已经停止，请先启动 ${SERVICE_NAME} ...... "
            fi
        ;;
    
        # 6. 其它情况
        *)
            echo "    脚本可传入一个参数，如下所示：                       "
            echo "        +--------------------------------------------+ "
            echo "        |  start | stop | restart | status | reload  | "
            echo "        +--------------------------------------------+ "
            echo "        |          start      ：    启动服务         | "
            echo "        |          stop       ：    关闭服务         | "
            echo "        |          restart    ：    重启服务         | "
            echo "        |          status     ：    查看状态         | "
            echo "        |          reload     ：    重新加载         | "
            echo "        +--------------------------------------------+ "
        ;;
    esac
    printf "=========================================================================\n\n"
```

#### 1.6.8 添加环境变量

```bash
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== mysql-8.0.31 ====================================== #
        export MYSQL_HOME=/opt/db/mysql
        export PATH=${PATH}:${MYSQL_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

#### 1.6.9 初始化 Mysql 

```bash
    # 切换到 Mysql 安装路径
    cd /opt/db/mysql/ || exit
    
    # 初始化 Mysql 并查看临时密码
    nohup ${MYSQL_HOME}/bin/mysqld --initialize --console > ${MYSQL_HOME}/logs/init.log 2>&1 &
    grep -ni "password" ${MYSQL_HOME}/logs/init.log  
```

#### 1.6.10 启动并测试安装

```bash
    # 启动 Mysql 服务
    ${MYSQL_HOME}/support-files/mysql.server start                             # Mysql 自带脚本启动服务
    ${MYSQL_HOME}/bin/mysql.sh start                                           # 自定义脚本启动 Mysql 
    
    # 查看 Mysql 启动状况
    ${MYSQL_HOME}/support-files/mysql.server status                            # Mysql 自带脚本启动服务
    ${MYSQL_HOME}/bin/mysql.sh status                                          # 自定义脚本查询 Mysql 
    netstat -tunlp | grep 3306                                                 # 查看 Mysql 进程占用端口
    
    # 安装 Mysql 客户端
    sudo pip install mycli                                                     # 基于 Python 的 客户端
    
    # 登录 Mysql
    ${MYSQL_HOME}/bin/mysql -h issac -P 3306 -u root -p Y2>yhAy>E%uV -D mysql  # Mysql 自带客户端连接 Mysql 服务
    mycli -h issac -P 3306 -u root -p Y2>yhAy>E%uV -D mysql                    # 使用临时密码进行登录
```

#### 1.6.11 后续处理

```mysql
    # 修改密码策略
    set global validate_password.policy=LOW;                                   # 修改密码复杂度
    set global validate_password.length=6;                                     # 修改密码长度
    
    # 修改密码并允许远程登录
    alter user 'root'@'localhost' identified by '111111';                      # 修改密码为六个 1
    update user set host = '%' where user = 'root';                            # 使 root 能在任何 host 访问
    flush privileges;                                                          # 刷新权限，使得修改生效
    
    # 退出登录，重新登录
    quit;                                                                      # 退出登录
    mycli -h issac -P 3306 -u root -p 111111 -D mysql                          # 使用修改后的密码进行登录
    
    # 创建 数据库
    create database if not exists issac;                                       # 创建 issac 数据库
    create database if not exists other;                                       # 创建 other 数据库
    create database if not exists test;                                        # 创建 test  数据库
    create database if not exists hive;                                        # 创建 hive  数据库
    create database if not exists kylin;                                       # 创建 kylin 数据库

    # 创建 用户
    create user if not exists 'issac'@'%' identified by '111111';              # 创建 issac 用户，密码六个 1

    # 将创建的 数据库，权限授权给 创建的用户
    grant all privileges on issac.* to 'issac'@'%';                            # 将数据库 issac 的所有授权给用户 issac
    grant all privileges on other.* to 'issac'@'%';                            # 将数据库 other 的所有授权给用户 issac
    grant all privileges on test.*  to 'issac'@'%';                            # 将数据库 test  的所有授权给用户 issac
    grant all privileges on hive.*  to 'issac'@'%';                            # 将数据库 hive  的所有授权给用户 issac
    grant all privileges on kylin.* to 'issac'@'%';                            # 将数据库 kylin 的所有授权给用户 issac
    flush privileges;                                                          # 刷新权限
    
    # 退出登录，使用创建的 issac 用户重新登录，并测试
    quit;                                                                      # 退出登录，或者：\q;
    mycli -h issac -P 3306 -u issac -p 111111 -D test                          # 使用修改后的密码进行登录
    show tables;                                                               # 查看 test 下的所有表
    create table if not exists test                                            # 创建 test 表
    (
        id   int          primary key,
        name varchar(64)  not null    default '',
        mark varchar(255) not null    default '未知' 
    ) engine = InnoDB;
    show create table test;
    insert into test (id, name, mark) values (101, 'issac', 'qazwsx');
    select * from test;
    quit;
```



## 2. Hadoop 安装配置

### 2.1 配置免密登录

```bash
    # 安装 ssh 
    sudo apt-get install openssh-server                                        # ubuntu，安装 ssh
    sudo yum install openssh-server                                            # redhat，安装 ssh
    
    # 配置免密登录
    ssh-keygen -t rsa                                                          # 每台机器生成秘钥（连续回车 3 次）
    ssh-copy-id issac@slaver-*                                                 # 将公钥复制到其它机器
    
    scp ~/.ssh/id_rsa.pub issac@master:~/.ssh/id_rsa.pub.slaver1               # 每个 slaver 主机的 id_rsa.pub 发给 master 节点
    cat ~/.ssh/id_rsa.pub* >> ~/.ssh/authorized_keys                           # 在 master 上，将所有公钥加到用于认证的公钥文件 authorized_keys 中
    scp ~/.ssh/authorized_keys issac@slaver*:~/.ssh/                           # 将 master 上生成的公钥文件 authorized_keys 分发给每台 slaver
    chmod 600 ~/.ssh/authorized_keys                                           # 修改每个主机 authorized_keys 的权限
    
    # 验证免密登录
    ssh slaver-*                                                               # 免密登录到其它机器
```

### 2.2 下载 Hadoop-3.2.4

  从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[hadoop-3.2.4](https://mirrors.aliyun.com/apache/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz)** 到本地

### 2.2 解压安装

```bash
    tar -zxvf hadoop-3.2.4.tar.gz -C /opt/apache/                              # 解压下载的 hadoop-3.2.4 压缩包
    cd /opt/apache/                                                            # 切换到解压目录
    mv hadoop-3.2.4/ hadoop/                                                   # 修改目录名称
```

### 2.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Hadoop 3.2.4 ====================================== #
        export HADOOP_HOME=/opt/apache/hadoop
        export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
        export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/*:${HADOOP_HOME}/share/hadoop/common/*:${HADOOP_HOME}/share/hadoop/hdfs:${HADOOP_HOME}/share/hadoop/hdfs/lib/*:${HADOOP_HOME}/share/hadoop/hdfs/*:${HADOOP_HOME}/share/hadoop/mapreduce/lib/*:${HADOOP_HOME}/share/hadoop/mapreduce/，安装 ssh*:${HADOOP_HOME}/share/hadoop/yarn:${HADOOP_HOME}/share/hadoop/yarn/lib/*:${HADOOP_HOME}/share/hadoop/yarn/*
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
    
    # 验证安装
    hadoop version
```

### 2.4 修改配置文件（${HADOOP_HOME}/etc/hadoop/）

#### 2.4.1 编辑 ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh 添加以下内容

```bash
    export JAVA_HOME=/opt/java/jdk-08
    export HADOOP_HOME=/opt/apache/hadoop
    # export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
```

#### 2.4.2 编辑 ${HADOOP_HOME}/etc/hadoop/yarn-env.sh 添加以下内容

```bash
    export JAVA_HOME=/opt/java/jdk-08
```

#### 2.4.3 编辑 ${HADOOP_HOME}/etc/hadoop/core-site.xml 添加以下内容

```xml
    <configuration>
        <!-- Hadoop 的临时文件夹位置 -->
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/opt/apache/hadoop/data/tmp</value>
        </property>
        <!-- 访问 HDFS 时的 host 和 port -->
        <property>
            <name>fs.default.name</name>
            <value>hdfs://issac:9000</value>
        </property>
        
        <!-- 缓冲区大小，根据服务器性能动态调整 -->
        <property>
            <name>io.file.buffer.size</name>
            <value>4096</value>
        </property>
        <!--  开启 HDFS 的垃圾桶机制，删除掉的数据可以从垃圾桶中回收，单位分钟 -->
        <property>
            <name>fs.trash.interval</name>
            <value>10080</value>
        </property>
        <!-- 开启 HDFS 支持压缩 -->
        <property>
            <name>io.compression.codecs</name>
            <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value>
        </property>
        <!-- 开启 Map 阶段文件压缩 -->
        <property>
            <name>mapreduce.map.output.compress</name>
            <value>true</value>
        </property>
        <!-- 设置 Map 阶段文件压缩编码 -->
        <property>
            <name>mapreduce.map.output.compress.codec</name>
            <value>org.apache.hadoop.io.compress.GzipCodec</value>
        </property>
        <!-- 开启 MapReduce 输出文件压缩 -->
        <property>
            <name>mapreduce.output.fileoutputformat.compress</name>
            <value>true</value>
        </property>
        <!-- 设置 MapReduce 输出文件压缩编码 -->
        <property>
            <name>mapreduce.output.fileoutputformat.compress.codec</name>
            <value>org.apache.hadoop.io.compress.GzipCodec</value>
        </property>
        
        <!-- 设置 root 账户在 Web 页面登录的代理 -->
        <property>
            <name>hadoop.proxyuser.root.hosts</name>
            <value>*</value>
        </property>
        <property>
            <name>hadoop.proxyuser.root.groups</name>
            <value>*</value>
        </property>
        
        <!-- 设置 issac 账户在 Web 页面登录的代理 -->
        <property>
            <name>hadoop.proxyuser.issac.hosts</name>
            <value>*</value>
        </property>
        <property>
            <name>hadoop.proxyuser.issac.groups</name>
            <value>*</value>
        </property>
        
        <!-- 高可用时，NameNode 访问 ZK 的地址 -->
        <property>
            <name>ha.zookeeper.quorum</name>
            <value>issac:2181</value>
        </property>
    </configuration>
```

#### 2.4.4 编辑 ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml 添加以下内容

```xml
    <configuration>
        <!-- 集群动态上下线 -->
        <!--
        <property>
            <name>dfs.hosts</name>
            <value>/opt/apache/hadoop/etc/hadoop/accept-host</value>
        </property>
        <property>
            <name>dfs.hosts.exclude</name>
            <value>/opt/apache/hadoop/etc/hadoop/deny-host</value>
        </property>
        -->
        <!-- 配置 NameNode 的存放位置 -->
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>/opt/apache/hadoop/data/namenode</value>
        </property>
        <!-- 定义 DataNode 数据存储的节点位置，一般先确定磁盘的挂载目录，然后多个目录用，进行分割 -->
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>/opt/apache/hadoop/data/datanode</value>
        </property>
        <!-- Edits 的存储位置 -->
        <property>
            <name>dfs.namenode.edits.dir</name>
            <value>/opt/apache/hadoop/data/edits</value>
        </property>
        <!-- 元数据信息检查点的存储位置 -->
        <property>
            <name>dfs.namenode.checkpoint.dir</name>
            <value>/opt/apache/hadoop/data/metapoint</value>
        </property>
        <!-- Edits 的检查点的存储位置 -->
        <property>
            <name>dfs.namenode.checkpoint.edits.dir</name>
            <value>/opt/apache/hadoop/data/editpoint</value>
        </property>
        <!-- 副本数量 -->
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
        
        <!-- NameNode 有一个工作线程池，默认值是 10 -->
        <property>
            <name>dfs.namenode.handler.count</name>
            <value>21</value>
        </property>
        <!-- 2NN 的访问路径和端口号 -->
        <property>
            <name>dfs.namenode.secondary.http-address</name>
            <value>issac:9860</value>
        </property>
        <!-- NameNode 的外部访问路径和端口号 -->
        <property>
            <name>dfs.namenode.http-address</name>
            <value>issac:9870</value>
        </property>
        <!-- 关闭 HDFS 的验证权限 -->
        <property>
            <name>dfs.permissions</name>
            <value>false</value>
        </property>
        <!-- HDFS 存储块的大小，4M -->
        <property>
            <name>dfs.blocksize</name>
            <value>4194304</value>
        </property>
        <!-- HDFS NameNode 最小块限制，4M -->
        <property>
            <name>dfs.namenode.fs-limits.min-block-size</name>
            <value>4194304</value>
        </property>
        <!-- 开启 HDFS WEB UI -->
        <property>
            <name>dfs.webhdfs.enabled</name>
            <value>true</value>
        </property>
        <!-- Zookeeper -->
        <property>
            <name>ha.zookeeper.quorum</name>
            <value>issac:2181</value>
        </property>
    </configuration>
```

#### 2.4.5 编辑 ${HADOOP_HOME}/etc/hadoop/mapred-site 添加以下内容

```xml
    <configuration>
        <!-- 配置 MapReduce 在 Yarn 集群上运行(默认本地运行) -->
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
        
        <!-- 开启 MapReduce 的小任务模式：开启 uber 模式，使用 JVM 重用，默认关闭 -->
        <property>
            <name>mapreduce.job.ubertask.enable</name>
            <value>true</value>
        </property>
        
        <!-- Uber 模式中最大的 MapTask 数量，可向下修改  --> 
        <property>
            <name>mapreduce.job.ubertask.maxmaps</name>
            <value>8</value>
        </property>
        
        <!-- Uber 模式中最大的 Reduce 数量，可向下修改 -->
        <property>
            <name>mapreduce.job.ubertask.maxreduces</name>
            <value>8</value>
        </property>
        
        <!-- Uber 模式中最大的输入数据量，默认使用 dfs.blocksize 的值，可向下修改 -->
        <property>
            <name>mapreduce.job.ubertask.maxbytes</name>
            <value>4194304</value>
        </property>
        
        <!-- 环形缓冲区大小，默认 100M -->
        <property>
            <name>mapreduce.task.io.sort.mb</name>
            <value>100</value>
        </property>
        
        <!-- 环形缓冲区溢写阈值，默认 0.8 -->
        <property>
            <name>mapreduce.map.sort.spill.percent</name>
            <value>0.80</value>
        </property>
        
        <!-- Merge 合并次数，默认 10个 -->
        <property>
            <name>mapreduce.task.io.sort.factor</name>
            <value>10</value>
        </property>
        
        <!-- MapTask 内存，默认 1g； MapTask 堆内存大小默认和该值大小一致 mapreduce.map.java.opts -->
        <property>
            <name>mapreduce.map.memory.mb</name>
            <value>-1</value>
        </property>
        
        <!-- MapTask 的 CPU 核数，默认 1 个 -->
        <property>
            <name>mapreduce.map.cpu.vcores</name>
            <value>4</value>
        </property>
        
        <!-- MapTask 异常重试次数，默认 4 次 -->
        <property>
            <name>mapreduce.map.maxattempts</name>
            <value>4</value>
        </property>
        
        <!-- 每个 Reduce 去 Map 中拉取数据的并行数，默认值是 5 -->
        <property>
            <name>mapreduce.reduce.shuffle.parallelcopies</name>
            <value>8</value>
        </property>
        
        <!-- Buffer 大小占 Reduce 可用内存的比例，默认值 0.7 -->
        <property>
            <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
            <value>0.70</value>
        </property>
        
        <!-- Buffer 中的数据达到多少比例开始写入磁盘，默认值 0.66 -->
        <property>
            <name>mapreduce.reduce.shuffle.merge.percent</name>
            <value>0.66</value>
        </property>
        
        <!-- ReduceTask 内存，默认 1g；ReduceTask 堆内存大小默认和该值大小一致 mapreduce.reduce.java.opts -->
        <property>
            <name>mapreduce.reduce.memory.mb</name>
            <value>-1</value>
        </property>
        
        <!-- ReduceTask 的 CPU 核数，默认 1 个 -->
        <property>
            <name>mapreduce.reduce.cpu.vcores</name>
            <value>4</value>
        </property>
        
        <!-- ReduceTask 失败重试次数，默认 4 次 -->
        <property>
            <name>mapreduce.reduce.maxattempts</name>
            <value>4</value>
        </property>
        
        <!-- 当 MapTask 完成的比例达到该值后才会为 ReduceTask 申请资源，默认是 0.05 -->
        <property>
            <name>mapreduce.job.reduce.slowstart.completedmaps</name>
            <value>0.05</value>
        </property>
        
        <!-- 如果程序在规定的默认 10 分钟内没有读到数据，将强制超时退出 -->
        <property>
            <name>mapreduce.task.timeout</name>
            <value>600000</value>
        </property>
        <!-- 配置 JobHistory 的访问路径和端口号，JobHistory 是执行完成的任务日志 -->
        <property>
            <name>mapreduce.jobhistory.address</name>
            <value>issac:10020</value>
        </property>
        <!-- 配置 JobHistory 的浏览器访问路径和端口号 -->
        <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>issac:19888</value>
        </property>
    </configuration>
```

#### 2.4.6 编辑 ${HADOOP_HOME}/etc/hadoop/yarn-site.xml 添加以下内容

```xml
    <configuration>
        <!-- 配置 ResourceManager 运行的机器地址 -->
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>issac</value>
        </property>
        <!-- Yarn WEB UI 地址 -->
        <property>
            <name>yarn.resourcemanager.webapp.address</name>
            <value>issac:8088</value>
        </property>
        <!-- 调度器地址 -->
        <property>
            <name>yarn.resourcemanager.scheduler.address</name>
            <value>issac:8098</value>
        </property>
        <!-- 配置 NodeManager 上运行的附属服务为 shuffle：需要配置成 mapreduce_shfffle，才可运行 MapReduce 程序默认值 -->
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <!-- 每个节点可用最小内存, 单位 MB, 默认 1024 MB -->
        <property>
            <name>yarn.scheduler.minimum-allocation-mb</name>
            <value>1024</value>
        </property>
        <!-- 每个节点可用内存, 单位 MB, 默认 8192 MB -->
        <property>
            <name>yarn.scheduler.maximum-allocation-mb</name>
            <value>8192</value>
        </property>
        <!-- 容器允许管理的物理内存大小，单位 MB， 默认 4096 MB -->
        <property>
            <name>yarn.nodemanager.resource.memory-mb</name>
            <value>8192</value>
        </property>
        <property>
            <name>yarn.nodemanager.vmem-pmem-ratio</name>
            <value>2.1</value>
        </property>
        <!-- 关闭 yarn 对物理内存的限制检查 -->
        <property>
            <name>yarn.nodemanager.pmem-check-enabled</name>
            <value>false</value>
        </property>
        <!-- 关闭 yarn 对虚拟内存的限制检查 -->
        <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
        </property>
        <!-- 可使用的 CPU Core -->
        <property>
          <name>yarn.nodemanager.resource.cpu-vcores</name>
          <value>4</value>
        </property>
        
        <!-- 开启日志聚集功能 -->
        <property>      
            <name>yarn.log-aggregation-enable</name>
            <value>true</value>
        </property>
        <!-- 设置日志聚集服务器地址 -->
        <property>
            <name>yarn.log.server.url</name>
            <value>http://issac:19888/jobhistory/logs</value>
        </property>
        <!-- 配置聚合日志保留时间为 7 天 -->
        <property>
            <name>yarn.log-aggregation.retain-seconds</name>
            <value>604800</value>
        </property>
        
        <!-- 每个 MapReduce 初始化堆大小 -->
        <property>
            <name>mapreduce.child.java.opts</name>
            <value>-Xmx1024m</value>
        </property>
        <!-- Yarn 的 ClassPath -->
        <property>
            <name>yarn.application.classpath</name>
            <value>/opt/apache/hadoop/etc/hadoop:/opt/apache/hadoop/share/hadoop/common/lib/*:/opt/apache/hadoop/share/hadoop/common/*:/opt/apache/hadoop/share/hadoop/hdfs:/opt/apache/hadoop/share/hadoop/hdfs/lib/*:/opt/apache/hadoop/share/hadoop/hdfs/*:/opt/apache/hadoop/share/hadoop/mapreduce/lib/*:/opt/apache/hadoop/share/hadoop/mapreduce/lib-examples/*:/opt/apache/hadoop/share/hadoop/mapreduce/*:/opt/apache/hadoop/share/hadoop/yarn:/opt/apache/hadoop/share/hadoop/yarn/lib/*:/opt/apache/hadoop/share/hadoop/yarn/*:/opt/apache/hadoop/share/hadoop/yarn/timelineservice/*</value>
        </property>
        
        <!-- 配置使用公平调度器 -->
        <!-- 
        <property>
            <name>yarn.resourcemanager.scheduler.class</name>
            <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>       
        </property>
         -->
        <!-- 指明公平调度器队列分配配置文件 -->
        <!-- 
        <property>
            <name>yarn.scheduler.fair.allocation.file</name>
            <value>/opt/apache/hadoop/etc/hadoop/fair-scheduler.xml</value>
        </property>
         -->
        <!-- 禁止队列间资源抢占 -->
        <property>
            <name>yarn.scheduler.fair.preemption</name>
            <value>false</value>
        </property>
        <!-- ResourceManager 处理调度器请求的线程数量，默认 50；如果提交的任务数大于 50，可以增加该值 -->
        <property>
            <name>yarn.resourcemanager.scheduler.client.thread-count</name>
            <value>8</value>
        </property>
    </configuration>
```

#### 2.4.7 编辑 ${HADOOP_HOME}/etc/hadoop/workers 添加以下内容

```python
    slaver1
    slaver2
    slaver3
```

#### 2.4.8 编辑 ${HADOOP_HOME}/etc/hadoop/capacity-scheduler.xml 添加以下内容 

```xml
    <configuration>
        <property>
            <name>yarn.scheduler.capacity.maximum-applications</name>
            <value>10000</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
            <value>0.8</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.resource-calculator</name>
            <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
        </property>
        
        <!-- 指定多队列，增加 bigdata 队列 -->
        <property>
            <name>yarn.scheduler.capacity.root.queues</name>
            <value>default,bigdata</value>
        </property>
        
        <!-- 指定队列的资源额定容量：降低 default 队列资源额定容量为 50%，默认 100% -->
        <property>
            <name>yarn.scheduler.capacity.root.default.capacity</name>
            <value>50</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.bigdata.capacity</name>
            <value>50</value>
        </property>
        
        <!-- 用户最多可以使用队列多少资源，1 表示 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
            <value>1</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.bigdata.user-limit-factor</name>
            <value>1</value>
        </property>
        
        <!-- 指定队列的资源最大容量：降低 default 队列资源最大容量为 75%，默认 100% -->
        <property>
            <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
            <value>75</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.bigdata.maximum-capacity</name>
            <value>75</value>
        </property>
        
        <!-- 启动队列 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.state</name>
            <value>RUNNING</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.bigdata.state</name>
            <value>RUNNING</value>
        </property>
        
        <!-- 哪些用户有权向队列提交作业 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
            <value>*</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.bigdata.acl_submit_applications</name>
            <value>*</value>
        </property>
        
        <!-- 哪些用户有权操作队列，管理员权限（查看/杀死） -->
        <property>
            <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
            <value>*</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.bigdata.acl_administer_queue</name>
            <value>*</value>
        </property>
        
        <!-- 哪些用户有权配置提交任务优先级 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.acl_application_max_priority</name>
            <value>*</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.bigdata.acl_application_max_priority</name>
            <value>*</value>
        </property>
        
        <!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout -->
        <!-- 如果 application 指定了超时时间，则提交到该队列的 application 能够指定的最大超时时间不能超过该值 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.maximum-application-lifetime
            </name>
            <value>-1</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.bigdata.maximum-application-lifetime</name>
            <value>-1</value>
        </property>
        
        <!-- 如果 application 没指定超时时间，则用 default-application-lifetime 作为默认值 -->
        <property>
            <name>yarn.scheduler.capacity.root.default.default-application-lifetime</name>
            <value>-1</value>
        </property>
        <property>
            <name>yarn.scheduler.capacity.root.bigdata.default-application-lifetime</name>
            <value>-1</value>
        </property>
        
        
        <property>
            <name>yarn.scheduler.capacity.node-locality-delay</name>
            <value>40</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.rack-locality-additional-delay</name>
            <value>-1</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.queue-mappings</name>
            <value></value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
            <value>false</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments</name>
            <value>1</value>
        </property>
        
        
        <property>
            <name>yarn.scheduler.capacity.application.fail-fast</name>
            <value>false</value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.workflow-priority-mappings</name>
            <value></value>
        </property>
        
        <property>
            <name>yarn.scheduler.capacity.workflow-priority-mappings-override.enable</name>
            <value>false</value>
        </property>
    </configuration>
```

#### 2.4.9 编辑 ${HADOOP_HOME}/etc/hadoop/fair-scheduler.xml 添加以下内容 capacity-scheduler

```xml
    <allocations>
        <!-- 单个队列中 Application Master 占用资源的最大比例,取值 0-1 ，企业一般配置 0.1 -->
        <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>
        
        <!-- 单个队列最大资源的默认值 bigdata default -->
        <queueMaxResourcesDefault>4096mb,4vcores</queueMaxResourcesDefault>
        
        <!-- 增加一个队列 test -->
        <queue name="test">
            <!-- 队列最小资源 -->
            <minResources>2048mb,2vcores</minResources>
            
            <!-- 队列最大资源 -->
            <maxResources>4096mb,4vcores</maxResources>
            
            <!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 -->
            <maxRunningApps>4</maxRunningApps>
            
            <!-- 队列中Application Master占用资源的最大比例 -->
            <maxAMShare>0.5</maxAMShare>
            
            <!-- 该队列资源权重,默认值为1.0 -->
            <weight>1.0</weight>
            
            <!-- 队列内部的资源分配策略 -->
            <schedulingPolicy>fair</schedulingPolicy>
        </queue>
        
        <!-- 增加一个队列    -->
        <queue name="bigdata" type="parent">
            <!-- 队列最小资源 -->
            <minResources>2048mb,2vcores</minResources>
            
            <!-- 队列最大资源 -->
            <maxResources>4096mb,4vcores</maxResources>
            
            <!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 -->
            <maxRunningApps>4</maxRunningApps>
            
            <!-- 队列中Application Master占用资源的最大比例 -->
            <maxAMShare>0.5</maxAMShare>
            
            <!-- 该队列资源权重,默认值为1.0 -->
            <weight>1.0</weight>
            
            <!-- 队列内部的资源分配策略 -->
            <schedulingPolicy>fair</schedulingPolicy>
        </queue>
        
        <!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 -->
        <queuePlacementPolicy>
            <!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false表示：如果指定队列不存在,不允许自动创建-->
            <rule name="specified" create="false"/>
            
            <!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 -->
            <rule name="nestedUserQueue" create="true">
                <rule name="primaryGroup" create="false"/>
            </rule>
            
            <!-- 最后一个规则必须为reject或者default。Reject表示拒绝创建提交失败，default表示把任务提交到default队列 -->
            <rule name="reject"/>
        </queuePlacementPolicy>
    </allocations>
```

#### 2.4.10 创建数据和日志存储路径

```bash
    cd /opt/apache/hadoop/                                                     # 切换到安装路径
    mkdir -p data logs                                                         # 创建 数据 和 日志 目录
                        
    cd /opt/apache/hadoop/data                                                 # 切换到数据存储路径
    mkdir -p ${HADOOP_HOME}/data/tmp                                           # 创建 临时存储 路径
    mkdir -p ${HADOOP_HOME}/data/namenode                                      # 创建 NameNode 数据存储路径
    mkdir -p ${HADOOP_HOME}/data/datanode                                      # 创建 DataNode 数据存储路径
    mkdir -p ${HADOOP_HOME}/data/edit                                          # 创建 编辑日志 存储路径
    mkdir -p ${HADOOP_HOME}/data/metapoint                                     # 创建 元数据信息检查点 存储路径
    mkdir -p ${HADOOP_HOME}/data/editpoint                                     # 创建 编辑日志检查点 存储路径
```

### 2.5 编写 hadoop 启停脚本

#### 2.5.1 创建 ${HADOOP_HOME}/bin/hadoop.sh

```bash
    cd /opt/apache/hadoop/bin/                                                 # 切换到 hadoop 安装目录的 bin
    touch ${HADOOP_HOME}/bin/hadoop.sh                                         # 脚本内容，如：2.5.2
    chmod +x ${HADOOP_HOME}/bin/hadoop.sh                                      # 添加可执行权限
```

#### 2.5.2 ${HADOOP_HOME}/bin/hadoop.sh

```bashpro shell script
    #!/usr/bin/env bash
    
    SERVICE_DIR=$(cd "$(dirname "$0")/../" || exit; pwd)
    SERVICE_NAME=Hadoop
    JUDGE_NAME=org.apache.hadoop
    
    NAME_NODE_PORT=9870
    DATA_NODE_PORT=9864
    SECOND_NAME_NODE_PORT=50090
    NODE_MANAGER_PORT=8042
    RESOURCE_MANAGER_PORT=8088
    JOB_HISTORY_PORT=19888
    
    NAME_NODE=org.apache.hadoop.hdfs.server.namenode.NameNode
    DATA_NODE=org.apache.hadoop.hdfs.server.datanode.DataNode
    SECOND_NAME_NODE=org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
    NODE_MANAGER=org.apache.hadoop.yarn.server.nodemanager.NodeManager
    RESOURCE_MANAGER=org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
    JOB_HISTORY_SERVER=org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer
    
    
    printf "\n=========================================================================\n"
    #  匹配输入参数
    case "$1" in
        #  1. 运行程序
        start)
            # 1.1 查找程序的 pid
            pid_list=$(ps -aux | grep -i "${JUDGE_NAME}" | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}')
            
            #  1.2 若 pid 不存在，则运行程序，否则打印程序运行状态
            if [ ! "${pid_list}" ]; then
                echo "    程序 ${SERVICE_NAME} 正在加载中 ......"
                "${SERVICE_DIR}/sbin/start-all.sh" > /dev/null 2>&1
                sleep 11
                
                # 1.3 判断程序 NameNode 启动是否成功
                name_pid=$(ps -aux | grep -i ${NAME_NODE} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${name_pid}" -ne 1 ]; then
                    echo "    程序 NameNode 启动失败 ...... "
                fi
                
                # 1.4 判断程序 DataNode 启动是否成功
                data_pid=$(ps -aux | grep -i ${DATA_NODE} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${data_pid}" -ne 1 ]; then
                    echo "    程序 DataNode 启动失败 ...... "
                fi
                
                # 1.5 判断程序 SecondaryNameNode 启动是否成功
                second_pid=$(ps -aux | grep -i ${SECOND_NAME_NODE} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${second_pid}" -ne 1 ]; then
                    echo "    程序 DataNode 启动失败 ...... "
                fi
                
                # 1.6 判断程序 NodeManager 启动是否成功
                node_pid=$(ps -aux | grep -i ${NODE_MANAGER} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${node_pid}" -ne 1 ]; then
                    echo "    程序 NodeManager 启动失败 ...... "
                fi
                
                # 1.7 判断程序 ResourceManager 启动是否成功
                resource_pid=$(ps -aux | grep -i ${RESOURCE_MANAGER} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${resource_pid}" -ne 1 ]; then
                    echo "    程序 ResourceManager 启动失败 ...... "
                fi
                
                "${SERVICE_DIR}/sbin/mr-jobhistory-daemon.sh" start historyserver > /dev/null 2>&1
                sleep 3
                
                # 1.8 判断程序 JobHistoryServer 启动是否成功
                history_pid=$(ps -aux | grep -i ${JOB_HISTORY_SERVER} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${history_pid}" -ne 1 ]; then
                    echo "    程序 JobHistoryServer 启动失败 ...... "
                fi
                
                # 1.9 判断所有程序启动是否成功
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${pid_count}" -ge 6 ]; then
                    echo "    程序 ${SERVICE_NAME} 启动成功 ...... "
                else
                    echo "    程序 ${SERVICE_NAME} 启动失败 ...... "
                fi
                
            else
                echo "    程序 ${SERVICE_NAME} 正在运行当中 ...... "
            fi
        ;;
        
        #  2. 停止
        stop)
            # 2.1 根据程序的 pid 查询程序运行状态
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            if [ "${pid_count}" -eq 0 ]; then
                echo "    ${SERVICE_NAME} 的进程不存在，程序没有运行 ...... "
            elif [ "${pid_count}" -eq 6 ]; then
                # 2.2 杀死进程，关闭程序
                "${SERVICE_DIR}/sbin/mr-jobhistory-daemon.sh" stop historyserver > /dev/null 2>&1
                sleep 1
                echo "    程序 ${SERVICE_NAME} 正在停止中 ...... "
                "${SERVICE_DIR}/sbin/stop-all.sh" > /dev/null 2>&1
                sleep 5
                
                # 2.3 若还未关闭，则强制杀死进程，关闭程序
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | wc -l)
                if [ "${pid_count}" -ge 1 ]; then
                    # temp=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | xargs kill -9)
                    echo "    "
                fi
                
                echo "    程序 ${SERVICE_NAME} 已经停止成功 ......"            
            else
                echo "    程序 ${SERVICE_NAME} 运行出现问题 ......"
            fi
        ;;
        
        #  3. 状态查询
        status)
            # 3.1 查看正在运行程序的 pid
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            #  3.2 判断 ES 运行状态
            if [ "${pid_count}" -eq 0 ]; then
                echo "    程序 ${SERVICE_NAME} 已经停止 ...... "
            elif [ "${pid_count}" -eq 6 ]; then
                echo "    程序 ${SERVICE_NAME} 正在运行中 ...... "
            else
                echo "    程序 ${SERVICE_NAME} 运行出现问题 ...... "
            fi
        ;;
        
        #  4. 重启程序
        restart)
            "$0" stop
            sleep 3
            "$0" start
        ;;
        
        #  5. 其它情况
        *)
            echo "    脚本可传入一个参数，如下所示：              "
            echo "        +-----------------------------------+ "
            echo "        |  start | stop | restart | status  | "
            echo "        +-----------------------------------+ "
            echo "        |        start    ：  启动服务      | "
            echo "        |        stop     ：  关闭服务      | "
            echo "        |        restart  ：  重启服务      | "
            echo "        |        status   ：  查看状态      | "
            echo "        +-----------------------------------+ "
        ;;
    esac
    printf "=========================================================================\n\n"
```

### 2.6 分发到其它节点

#### 2.6.1 安装同步命令

```bash
    # 安装同步命令
    sudo yum install rsync                                                     # redhat
    sudo apt install rsync                                                     # ubuntu
    
    mkdir -p /home/issac/shell                                                 # 创建 shell 存放目录
    
    # 添加 shell 到环境变量（redhat：~/.bash_profile，ubuntu：~/.bashrc）
    echo "# ============================== issac-1.0.0 ============================== #" >> ~/.bash_profile  # 添加 shell 到环境变量（redhat）
    echo "export ISSAC_HOME=/home/issac" >> ~/.bash_profile                    # 创建 ISSAC_HOME 环境变量
    echo "PATH=${PATH}:${ISSAC_HOME}/shell" >> ~/.bash_profile                 # 添加 ISSAC_HOME 到环境变量
    source ~/.bash_profile                                                     # 使环境变量生效
    
    vim ${ISSAC_HOME}/shell/xync.sh                                              # 创建同步脚本，内容如 2.6.2
    chmod +x /home/issac/shell/*.sh                                            # 添加可执行权限
```

#### 2.6.2 集群同步脚本 /home/issac/shell/xync.sh 

```bashpro shell script
    #!/usr/bin/env bash
    
    HOST_LIST=(master slaver1 slaver2 slaver3)
    TARGET_PATH=$(pwd)/
    USER=$(whoami)
    
    # 判断输入的参数个数，以及是文件夹还是文件   
    if [ "$#" -gt 1 ]; then
        echo "    脚本只可以收传入一个参数，为文件或文件夹路径 ......"
        exit
    elif [ "$#" -lt 1 ]; then
        echo "    没有输入参数，将同步当前文件夹中所有的文件夹和文件 ......   "
    elif [ -d "$1" ]; then 
        TARGET_PATH=$(cd -P "$1" || exit; pwd)/
        echo "    文件夹：${TARGET_PATH} "
    elif [ -f "$1" ]; then
        TARGET_PATH=$(cd -P $(dirname "$1") || exit; pwd)/$(basename "$1")
        echo "    文件：${TARGET_PATH} "
    else
        echo "    输入的路径（$1）不存在 ......   "
        exit
    fi
        
    # 循环读取主机 ${HOST_LIST[@]}
    for host_name in "${HOST_LIST[@]}"
    do
        echo "============================== 向主机（${host_name}）同步数据 =============================="    
        # rsync -rvl --delete  "${TARGET_PATH}"  "${USER}@${host_name}:${TARGET_PATH}"
        rsync -zav --delete  "${TARGET_PATH}"  "${USER}@${host_name}:${TARGET_PATH}"
    done
```

#### 2.6.3 查看集群命令 /home/issac/shell/xcall.sh

```bashpro shell script
    #!/usr/bin/env bash
    
    # HOST_LIST=(master slaver1 slaver2 slaver3)                 # 集群主机
    HOST_LIST=(master slaver)                                  # 集群主机
    USER=$(whoami)                                             # 获取当前登录用户
    
    # 1. 判断指令是否为空：空，则退出
    if [ "$#" -lt 1 ]; then
        cmd="pwd; ls -l" 
    elif [ "$#" -gt 1 ] || [ "${1}" = "" ]; then
        echo "    脚本最多可输入一个参数 ......   "
        exit
    else 
        cmd="$*"
    fi
    
    printf "\n================================================================================\n"
    
    # 2. 遍历所有的主机，执行命令
    for host_name in "${HOST_LIST[@]}"
    do
        echo "    ********** 在主机 ${host_name} 上执行命令：${cmd} **********    "
        # ssh "${USER}@${host_name}" "$@"
        
        # 3. 执行命令    
        ssh "${USER}@${host_name}" "source /etc/profile; ${cmd}"
    done
    
    printf "================================================================================\n\n"
```

#### 2.6.4 同步 Hadoop 安装路径  

```bash
    cd /opt/apache/                                                            # 切换到 hadoop 安装父路径
    ~/shell/xync.sh hadoop                                                     # 同步 hadoop 到其它节点
````

### 2.7 格式化 NameNode

```bash
    cd /opt/apache/hadoop/                                                     # 进入 hadoop 安装目录
    ${HADOOP_HOME}/bin/hadoop namenode -format > ${HADOOP_HOME}/logs/format.log 2>&1 &   # 格式化 namenode
    grep -ni "successfully formatted" ${HADOOP_HOME}/logs/format.log           # 查看格式化结果
```

### 2.8 启动集群并测试

```bash
    cd /opt/apache/hadoop/                                                     # 进入 hadoop 安装目录
    
    # 单独启动各个进程
    ${HADOOP_HOME}/sbin/hadoop-daemons.sh start namenode                       # 启动 NameNode 守护进程
    ${HADOOP_HOME}/sbin/hadoop-daemons.sh start datanode                       # 启动 DataNode 守护进程
    ${HADOOP_HOME}/sbin/hadoop-daemons.sh start SecondaryNameNode              # 启动 SecondaryNameNode 守护进程
    ${HADOOP_HOME}/sbin/yarn-daemon.sh start resourcemanager                   # 启动 ResourceManager 守护进程
    ${HADOOP_HOME}/sbin/yarn-daemon.sh start nodemanager                       # 启动 NodeManager 守护进程
    
    # 分别启动集群启动 hdfs yarn
    ${HADOOP_HOME}/sbin/start-dfs.sh                                           # 启动 hdfs 集群
    ${HADOOP_HOME}/sbin/start-yarn.sh                                          # 启动 yarn 集群
    
    # 全部启动 hdfs yarn
    ${HADOOP_HOME}/sbin/start-all.sh                                           # 启动 hdfs、yarn 集群
    
    # 历史服务器
    ${HADOOP_HOME}/sbin/mr-jobhistory-dameon.sh start historyserver            # 启动 HistoryServer 集群
    
    # 编写的脚本启动
    ${HADOOP_HOME}/bin/hadoop.sh start                                         # 启动 hdfs、yarn、HistoryServer 并验证启动结果
    
    jps -l                                                                     # 查看 hadoop 启动的 jvm 进程
    http://master:9870                                                         # 浏览器访问 NameNode
    http://master:9860                                                         # 浏览器访问 2NN
    http://master:8088/cluster                                                 # 浏览器访问 Yarn
    http://master:19888/jobhistory                                             # 浏览器访问 历史服务器
    
    # 计算 pi 
    ${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar pi 10 10
    
    # 计算
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /hadoop/test/wc/input               # 创建 hdfs 数据输入目录
    ${HADOOP_HOME}/bin/hadoop fs -ls /hadoop/test/wc/input                     # 查看创建的目录
    ${HADOOP_HOME}/bin/hadoop fs -put ${HADOOP_HOME}/*.txt /hadoop/test/wc/input    # 上传文件到 hdfs
    ${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar wordcount /hadoop/test/wc/input /hadoop/test/wc/output
    ${HADOOP_HOME}/bin/hadoop fs -cat /hadoop/test/wc/output/part-r-00000  # 查看统计结果    
```



## 3. Spark 安装配置

### 3.1 下载 Spark-3.2.3 源码

  从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[spark-3.2.3 源码](https://mirrors.aliyun.com/apache/spark/spark-3.2.3/spark-3.2.3.tgz)** 和不含有 hadoop 的 [**spark-3.2.3**](https://mirrors.aliyun.com/apache/spark/spark-3.2.3/spark-3.2.3-bin-without-hadoop.tgz) 到本地

### 3.2 编译 Spark（spark-3.2.3 和 hadoop-3.2.4 在兼容性上存在问题，需要修改源码重新编译）

#### 3.2.1 解压 spark 源码

```bash
    sudo yum install -y git                                                    # 安装 git 用于编译 spark 时执行补丁
    mkdir -p /home/issac/coding/java/src/                                      # 创建目录存放代码
    
    git clone https://github.com/apache/spark.git                              # 使用 git 下载源码
```

#### 3.2.2 修改源码前，编译

```bash
    cd /home/issac/coding/java/src  || exit                                    # 进入 spark 源码目录
    git branch                                                                 # 查看本地分支
    git branch -r                                                              # 查看远程分支，-r 表示 remote
    git branch -a                                                              # 查看所有分支
    git checkout v3.2.3                                                        # 切换到 3.2.3 分支
    
    # 编译 spark-3.2.3
    ./dev/make-distribution.sh --name build --tgz -Phive-3.1 -Phive-thriftserver -Phadoop-3.2 -Phadoop-provided -Pyarn -Pscala-2.12 -Dhadoop.version=3.2.4 -DskipTests
```

#### 3.2.3 应用补丁文件，并进行编译

```bash
    cd /home/issac/coding/java/src  || exit                                    # 进入 spark 源码目录
    touch issac-spark.patch                                                    # 创建 issac-spark 补丁，内容如：3.2.4
    
    git apply --check ./issac-spark.patch                                      # 检查 patch 是否可用
    git apply ./issac-spark.patch                                              # 应用补丁，不包含 commit 内容
    git am ./issac-spark.patch                                                 # 应用补丁，包含 commit 内容
    
    # 再次编译
    ./dev/make-distribution.sh --name build --tgz -Phive-3.1 -Phive-thriftserver -Phadoop-3.2 -Phadoop-provided -Pyarn -Pscala-2.12 -Dhadoop.version=3.2.4 -DskipTests
```

#### 3.2.4 补丁内容 issac-spark.patch (batch 文件夹下)

```bash
    Index: pom.xml
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/pom.xml b/pom.xml
    --- a/pom.xml   (revision b53c341e0fefbb33d115ab630369a18765b7763d)
    +++ b/pom.xml   (date 1675670990999)
    @@ -18,217 +18,217 @@
     
     <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
              xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    -  <modelVersion>4.0.0</modelVersion>
    -  <parent>
    -    <groupId>org.apache</groupId>
    -    <artifactId>apache</artifactId>
    -    <version>18</version>
    -  </parent>
    -  <groupId>org.apache.spark</groupId>
    -  <artifactId>spark-parent_2.12</artifactId>
    -  <version>3.2.3</version>
    -  <packaging>pom</packaging>
    -  <name>Spark Project Parent POM</name>
    -  <url>http://spark.apache.org/</url>
    -  <licenses>
    -    <license>
    -      <name>Apache 2.0 License</name>
    -      <url>http://www.apache.org/licenses/LICENSE-2.0.html</url>
    -      <distribution>repo</distribution>
    -    </license>
    -  </licenses>
    -  <scm>
    -    <connection>scm:git:git@github.com:apache/spark.git</connection>
    -    <developerConnection>scm:git:https://gitbox.apache.org/repos/asf/spark.git</developerConnection>
    -    <url>scm:git:git@github.com:apache/spark.git</url>
    -    <tag>HEAD</tag>
    -  </scm>
    -  <developers>
    -    <developer>
    -      <id>matei</id>
    -      <name>Matei Zaharia</name>
    -      <email>matei.zaharia@gmail.com</email>
    -      <url>https://cs.stanford.edu/people/matei</url>
    -      <organization>Apache Software Foundation</organization>
    -      <organizationUrl>http://spark.apache.org</organizationUrl>
    -    </developer>
    -  </developers>
    -  <issueManagement>
    -    <system>JIRA</system>
    -    <url>https://issues.apache.org/jira/browse/SPARK</url>
    -  </issueManagement>
    -
    -  <mailingLists>
    -    <mailingList>
    -      <name>Dev Mailing List</name>
    -      <post>dev@spark.apache.org</post>
    -      <subscribe>dev-subscribe@spark.apache.org</subscribe>
    -      <unsubscribe>dev-unsubscribe@spark.apache.org</unsubscribe>
    -    </mailingList>
    -
    -    <mailingList>
    -      <name>User Mailing List</name>
    -      <post>user@spark.apache.org</post>
    -      <subscribe>user-subscribe@spark.apache.org</subscribe>
    -      <unsubscribe>user-unsubscribe@spark.apache.org</unsubscribe>
    -    </mailingList>
    -
    -    <mailingList>
    -      <name>Commits Mailing List</name>
    -      <post>commits@spark.apache.org</post>
    -      <subscribe>commits-subscribe@spark.apache.org</subscribe>
    -      <unsubscribe>commits-unsubscribe@spark.apache.org</unsubscribe>
    -    </mailingList>
    -  </mailingLists>
    -
    -  <modules>
    -    <module>common/sketch</module>
    -    <module>common/kvstore</module>
    -    <module>common/network-common</module>
    -    <module>common/network-shuffle</module>
    -    <module>common/unsafe</module>
    -    <module>common/tags</module>
    -    <module>core</module>
    -    <module>graphx</module>
    -    <module>mllib</module>
    -    <module>mllib-local</module>
    -    <module>tools</module>
    -    <module>streaming</module>
    -    <module>sql/catalyst</module>
    -    <module>sql/core</module>
    -    <module>sql/hive</module>
    -    <module>assembly</module>
    -    <module>examples</module>
    -    <module>repl</module>
    -    <module>launcher</module>
    -    <module>external/kafka-0-10-token-provider</module>
    -    <module>external/kafka-0-10</module>
    -    <module>external/kafka-0-10-assembly</module>
    -    <module>external/kafka-0-10-sql</module>
    -    <module>external/avro</module>
    -    <!-- See additional modules enabled by profiles below -->
    -  </modules>
    -
    -  <properties>
    -    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    -    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    -    <java.version>1.8</java.version>
    -    <maven.compiler.source>${java.version}</maven.compiler.source>
    -    <maven.compiler.target>${java.version}</maven.compiler.target>
    -    <maven.version>3.6.3</maven.version>
    -    <exec-maven-plugin.version>1.6.0</exec-maven-plugin.version>
    -    <sbt.project.name>spark</sbt.project.name>
    -    <slf4j.version>1.7.30</slf4j.version>
    -    <log4j.version>1.2.17</log4j.version>
    -    <hadoop.version>3.3.1</hadoop.version>
    -    <protobuf.version>2.5.0</protobuf.version>
    -    <yarn.version>${hadoop.version}</yarn.version>
    -    <zookeeper.version>3.6.2</zookeeper.version>
    -    <curator.version>2.13.0</curator.version>
    -    <hive.group>org.apache.hive</hive.group>
    -    <hive.classifier>core</hive.classifier>
    -    <!-- Version used in Maven Hive dependency -->
    -    <hive.version>2.3.9</hive.version>
    -    <hive23.version>2.3.9</hive23.version>
    -    <!-- Version used for internal directory structure -->
    -    <hive.version.short>2.3</hive.version.short>
    -    <!-- note that this should be compatible with Kafka brokers version 0.10 and up -->
    -    <kafka.version>2.8.1</kafka.version>
    -    <!-- After 10.15.1.3, the minimum required version is JDK9 -->
    -    <derby.version>10.14.2.0</derby.version>
    -    <parquet.version>1.12.2</parquet.version>
    -    <orc.version>1.6.14</orc.version>
    -    <jetty.version>9.4.44.v20210927</jetty.version>
    -    <jakartaservlet.version>4.0.3</jakartaservlet.version>
    -    <chill.version>0.10.0</chill.version>
    -    <ivy.version>2.5.0</ivy.version>
    -    <oro.version>2.0.8</oro.version>
    -    <!--
    +    <modelVersion>4.0.0</modelVersion>
    +    <parent>
    +        <groupId>org.apache</groupId>
    +        <artifactId>apache</artifactId>
    +        <version>18</version>
    +    </parent>
    +    <groupId>org.apache.spark</groupId>
    +    <artifactId>spark-parent_2.12</artifactId>
    +    <version>3.2.3</version>
    +    <packaging>pom</packaging>
    +    <name>Spark Project Parent POM</name>
    +    <url>http://spark.apache.org/</url>
    +    <licenses>
    +        <license>
    +            <name>Apache 2.0 License</name>
    +            <url>http://www.apache.org/licenses/LICENSE-2.0.html</url>
    +            <distribution>repo</distribution>
    +        </license>
    +    </licenses>
    +    <scm>
    +        <connection>scm:git:git@github.com:apache/spark.git</connection>
    +        <developerConnection>scm:git:https://gitbox.apache.org/repos/asf/spark.git</developerConnection>
    +        <url>scm:git:git@github.com:apache/spark.git</url>
    +        <tag>HEAD</tag>
    +    </scm>
    +    <developers>
    +        <developer>
    +            <id>matei</id>
    +            <name>Matei Zaharia</name>
    +            <email>matei.zaharia@gmail.com</email>
    +            <url>https://cs.stanford.edu/people/matei</url>
    +            <organization>Apache Software Foundation</organization>
    +            <organizationUrl>http://spark.apache.org</organizationUrl>
    +        </developer>
    +    </developers>
    +    <issueManagement>
    +        <system>JIRA</system>
    +        <url>https://issues.apache.org/jira/browse/SPARK</url>
    +    </issueManagement>
    +    
    +    <mailingLists>
    +        <mailingList>
    +            <name>Dev Mailing List</name>
    +            <post>dev@spark.apache.org</post>
    +            <subscribe>dev-subscribe@spark.apache.org</subscribe>
    +            <unsubscribe>dev-unsubscribe@spark.apache.org</unsubscribe>
    +        </mailingList>
    +        
    +        <mailingList>
    +            <name>User Mailing List</name>
    +            <post>user@spark.apache.org</post>
    +            <subscribe>user-subscribe@spark.apache.org</subscribe>
    +            <unsubscribe>user-unsubscribe@spark.apache.org</unsubscribe>
    +        </mailingList>
    +        
    +        <mailingList>
    +            <name>Commits Mailing List</name>
    +            <post>commits@spark.apache.org</post>
    +            <subscribe>commits-subscribe@spark.apache.org</subscribe>
    +            <unsubscribe>commits-unsubscribe@spark.apache.org</unsubscribe>
    +        </mailingList>
    +    </mailingLists>
    +    
    +    <modules>
    +        <module>common/sketch</module>
    +        <module>common/kvstore</module>
    +        <module>common/network-common</module>
    +        <module>common/network-shuffle</module>
    +        <module>common/unsafe</module>
    +        <module>common/tags</module>
    +        <module>core</module>
    +        <module>graphx</module>
    +        <module>mllib</module>
    +        <module>mllib-local</module>
    +        <module>tools</module>
    +        <module>streaming</module>
    +        <module>sql/catalyst</module>
    +        <module>sql/core</module>
    +        <module>sql/hive</module>
    +        <module>assembly</module>
    +        <module>examples</module>
    +        <module>repl</module>
    +        <module>launcher</module>
    +        <module>external/kafka-0-10-token-provider</module>
    +        <module>external/kafka-0-10</module>
    +        <module>external/kafka-0-10-assembly</module>
    +        <module>external/kafka-0-10-sql</module>
    +        <module>external/avro</module>
    +        <!-- See additional modules enabled by profiles below -->
    +    </modules>
    +    
    +    <properties>
    +        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    +        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    +        <java.version>1.8</java.version>
    +        <maven.compiler.source>${java.version}</maven.compiler.source>
    +        <maven.compiler.target>${java.version}</maven.compiler.target>
    +        <maven.version>3.6.3</maven.version>
    +        <exec-maven-plugin.version>1.6.0</exec-maven-plugin.version>
    +        <sbt.project.name>spark</sbt.project.name>
    +        <slf4j.version>1.7.30</slf4j.version>
    +        <log4j.version>1.2.17</log4j.version>
    +        <hadoop.version>3.2.4</hadoop.version>
    +        <protobuf.version>2.5.0</protobuf.version>
    +        <yarn.version>${hadoop.version}</yarn.version>
    +        <zookeeper.version>3.6.2</zookeeper.version>
    +        <curator.version>2.13.0</curator.version>
    +        <hive.group>org.apache.hive</hive.group>
    +        <hive.classifier>core</hive.classifier>
    +        <!-- Version used in Maven Hive dependency -->
    +        <hive.version>2.3.9</hive.version>
    +        <hive23.version>2.3.9</hive23.version>
    +        <!-- Version used for internal directory structure -->
    +        <hive.version.short>2.3</hive.version.short>
    +        <!-- note that this should be compatible with Kafka brokers version 0.10 and up -->
    +        <kafka.version>2.8.1</kafka.version>
    +        <!-- After 10.15.1.3, the minimum required version is JDK9 -->
    +        <derby.version>10.14.2.0</derby.version>
    +        <parquet.version>1.12.2</parquet.version>
    +        <orc.version>1.6.14</orc.version>
    +        <jetty.version>9.4.44.v20210927</jetty.version>
    +        <jakartaservlet.version>4.0.3</jakartaservlet.version>
    +        <chill.version>0.10.0</chill.version>
    +        <ivy.version>2.5.0</ivy.version>
    +        <oro.version>2.0.8</oro.version>
    +        <!--
         If you changes codahale.metrics.version, you also need to change
         the link to metrics.dropwizard.io in docs/monitoring.md.
         -->
    -    <codahale.metrics.version>4.2.0</codahale.metrics.version>
    -    <avro.version>1.10.2</avro.version>
    -    <aws.kinesis.client.version>1.12.0</aws.kinesis.client.version>
    -    <!-- Should be consistent with Kinesis client dependency -->
    -    <aws.java.sdk.version>1.11.655</aws.java.sdk.version>
    -    <!-- the producer is used in tests -->
    -    <aws.kinesis.producer.version>0.12.8</aws.kinesis.producer.version>
    -    <!--  org.apache.httpcomponents/httpclient-->
    -    <commons.httpclient.version>4.5.13</commons.httpclient.version>
    -    <commons.httpcore.version>4.4.14</commons.httpcore.version>
    -    <commons.math3.version>3.4.1</commons.math3.version>
    -    <!-- managed up from 3.2.1 for SPARK-11652 -->
    -    <commons.collections.version>3.2.2</commons.collections.version>
    -    <scala.version>2.12.15</scala.version>
    -    <scala.binary.version>2.12</scala.binary.version>
    -    <scalatest-maven-plugin.version>2.0.2</scalatest-maven-plugin.version>
    -    <scalafmt.parameters>--test</scalafmt.parameters>
    -    <!-- for now, not running scalafmt as part of default verify pipeline -->
    -    <scalafmt.skip>true</scalafmt.skip>
    -    <codehaus.jackson.version>1.9.13</codehaus.jackson.version>
    -    <fasterxml.jackson.version>2.12.3</fasterxml.jackson.version>
    -    <snappy.version>1.1.8.4</snappy.version>
    -    <netlib.java.version>1.1.2</netlib.java.version>
    -    <netlib.ludovic.dev.version>2.2.1</netlib.ludovic.dev.version>
    -    <commons-codec.version>1.15</commons-codec.version>
    -    <commons-compress.version>1.21</commons-compress.version>
    -    <commons-io.version>2.8.0</commons-io.version>
    -    <!-- org.apache.commons/commons-lang/-->
    -    <commons-lang2.version>2.6</commons-lang2.version>
    -    <!-- org.apache.commons/commons-lang3/-->
    -    <commons-lang3.version>3.12.0</commons-lang3.version>
    -    <!-- org.apache.commons/commons-pool2/-->
    -    <commons-pool2.version>2.6.2</commons-pool2.version>
    -    <datanucleus-core.version>4.1.17</datanucleus-core.version>
    -    <guava.version>14.0.1</guava.version>
    -    <janino.version>3.0.16</janino.version>
    -    <jersey.version>2.34</jersey.version>
    -    <joda.version>2.10.10</joda.version>
    -    <jodd.version>3.5.2</jodd.version>
    -    <jsr305.version>3.0.0</jsr305.version>
    -    <libthrift.version>0.12.0</libthrift.version>
    -    <antlr4.version>4.8</antlr4.version>
    -    <jpam.version>1.1</jpam.version>
    -    <selenium.version>3.141.59</selenium.version>
    -    <htmlunit.version>2.50.0</htmlunit.version>
    -    <maven-antrun.version>1.8</maven-antrun.version>
    -    <commons-crypto.version>1.1.0</commons-crypto.version>
    -    <commons-cli.version>1.2</commons-cli.version>
    -    <bouncycastle.version>1.60</bouncycastle.version>
    -    <tink.version>1.6.0</tink.version>
    -    <!--
    +        <codahale.metrics.version>4.2.0</codahale.metrics.version>
    +        <avro.version>1.10.2</avro.version>
    +        <aws.kinesis.client.version>1.12.0</aws.kinesis.client.version>
    +        <!-- Should be consistent with Kinesis client dependency -->
    +        <aws.java.sdk.version>1.11.655</aws.java.sdk.version>
    +        <!-- the producer is used in tests -->
    +        <aws.kinesis.producer.version>0.12.8</aws.kinesis.producer.version>
    +        <!--  org.apache.httpcomponents/httpclient-->
    +        <commons.httpclient.version>4.5.13</commons.httpclient.version>
    +        <commons.httpcore.version>4.4.14</commons.httpcore.version>
    +        <commons.math3.version>3.4.1</commons.math3.version>
    +        <!-- managed up from 3.2.1 for SPARK-11652 -->
    +        <commons.collections.version>3.2.2</commons.collections.version>
    +        <scala.version>2.12.15</scala.version>
    +        <scala.binary.version>2.12</scala.binary.version>
    +        <scalatest-maven-plugin.version>2.0.2</scalatest-maven-plugin.version>
    +        <scalafmt.parameters>--test</scalafmt.parameters>
    +        <!-- for now, not running scalafmt as part of default verify pipeline -->
    +        <scalafmt.skip>true</scalafmt.skip>
    +        <codehaus.jackson.version>1.9.13</codehaus.jackson.version>
    +        <fasterxml.jackson.version>2.12.3</fasterxml.jackson.version>
    +        <snappy.version>1.1.8.4</snappy.version>
    +        <netlib.java.version>1.1.2</netlib.java.version>
    +        <netlib.ludovic.dev.version>2.2.1</netlib.ludovic.dev.version>
    +        <commons-codec.version>1.15</commons-codec.version>
    +        <commons-compress.version>1.21</commons-compress.version>
    +        <commons-io.version>2.8.0</commons-io.version>
    +        <!-- org.apache.commons/commons-lang/-->
    +        <commons-lang2.version>2.6</commons-lang2.version>
    +        <!-- org.apache.commons/commons-lang3/-->
    +        <commons-lang3.version>3.12.0</commons-lang3.version>
    +        <!-- org.apache.commons/commons-pool2/-->
    +        <commons-pool2.version>2.6.2</commons-pool2.version>
    +        <datanucleus-core.version>4.1.17</datanucleus-core.version>
    +        <guava.version>14.0.1</guava.version>
    +        <janino.version>3.0.16</janino.version>
    +        <jersey.version>2.34</jersey.version>
    +        <joda.version>2.10.10</joda.version>
    +        <jodd.version>3.5.2</jodd.version>
    +        <jsr305.version>3.0.0</jsr305.version>
    +        <libthrift.version>0.12.0</libthrift.version>
    +        <antlr4.version>4.8</antlr4.version>
    +        <jpam.version>1.1</jpam.version>
    +        <selenium.version>3.141.59</selenium.version>
    +        <htmlunit.version>2.50.0</htmlunit.version>
    +        <maven-antrun.version>1.8</maven-antrun.version>
    +        <commons-crypto.version>1.1.0</commons-crypto.version>
    +        <commons-cli.version>1.2</commons-cli.version>
    +        <bouncycastle.version>1.60</bouncycastle.version>
    +        <tink.version>1.6.0</tink.version>
    +        <!--
         If you are changing Arrow version specification, please check
         ./python/pyspark/sql/pandas/utils.py, and ./python/setup.py too.
         -->
    -    <arrow.version>2.0.0</arrow.version>
    -    <!-- org.fusesource.leveldbjni will be used except on arm64 platform. -->
    -    <leveldbjni.group>org.fusesource.leveldbjni</leveldbjni.group>
    -    <kubernetes-client.version>5.4.1</kubernetes-client.version>
    -
    -    <test.java.home>${java.home}</test.java.home>
    -
    -    <!-- Some UI tests require Chrome and Chrome driver installed so those tests are disabled by default. -->
    -    <test.default.exclude.tags>org.apache.spark.tags.ChromeUITest</test.default.exclude.tags>
    -    <test.exclude.tags></test.exclude.tags>
    -    <test.include.tags></test.include.tags>
    -
    -    <test.jdwp.address>localhost:0</test.jdwp.address>
    -    <test.jdwp.suspend>y</test.jdwp.suspend>
    -    <test.jdwp.server>y</test.jdwp.server>
    -    <test.debug.suite>false</test.debug.suite>
    -
    -    <!-- Package to use when relocating shaded classes. -->
    -    <spark.shade.packageName>org.sparkproject</spark.shade.packageName>
    -
    -    <!-- Modules that copy jars to the build directory should do so under this location. -->
    -    <jars.target.dir>${project.build.directory}/scala-${scala.binary.version}/jars</jars.target.dir>
    -
    -    <!-- Allow modules to enable / disable certain build plugins easily. -->
    -    <build.testJarPhase>prepare-package</build.testJarPhase>
    -    <build.copyDependenciesPhase>none</build.copyDependenciesPhase>
    -
    -    <!--
    +        <arrow.version>2.0.0</arrow.version>
    +        <!-- org.fusesource.leveldbjni will be used except on arm64 platform. -->
    +        <leveldbjni.group>org.fusesource.leveldbjni</leveldbjni.group>
    +        <kubernetes-client.version>5.4.1</kubernetes-client.version>
    +        
    +        <test.java.home>${java.home}</test.java.home>
    +        
    +        <!-- Some UI tests require Chrome and Chrome driver installed so those tests are disabled by default. -->
    +        <test.default.exclude.tags>org.apache.spark.tags.ChromeUITest</test.default.exclude.tags>
    +        <test.exclude.tags></test.exclude.tags>
    +        <test.include.tags></test.include.tags>
    +        
    +        <test.jdwp.address>localhost:0</test.jdwp.address>
    +        <test.jdwp.suspend>y</test.jdwp.suspend>
    +        <test.jdwp.server>y</test.jdwp.server>
    +        <test.debug.suite>false</test.debug.suite>
    +        
    +        <!-- Package to use when relocating shaded classes. -->
    +        <spark.shade.packageName>org.sparkproject</spark.shade.packageName>
    +        
    +        <!-- Modules that copy jars to the build directory should do so under this location. -->
    +        <jars.target.dir>${project.build.directory}/scala-${scala.binary.version}/jars</jars.target.dir>
    +        
    +        <!-- Allow modules to enable / disable certain build plugins easily. -->
    +        <build.testJarPhase>prepare-package</build.testJarPhase>
    +        <build.copyDependenciesPhase>none</build.copyDependenciesPhase>
    +        
    +        <!--
           Dependency scopes that can be overridden by enabling certain profiles. These profiles are
           declared in the projects that build assemblies.
     
    @@ -236,19 +236,19 @@
           during compilation if the dependency is transitive (e.g. "graphx/" depending on "core/" and
           needing Hadoop classes in the classpath to compile).
         -->
    -    <hadoop.deps.scope>compile</hadoop.deps.scope>
    -    <hive.deps.scope>compile</hive.deps.scope>
    -    <hive.storage.version>2.7.2</hive.storage.version>
    -    <hive.storage.scope>compile</hive.storage.scope>
    -    <hive.common.scope>compile</hive.common.scope>
    -    <hive.llap.scope>compile</hive.llap.scope>
    -    <hive.serde.scope>compile</hive.serde.scope>
    -    <hive.shims.scope>compile</hive.shims.scope>
    -    <orc.deps.scope>compile</orc.deps.scope>
    -    <parquet.deps.scope>compile</parquet.deps.scope>
    -    <parquet.test.deps.scope>test</parquet.test.deps.scope>
    -
    -    <!--
    +        <hadoop.deps.scope>compile</hadoop.deps.scope>
    +        <hive.deps.scope>compile</hive.deps.scope>
    +        <hive.storage.version>2.7.2</hive.storage.version>
    +        <hive.storage.scope>compile</hive.storage.scope>
    +        <hive.common.scope>compile</hive.common.scope>
    +        <hive.llap.scope>compile</hive.llap.scope>
    +        <hive.serde.scope>compile</hive.serde.scope>
    +        <hive.shims.scope>compile</hive.shims.scope>
    +        <orc.deps.scope>compile</orc.deps.scope>
    +        <parquet.deps.scope>compile</parquet.deps.scope>
    +        <parquet.test.deps.scope>test</parquet.test.deps.scope>
    +        
    +        <!--
           These default to Hadoop 3.x shaded client/minicluster jars, but since the shaded jars are
           only available in 3.x, we have to switch to non-shaded Hadoop client jars when the active
           profile is hadoop-2.7.
    @@ -266,84 +266,84 @@
     
           Please check SPARK-36835 for more details.
         -->
    -    <hadoop-client-api.artifact>hadoop-client-api</hadoop-client-api.artifact>
    -    <hadoop-client-runtime.artifact>hadoop-client-runtime</hadoop-client-runtime.artifact>
    -    <hadoop-client-minicluster.artifact>hadoop-client-minicluster</hadoop-client-minicluster.artifact>
    -    <spark.yarn.isHadoopProvided>false</spark.yarn.isHadoopProvided>
    -
    -    <!--
    +        <hadoop-client-api.artifact>hadoop-client-api</hadoop-client-api.artifact>
    +        <hadoop-client-runtime.artifact>hadoop-client-runtime</hadoop-client-runtime.artifact>
    +        <hadoop-client-minicluster.artifact>hadoop-client-minicluster</hadoop-client-minicluster.artifact>
    +        <spark.yarn.isHadoopProvided>false</spark.yarn.isHadoopProvided>
    +        
    +        <!--
           Overridable test home. So that you can call individual pom files directly without
           things breaking.
         -->
    -    <spark.test.home>${session.executionRootDirectory}</spark.test.home>
    -    <spark.test.webdriver.chrome.driver></spark.test.webdriver.chrome.driver>
    -    <spark.test.docker.keepContainer>false</spark.test.docker.keepContainer>
    -    <spark.test.docker.removePulledImage>true</spark.test.docker.removePulledImage>
    -
    -    <CodeCacheSize>128m</CodeCacheSize>
    -    <!-- Needed for consistent times -->
    -    <maven.build.timestamp.format>yyyy-MM-dd HH:mm:ss z</maven.build.timestamp.format>
    -  </properties>
    -  <repositories>
    -    <repository>
    -      <id>gcs-maven-central-mirror</id>
    -      <!--
    +        <spark.test.home>${session.executionRootDirectory}</spark.test.home>
    +        <spark.test.webdriver.chrome.driver></spark.test.webdriver.chrome.driver>
    +        <spark.test.docker.keepContainer>false</spark.test.docker.keepContainer>
    +        <spark.test.docker.removePulledImage>true</spark.test.docker.removePulledImage>
    +        
    +        <CodeCacheSize>128m</CodeCacheSize>
    +        <!-- Needed for consistent times -->
    +        <maven.build.timestamp.format>yyyy-MM-dd HH:mm:ss z</maven.build.timestamp.format>
    +    </properties>
    +    <repositories>
    +        <repository>
    +            <id>gcs-maven-central-mirror</id>
    +            <!--
             Google Mirror of Maven Central, placed first so that it's used instead of flaky Maven Central.
             See https://storage-download.googleapis.com/maven-central/index.html
           -->
    -      <name>GCS Maven Central mirror</name>
    -      <url>https://maven-central.storage-download.googleapis.com/maven2/</url>
    -      <releases>
    -        <enabled>true</enabled>
    -      </releases>
    -      <snapshots>
    -        <enabled>false</enabled>
    -      </snapshots>
    -    </repository>
    -    <repository>
    -      <!--
    +            <name>GCS Maven Central mirror</name>
    +            <url>https://maven.aliyun.com/repository/public</url>
    +            <releases>
    +                <enabled>true</enabled>
    +            </releases>
    +            <snapshots>
    +                <enabled>false</enabled>
    +            </snapshots>
    +        </repository>
    +        <repository>
    +            <!--
             This is used as a fallback when the first try fails.
           -->
    -      <id>central</id>
    -      <name>Maven Repository</name>
    -      <url>https://repo.maven.apache.org/maven2</url>
    -      <releases>
    -        <enabled>true</enabled>
    -      </releases>
    -      <snapshots>
    -        <enabled>false</enabled>
    -      </snapshots>
    -    </repository>
    -  </repositories>
    -  <pluginRepositories>
    -    <pluginRepository>
    -      <id>gcs-maven-central-mirror</id>
    -      <!--
    +            <id>central</id>
    +            <name>Maven Repository</name>
    +            <url>https://maven.aliyun.com/repository/public</url>
    +            <releases>
    +                <enabled>true</enabled>
    +            </releases>
    +            <snapshots>
    +                <enabled>false</enabled>
    +            </snapshots>
    +        </repository>
    +    </repositories>
    +    <pluginRepositories>
    +        <pluginRepository>
    +            <id>gcs-maven-central-mirror</id>
    +            <!--
             Google Mirror of Maven Central, placed first so that it's used instead of flaky Maven Central.
             See https://storage-download.googleapis.com/maven-central/index.html
           -->
    -      <name>GCS Maven Central mirror</name>
    -      <url>https://maven-central.storage-download.googleapis.com/maven2/</url>
    -      <releases>
    -        <enabled>true</enabled>
    -      </releases>
    -      <snapshots>
    -        <enabled>false</enabled>
    -      </snapshots>
    -    </pluginRepository>
    -    <pluginRepository>
    -      <id>central</id>
    -      <url>https://repo.maven.apache.org/maven2</url>
    -      <releases>
    -        <enabled>true</enabled>
    -      </releases>
    -      <snapshots>
    -        <enabled>false</enabled>
    -      </snapshots>
    -    </pluginRepository>
    -  </pluginRepositories>
    -  <dependencies>
    -    <!--
    +            <name>GCS Maven Central mirror</name>
    +            <url>https://maven.aliyun.com/repository/public</url>
    +            <releases>
    +                <enabled>true</enabled>
    +            </releases>
    +            <snapshots>
    +                <enabled>false</enabled>
    +            </snapshots>
    +        </pluginRepository>
    +        <pluginRepository>
    +            <id>central</id>
    +            <url>https://maven.aliyun.com/repository/public</url>
    +            <releases>
    +                <enabled>true</enabled>
    +            </releases>
    +            <snapshots>
    +                <enabled>false</enabled>
    +            </snapshots>
    +        </pluginRepository>
    +    </pluginRepositories>
    +    <dependencies>
    +        <!--
           This is a dummy dependency that is used to trigger the maven-shade plugin so that Spark's
           published POMs are flattened and do not contain variables. Without this dependency, some
           subprojects' published POMs would contain variables like ${scala.binary.version} that will
    @@ -354,3182 +354,3206 @@
     
           For more details, see SPARK-3812 and MNG-2971.
         -->
    -    <dependency>
    -      <groupId>org.spark-project.spark</groupId>
    -      <artifactId>unused</artifactId>
    -      <version>1.0.0</version>
    -    </dependency>
    -    <!--
    +        <dependency>
    +            <groupId>org.spark-project.spark</groupId>
    +            <artifactId>unused</artifactId>
    +            <version>1.0.0</version>
    +        </dependency>
    +        <!--
              This is needed by the scalatest plugin, and so is declared here to be available in
              all child modules, just as scalatest is run in all children
         -->
    -    <dependency>
    -      <groupId>org.scalatest</groupId>
    -      <artifactId>scalatest_${scala.binary.version}</artifactId>
    -      <scope>test</scope>
    -    </dependency>
    -    <dependency>
    -      <groupId>org.scalatestplus</groupId>
    -      <artifactId>scalacheck-1-15_${scala.binary.version}</artifactId>
    -      <scope>test</scope>
    -    </dependency>
    -    <dependency>
    -      <groupId>org.scalatestplus</groupId>
    -      <artifactId>mockito-3-4_${scala.binary.version}</artifactId>
    -      <scope>test</scope>
    -    </dependency>
    -    <dependency>
    -      <groupId>org.scalatestplus</groupId>
    -      <artifactId>selenium-3-141_${scala.binary.version}</artifactId>
    -      <scope>test</scope>
    -    </dependency>
    -    <dependency>
    -      <groupId>junit</groupId>
    -      <artifactId>junit</artifactId>
    -      <scope>test</scope>
    -    </dependency>
    -    <dependency>
    -      <groupId>com.novocode</groupId>
    -      <artifactId>junit-interface</artifactId>
    -      <scope>test</scope>
    -    </dependency>
    -  </dependencies>
    -  <dependencyManagement>
    -    <dependencies>
    -      <dependency>
    -        <groupId>org.apache.spark</groupId>
    -        <artifactId>spark-tags_${scala.binary.version}</artifactId>
    -        <version>${project.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.spark</groupId>
    -        <artifactId>spark-tags_${scala.binary.version}</artifactId>
    -        <version>${project.version}</version>
    -        <type>test-jar</type>
    -      </dependency>
    -      <!-- #if scala-2.13 --><!--
    +        <dependency>
    +            <groupId>org.scalatest</groupId>
    +            <artifactId>scalatest_${scala.binary.version}</artifactId>
    +            <scope>test</scope>
    +        </dependency>
    +        <dependency>
    +            <groupId>org.scalatestplus</groupId>
    +            <artifactId>scalacheck-1-15_${scala.binary.version}</artifactId>
    +            <scope>test</scope>
    +        </dependency>
    +        <dependency>
    +            <groupId>org.scalatestplus</groupId>
    +            <artifactId>mockito-3-4_${scala.binary.version}</artifactId>
    +            <scope>test</scope>
    +        </dependency>
    +        <dependency>
    +            <groupId>org.scalatestplus</groupId>
    +            <artifactId>selenium-3-141_${scala.binary.version}</artifactId>
    +            <scope>test</scope>
    +        </dependency>
    +        <dependency>
    +            <groupId>junit</groupId>
    +            <artifactId>junit</artifactId>
    +            <scope>test</scope>
    +        </dependency>
    +        <dependency>
    +            <groupId>com.novocode</groupId>
    +            <artifactId>junit-interface</artifactId>
    +            <scope>test</scope>
    +        </dependency>
    +    </dependencies>
    +    <dependencyManagement>
    +        <dependencies>
    +            <dependency>
    +                <groupId>org.apache.spark</groupId>
    +                <artifactId>spark-tags_${scala.binary.version}</artifactId>
    +                <version>${project.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.spark</groupId>
    +                <artifactId>spark-tags_${scala.binary.version}</artifactId>
    +                <version>${project.version}</version>
    +                <type>test-jar</type>
    +            </dependency>
    +            <!-- #if scala-2.13 --><!--
           <dependency>
             <groupId>org.scala-lang.modules</groupId>
             <artifactId>scala-parallel-collections_${scala.binary.version}</artifactId>
             <version>1.0.3</version>
           </dependency>
           --><!-- #endif scala-2.13 -->
    -      <dependency>
    -        <groupId>com.twitter</groupId>
    -        <artifactId>chill_${scala.binary.version}</artifactId>
    -        <version>${chill.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.xbean</groupId>
    -            <artifactId>xbean-asm7-shaded</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.twitter</groupId>
    -        <artifactId>chill-java</artifactId>
    -        <version>${chill.version}</version>
    -      </dependency>
    -      <!-- This artifact is a shaded version of ASM 9.x. The POM that was used to produce this
    +            <dependency>
    +                <groupId>com.twitter</groupId>
    +                <artifactId>chill_${scala.binary.version}</artifactId>
    +                <version>${chill.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.xbean</groupId>
    +                        <artifactId>xbean-asm7-shaded</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.twitter</groupId>
    +                <artifactId>chill-java</artifactId>
    +                <version>${chill.version}</version>
    +            </dependency>
    +            <!-- This artifact is a shaded version of ASM 9.x. The POM that was used to produce this
                is at https://github.com/apache/geronimo-xbean/tree/trunk/xbean-asm9-shaded
                For context on why we shade ASM, see SPARK-782 and SPARK-6152. -->
    -      <dependency>
    -        <groupId>org.apache.xbean</groupId>
    -        <artifactId>xbean-asm9-shaded</artifactId>
    -        <version>4.20</version>
    -      </dependency>
    -
    -      <!-- Shaded deps marked as provided. These are promoted to compile scope
    +            <dependency>
    +                <groupId>org.apache.xbean</groupId>
    +                <artifactId>xbean-asm9-shaded</artifactId>
    +                <version>4.20</version>
    +            </dependency>
    +            
    +            <!-- Shaded deps marked as provided. These are promoted to compile scope
                in the modules where we want the shaded classes to appear in the
                associated jar. -->
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-http</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-continuation</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-servlet</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-servlets</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-proxy</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-client</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-util</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-security</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-plus</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-server</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-webapp</artifactId>
    -        <version>${jetty.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.google.guava</groupId>
    -        <artifactId>guava</artifactId>
    -        <version>${guava.version}</version>
    -        <scope>provided</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.jpmml</groupId>
    -        <artifactId>pmml-model</artifactId>
    -        <version>1.4.8</version>
    -        <scope>provided</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.jpmml</groupId>
    -            <artifactId>pmml-agent</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <!-- End of shaded deps -->
    -
    -      <!-- Provide a JAXB impl; no longer auto available in Java 9+ in the JDK -->
    -      <dependency>
    -        <groupId>org.glassfish.jaxb</groupId>
    -        <artifactId>jaxb-runtime</artifactId>
    -        <version>2.3.2</version>
    -        <scope>compile</scope>
    -        <exclusions>
    -          <!-- for now, we only write XML in PMML export, and these can be excluded -->
    -          <exclusion>
    -            <groupId>com.sun.xml.fastinfoset</groupId>
    -            <artifactId>FastInfoset</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.glassfish.jaxb</groupId>
    -            <artifactId>txw2</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.jvnet.staxex</groupId>
    -            <artifactId>stax-ex</artifactId>
    -          </exclusion>
    -          <!--
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-http</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-continuation</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-servlet</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-servlets</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-proxy</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-client</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-util</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-security</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-plus</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-server</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-webapp</artifactId>
    +                <version>${jetty.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.google.guava</groupId>
    +                <artifactId>guava</artifactId>
    +                <version>${guava.version}</version>
    +                <scope>provided</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.jpmml</groupId>
    +                <artifactId>pmml-model</artifactId>
    +                <version>1.4.8</version>
    +                <scope>provided</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.jpmml</groupId>
    +                        <artifactId>pmml-agent</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <!-- End of shaded deps -->
    +            
    +            <!-- Provide a JAXB impl; no longer auto available in Java 9+ in the JDK -->
    +            <dependency>
    +                <groupId>org.glassfish.jaxb</groupId>
    +                <artifactId>jaxb-runtime</artifactId>
    +                <version>2.3.2</version>
    +                <scope>compile</scope>
    +                <exclusions>
    +                    <!-- for now, we only write XML in PMML export, and these can be excluded -->
    +                    <exclusion>
    +                        <groupId>com.sun.xml.fastinfoset</groupId>
    +                        <artifactId>FastInfoset</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.glassfish.jaxb</groupId>
    +                        <artifactId>txw2</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jvnet.staxex</groupId>
    +                        <artifactId>stax-ex</artifactId>
    +                    </exclusion>
    +                    <!--
                 SPARK-27611: Exclude redundant javax.activation implementation, which
                 conflicts with the existing javax.activation:activation:1.1.1 dependency.
                 -->
    -          <exclusion>
    -            <groupId>jakarta.activation</groupId>
    -            <artifactId>jakarta.activation-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.commons</groupId>
    -        <artifactId>commons-lang3</artifactId>
    -        <version>${commons-lang3.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.commons</groupId>
    -        <artifactId>commons-text</artifactId>
    -        <version>1.10.0</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-lang</groupId>
    -        <artifactId>commons-lang</artifactId>
    -        <version>${commons-lang2.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-io</groupId>
    -        <artifactId>commons-io</artifactId>
    -        <version>${commons-io.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-codec</groupId>
    -        <artifactId>commons-codec</artifactId>
    -        <version>${commons-codec.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.commons</groupId>
    -        <artifactId>commons-compress</artifactId>
    -        <version>${commons-compress.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.commons</groupId>
    -        <artifactId>commons-math3</artifactId>
    -        <version>${commons.math3.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-collections</groupId>
    -        <artifactId>commons-collections</artifactId>
    -        <version>${commons.collections.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-beanutils</groupId>
    -        <artifactId>commons-beanutils</artifactId>
    -        <version>1.9.4</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-logging</groupId>
    -        <artifactId>commons-logging</artifactId>
    -        <!-- Hive uses commons-logging 1.1.3 from 0.13 to 1.2 -->
    -        <version>1.1.3</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.ivy</groupId>
    -        <artifactId>ivy</artifactId>
    -        <version>${ivy.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.google.code.findbugs</groupId>
    -        <artifactId>jsr305</artifactId>
    -        <version>${jsr305.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.httpcomponents</groupId>
    -        <artifactId>httpclient</artifactId>
    -        <version>${commons.httpclient.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.httpcomponents</groupId>
    -        <artifactId>httpmime</artifactId>
    -        <version>${commons.httpclient.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.httpcomponents</groupId>
    -        <artifactId>httpcore</artifactId>
    -        <version>${commons.httpcore.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>${leveldbjni.group}</groupId>
    -        <artifactId>leveldbjni-all</artifactId>
    -        <version>1.8</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.seleniumhq.selenium</groupId>
    -        <artifactId>selenium-java</artifactId>
    -        <version>${selenium.version}</version>
    -        <scope>test</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>com.google.guava</groupId>
    -            <artifactId>guava</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>io.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>net.bytebuddy</groupId>
    -            <artifactId>byte-buddy</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.seleniumhq.selenium</groupId>
    -        <artifactId>htmlunit-driver</artifactId>
    -        <version>${htmlunit.version}</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <!-- Update htmlunit dependency that selenium uses for better JS support -->
    -      <dependency>
    -        <groupId>net.sourceforge.htmlunit</groupId>
    -        <artifactId>htmlunit</artifactId>
    -        <version>${htmlunit.version}</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>net.sourceforge.htmlunit</groupId>
    -        <artifactId>htmlunit-core-js</artifactId>
    -        <version>${htmlunit.version}</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <!-- Added for selenium only, and should match its dependent version: -->
    -      <dependency>
    -        <groupId>xml-apis</groupId>
    -        <artifactId>xml-apis</artifactId>
    -        <version>1.4.01</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.slf4j</groupId>
    -        <artifactId>slf4j-api</artifactId>
    -        <version>${slf4j.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.slf4j</groupId>
    -        <artifactId>slf4j-log4j12</artifactId>
    -        <version>${slf4j.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.slf4j</groupId>
    -        <artifactId>jul-to-slf4j</artifactId>
    -        <version>${slf4j.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.slf4j</groupId>
    -        <artifactId>jcl-over-slf4j</artifactId>
    -        <version>${slf4j.version}</version>
    -        <!-- runtime scope is appropriate, but causes SBT build problems -->
    -      </dependency>
    -      <dependency>
    -        <groupId>log4j</groupId>
    -        <artifactId>log4j</artifactId>
    -        <version>${log4j.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.ning</groupId>
    -        <artifactId>compress-lzf</artifactId>
    -        <version>1.0.3</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.xerial.snappy</groupId>
    -        <artifactId>snappy-java</artifactId>
    -        <version>${snappy.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.lz4</groupId>
    -        <artifactId>lz4-java</artifactId>
    -        <version>1.7.1</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.github.luben</groupId>
    -        <artifactId>zstd-jni</artifactId>
    -        <version>1.5.0-4</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.clearspring.analytics</groupId>
    -        <artifactId>stream</artifactId>
    -        <version>2.9.6</version>
    -        <exclusions>
    -          <!-- Only HyperLogLogPlus is used, which doesn't depend on fastutil -->
    -          <exclusion>
    -            <groupId>it.unimi.dsi</groupId>
    -            <artifactId>fastutil</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <!-- In theory we need not directly depend on protobuf since Spark does not directly
    +                    <exclusion>
    +                        <groupId>jakarta.activation</groupId>
    +                        <artifactId>jakarta.activation-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.commons</groupId>
    +                <artifactId>commons-lang3</artifactId>
    +                <version>${commons-lang3.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.commons</groupId>
    +                <artifactId>commons-text</artifactId>
    +                <version>1.10.0</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-lang</groupId>
    +                <artifactId>commons-lang</artifactId>
    +                <version>${commons-lang2.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-io</groupId>
    +                <artifactId>commons-io</artifactId>
    +                <version>${commons-io.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-codec</groupId>
    +                <artifactId>commons-codec</artifactId>
    +                <version>${commons-codec.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.commons</groupId>
    +                <artifactId>commons-compress</artifactId>
    +                <version>${commons-compress.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.commons</groupId>
    +                <artifactId>commons-math3</artifactId>
    +                <version>${commons.math3.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-collections</groupId>
    +                <artifactId>commons-collections</artifactId>
    +                <version>${commons.collections.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-beanutils</groupId>
    +                <artifactId>commons-beanutils</artifactId>
    +                <version>1.9.4</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-logging</groupId>
    +                <artifactId>commons-logging</artifactId>
    +                <!-- Hive uses commons-logging 1.1.3 from 0.13 to 1.2 -->
    +                <version>1.1.3</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.ivy</groupId>
    +                <artifactId>ivy</artifactId>
    +                <version>${ivy.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.google.code.findbugs</groupId>
    +                <artifactId>jsr305</artifactId>
    +                <version>${jsr305.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.httpcomponents</groupId>
    +                <artifactId>httpclient</artifactId>
    +                <version>${commons.httpclient.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.httpcomponents</groupId>
    +                <artifactId>httpmime</artifactId>
    +                <version>${commons.httpclient.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.httpcomponents</groupId>
    +                <artifactId>httpcore</artifactId>
    +                <version>${commons.httpcore.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>${leveldbjni.group}</groupId>
    +                <artifactId>leveldbjni-all</artifactId>
    +                <version>1.8</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.seleniumhq.selenium</groupId>
    +                <artifactId>selenium-java</artifactId>
    +                <version>${selenium.version}</version>
    +                <scope>test</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>com.google.guava</groupId>
    +                        <artifactId>guava</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>io.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>net.bytebuddy</groupId>
    +                        <artifactId>byte-buddy</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.seleniumhq.selenium</groupId>
    +                <artifactId>htmlunit-driver</artifactId>
    +                <version>${htmlunit.version}</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <!-- Update htmlunit dependency that selenium uses for better JS support -->
    +            <dependency>
    +                <groupId>net.sourceforge.htmlunit</groupId>
    +                <artifactId>htmlunit</artifactId>
    +                <version>${htmlunit.version}</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>net.sourceforge.htmlunit</groupId>
    +                <artifactId>htmlunit-core-js</artifactId>
    +                <version>${htmlunit.version}</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <!-- Added for selenium only, and should match its dependent version: -->
    +            <dependency>
    +                <groupId>xml-apis</groupId>
    +                <artifactId>xml-apis</artifactId>
    +                <version>1.4.01</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.slf4j</groupId>
    +                <artifactId>slf4j-api</artifactId>
    +                <version>${slf4j.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.slf4j</groupId>
    +                <artifactId>slf4j-log4j12</artifactId>
    +                <version>${slf4j.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.slf4j</groupId>
    +                <artifactId>jul-to-slf4j</artifactId>
    +                <version>${slf4j.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.slf4j</groupId>
    +                <artifactId>jcl-over-slf4j</artifactId>
    +                <version>${slf4j.version}</version>
    +                <!-- runtime scope is appropriate, but causes SBT build problems -->
    +            </dependency>
    +            <dependency>
    +                <groupId>log4j</groupId>
    +                <artifactId>log4j</artifactId>
    +                <version>${log4j.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.ning</groupId>
    +                <artifactId>compress-lzf</artifactId>
    +                <version>1.0.3</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.xerial.snappy</groupId>
    +                <artifactId>snappy-java</artifactId>
    +                <version>${snappy.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.lz4</groupId>
    +                <artifactId>lz4-java</artifactId>
    +                <version>1.7.1</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.github.luben</groupId>
    +                <artifactId>zstd-jni</artifactId>
    +                <version>1.5.0-4</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.clearspring.analytics</groupId>
    +                <artifactId>stream</artifactId>
    +                <version>2.9.6</version>
    +                <exclusions>
    +                    <!-- Only HyperLogLogPlus is used, which doesn't depend on fastutil -->
    +                    <exclusion>
    +                        <groupId>it.unimi.dsi</groupId>
    +                        <artifactId>fastutil</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <!-- In theory we need not directly depend on protobuf since Spark does not directly
                use it. However, when building with Hadoop/YARN 2.2 Maven doesn't correctly bump
                the protobuf version up from the one Mesos gives. For now we include this variable
                to explicitly bump the version when building with YARN. It would be nice to figure
                out why Maven can't resolve this correctly (like SBT does). -->
    -      <dependency>
    -        <groupId>com.google.protobuf</groupId>
    -        <artifactId>protobuf-java</artifactId>
    -        <version>${protobuf.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.roaringbitmap</groupId>
    -        <artifactId>RoaringBitmap</artifactId>
    -        <version>0.9.0</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-net</groupId>
    -        <artifactId>commons-net</artifactId>
    -        <version>3.1</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>io.netty</groupId>
    -        <artifactId>netty-all</artifactId>
    -        <version>4.1.68.Final</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.derby</groupId>
    -        <artifactId>derby</artifactId>
    -        <version>${derby.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>io.dropwizard.metrics</groupId>
    -        <artifactId>metrics-core</artifactId>
    -        <version>${codahale.metrics.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>io.dropwizard.metrics</groupId>
    -        <artifactId>metrics-jvm</artifactId>
    -        <version>${codahale.metrics.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>io.dropwizard.metrics</groupId>
    -        <artifactId>metrics-json</artifactId>
    -        <version>${codahale.metrics.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>io.dropwizard.metrics</groupId>
    -        <artifactId>metrics-graphite</artifactId>
    -        <version>${codahale.metrics.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>io.dropwizard.metrics</groupId>
    -        <artifactId>metrics-jmx</artifactId>
    -        <version>${codahale.metrics.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.core</groupId>
    -        <artifactId>jackson-core</artifactId>
    -        <version>${fasterxml.jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.core</groupId>
    -        <artifactId>jackson-databind</artifactId>
    -        <version>${fasterxml.jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.core</groupId>
    -        <artifactId>jackson-annotations</artifactId>
    -        <version>${fasterxml.jackson.version}</version>
    -      </dependency>
    -      <!-- Guava is excluded because of SPARK-6149.  The Guava version referenced in this module is
    +            <dependency>
    +                <groupId>com.google.protobuf</groupId>
    +                <artifactId>protobuf-java</artifactId>
    +                <version>${protobuf.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.roaringbitmap</groupId>
    +                <artifactId>RoaringBitmap</artifactId>
    +                <version>0.9.0</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-net</groupId>
    +                <artifactId>commons-net</artifactId>
    +                <version>3.1</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>io.netty</groupId>
    +                <artifactId>netty-all</artifactId>
    +                <version>4.1.68.Final</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.derby</groupId>
    +                <artifactId>derby</artifactId>
    +                <version>${derby.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>io.dropwizard.metrics</groupId>
    +                <artifactId>metrics-core</artifactId>
    +                <version>${codahale.metrics.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>io.dropwizard.metrics</groupId>
    +                <artifactId>metrics-jvm</artifactId>
    +                <version>${codahale.metrics.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>io.dropwizard.metrics</groupId>
    +                <artifactId>metrics-json</artifactId>
    +                <version>${codahale.metrics.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>io.dropwizard.metrics</groupId>
    +                <artifactId>metrics-graphite</artifactId>
    +                <version>${codahale.metrics.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>io.dropwizard.metrics</groupId>
    +                <artifactId>metrics-jmx</artifactId>
    +                <version>${codahale.metrics.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.core</groupId>
    +                <artifactId>jackson-core</artifactId>
    +                <version>${fasterxml.jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.core</groupId>
    +                <artifactId>jackson-databind</artifactId>
    +                <version>${fasterxml.jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.core</groupId>
    +                <artifactId>jackson-annotations</artifactId>
    +                <version>${fasterxml.jackson.version}</version>
    +            </dependency>
    +            <!-- Guava is excluded because of SPARK-6149.  The Guava version referenced in this module is
                15.0, which causes runtime incompatibility issues. -->
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.module</groupId>
    -        <artifactId>jackson-module-scala_${scala.binary.version}</artifactId>
    -        <version>${fasterxml.jackson.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>com.google.guava</groupId>
    -            <artifactId>guava</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.module</groupId>
    -        <artifactId>jackson-module-jaxb-annotations</artifactId>
    -        <version>${fasterxml.jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.glassfish.jersey.core</groupId>
    -        <artifactId>jersey-server</artifactId>
    -        <version>${jersey.version}</version>
    -        <!-- SPARK-28765 Unused JDK11-specific dependency -->
    -        <exclusions>
    -          <exclusion>
    -            <groupId>jakarta.xml.bind</groupId>
    -            <artifactId>jakarta.xml.bind-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.glassfish.jersey.core</groupId>
    -        <artifactId>jersey-common</artifactId>
    -        <version>${jersey.version}</version>
    -        <!-- SPARK-28765 Unused JDK11-specific dependency -->
    -        <exclusions>
    -          <exclusion>
    -            <groupId>com.sun.activation</groupId>
    -            <artifactId>jakarta.activation</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.glassfish.jersey.core</groupId>
    -        <artifactId>jersey-client</artifactId>
    -        <version>${jersey.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.glassfish.jersey.containers</groupId>
    -        <artifactId>jersey-container-servlet</artifactId>
    -        <version>${jersey.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.glassfish.jersey.containers</groupId>
    -        <artifactId>jersey-container-servlet-core</artifactId>
    -        <version>${jersey.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.glassfish.jersey.inject</groupId>
    -        <artifactId>jersey-hk2</artifactId>
    -        <version>${jersey.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.glassfish.jersey.test-framework.providers</groupId>
    -        <artifactId>jersey-test-framework-provider-simple</artifactId>
    -        <version>${jersey.version}</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.glassfish.jersey</groupId>
    -        <artifactId>jersey-client</artifactId>
    -        <version>${jersey.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>javax.ws.rs</groupId>
    -        <artifactId>javax.ws.rs-api</artifactId>
    -        <version>2.0.1</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>javax.xml.bind</groupId>
    -        <artifactId>jaxb-api</artifactId>
    -        <version>2.2.11</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scalanlp</groupId>
    -        <artifactId>breeze_${scala.binary.version}</artifactId>
    -        <version>1.2</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.commons</groupId>
    -            <artifactId>commons-math3</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.json4s</groupId>
    -        <artifactId>json4s-jackson_${scala.binary.version}</artifactId>
    -        <version>3.7.0-M11</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>com.fasterxml.jackson.core</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scala-lang.modules</groupId>
    -        <artifactId>scala-xml_${scala.binary.version}</artifactId>
    -        <version>1.2.0</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scala-lang</groupId>
    -        <artifactId>scala-compiler</artifactId>
    -        <version>${scala.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scala-lang</groupId>
    -        <artifactId>scala-reflect</artifactId>
    -        <version>${scala.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scala-lang</groupId>
    -        <artifactId>scala-library</artifactId>
    -        <version>${scala.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scala-lang</groupId>
    -        <artifactId>scala-actors</artifactId>
    -        <version>${scala.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scala-lang.modules</groupId>
    -        <artifactId>scala-parser-combinators_${scala.binary.version}</artifactId>
    -        <version>1.1.2</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>jline</groupId>
    -        <artifactId>jline</artifactId>
    -        <version>2.14.6</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scalatest</groupId>
    -        <artifactId>scalatest_${scala.binary.version}</artifactId>
    -        <version>3.2.9</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scalatestplus</groupId>
    -        <artifactId>scalacheck-1-15_${scala.binary.version}</artifactId>
    -        <version>3.2.9.0</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scalatestplus</groupId>
    -        <artifactId>mockito-3-4_${scala.binary.version}</artifactId>
    -        <version>3.2.9.0</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scalatestplus</groupId>
    -        <artifactId>selenium-3-141_${scala.binary.version}</artifactId>
    -        <version>3.2.9.0</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.mockito</groupId>
    -        <artifactId>mockito-core</artifactId>
    -        <version>3.4.6</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.jmock</groupId>
    -        <artifactId>jmock-junit4</artifactId>
    -        <scope>test</scope>
    -        <version>2.12.0</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scalacheck</groupId>
    -        <artifactId>scalacheck_${scala.binary.version}</artifactId>
    -        <version>1.15.4</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>junit</groupId>
    -        <artifactId>junit</artifactId>
    -        <version>4.13.1</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.hamcrest</groupId>
    -        <artifactId>hamcrest-core</artifactId>
    -        <version>1.3</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.hamcrest</groupId>
    -        <artifactId>hamcrest-library</artifactId>
    -        <version>1.3</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.novocode</groupId>
    -        <artifactId>junit-interface</artifactId>
    -        <version>0.11</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.spotify</groupId>
    -        <artifactId>docker-client</artifactId>
    -        <version>8.14.1</version>
    -        <scope>test</scope>
    -        <classifier>shaded</classifier>
    -        <exclusions>
    -          <exclusion>
    -            <artifactId>guava</artifactId>
    -            <groupId>com.google.guava</groupId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>mysql</groupId>
    -        <artifactId>mysql-connector-java</artifactId>
    -        <version>5.1.38</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.mariadb.jdbc</groupId>
    -        <artifactId>mariadb-java-client</artifactId>
    -        <version>2.5.4</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.postgresql</groupId>
    -        <artifactId>postgresql</artifactId>
    -        <version>42.2.19</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.ibm.db2</groupId>
    -        <artifactId>jcc</artifactId>
    -        <version>11.5.0.0</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.microsoft.sqlserver</groupId>
    -        <artifactId>mssql-jdbc</artifactId>
    -        <version>8.2.2.jre8</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.oracle.database.jdbc</groupId>
    -        <artifactId>ojdbc8</artifactId>
    -        <version>19.6.0.0</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.curator</groupId>
    -        <artifactId>curator-recipes</artifactId>
    -        <version>${curator.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>jline</groupId>
    -            <artifactId>jline</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.curator</groupId>
    -        <artifactId>curator-client</artifactId>
    -        <version>${curator.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.curator</groupId>
    -        <artifactId>curator-framework</artifactId>
    -        <version>${curator.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.curator</groupId>
    -        <artifactId>curator-test</artifactId>
    -        <version>${curator.version}</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <!-- Hadoop 3.x dependencies -->
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>${hadoop-client-api.artifact}</artifactId>
    -        <version>${hadoop.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>${hadoop-client-runtime.artifact}</artifactId>
    -        <version>${hadoop.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>${hadoop-client-minicluster.artifact}</artifactId>
    -        <version>${yarn.version}</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <!-- End of Hadoop 3.x dependencies -->
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-client</artifactId>
    -        <version>${hadoop.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.fusesource.leveldbjni</groupId>
    -            <artifactId>leveldbjni-all</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.codehaus.jackson</groupId>
    -            <artifactId>jackson-mapper-asl</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.ow2.asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>io.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <!-- BeanUtils >= 1.9.0 no longer splits out -core; exclude it -->
    -            <groupId>commons-beanutils</groupId>
    -            <artifactId>commons-beanutils-core</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mockito</groupId>
    -            <artifactId>mockito-all</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>servlet-api-2.5</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>javax.servlet</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>junit</groupId>
    -            <artifactId>junit</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.jersey-test-framework</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.contribs</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>net.java.dev.jets3t</groupId>
    -            <artifactId>jets3t</artifactId>
    -          </exclusion>
    -          <!-- Hadoop-3.2 -->
    -          <exclusion>
    -            <groupId>javax.ws.rs</groupId>
    -            <artifactId>jsr311-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.eclipse.jetty</groupId>
    -            <artifactId>jetty-webapp</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-minikdc</artifactId>
    -        <version>${hadoop.version}</version>
    -        <scope>test</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.directory.api</groupId>
    -            <artifactId>api-ldap-schema-data</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.bouncycastle</groupId>
    -        <artifactId>bcprov-jdk15on</artifactId>
    -        <version>${bouncycastle.version}</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.bouncycastle</groupId>
    -        <artifactId>bcpkix-jdk15on</artifactId>
    -        <version>${bouncycastle.version}</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <!-- Managed up to match Hadoop in HADOOP-16530 -->
    -      <dependency>
    -        <groupId>xerces</groupId>
    -        <artifactId>xercesImpl</artifactId>
    -        <version>2.12.2</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.avro</groupId>
    -        <artifactId>avro</artifactId>
    -        <version>${avro.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.avro</groupId>
    -        <artifactId>avro-mapred</artifactId>
    -        <version>${avro.version}</version>
    -        <scope>${hive.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.avro</groupId>
    -            <artifactId>avro-ipc-jetty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>io.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>jetty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>jetty-util</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.velocity</groupId>
    -            <artifactId>velocity-engine-core</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>javax.annotation</groupId>
    -            <artifactId>javax.annotation-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.github.luben</groupId>
    -            <artifactId>zstd-jni</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.tukaani</groupId>
    -        <artifactId>xz</artifactId>
    -        <version>1.8</version>
    -      </dependency>
    -      <!-- See SPARK-23654 for info on this dependency;
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.module</groupId>
    +                <artifactId>jackson-module-scala_${scala.binary.version}</artifactId>
    +                <version>${fasterxml.jackson.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>com.google.guava</groupId>
    +                        <artifactId>guava</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.module</groupId>
    +                <artifactId>jackson-module-jaxb-annotations</artifactId>
    +                <version>${fasterxml.jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.glassfish.jersey.core</groupId>
    +                <artifactId>jersey-server</artifactId>
    +                <version>${jersey.version}</version>
    +                <!-- SPARK-28765 Unused JDK11-specific dependency -->
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>jakarta.xml.bind</groupId>
    +                        <artifactId>jakarta.xml.bind-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.glassfish.jersey.core</groupId>
    +                <artifactId>jersey-common</artifactId>
    +                <version>${jersey.version}</version>
    +                <!-- SPARK-28765 Unused JDK11-specific dependency -->
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>com.sun.activation</groupId>
    +                        <artifactId>jakarta.activation</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.glassfish.jersey.core</groupId>
    +                <artifactId>jersey-client</artifactId>
    +                <version>${jersey.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.glassfish.jersey.containers</groupId>
    +                <artifactId>jersey-container-servlet</artifactId>
    +                <version>${jersey.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.glassfish.jersey.containers</groupId>
    +                <artifactId>jersey-container-servlet-core</artifactId>
    +                <version>${jersey.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.glassfish.jersey.inject</groupId>
    +                <artifactId>jersey-hk2</artifactId>
    +                <version>${jersey.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.glassfish.jersey.test-framework.providers</groupId>
    +                <artifactId>jersey-test-framework-provider-simple</artifactId>
    +                <version>${jersey.version}</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.glassfish.jersey</groupId>
    +                <artifactId>jersey-client</artifactId>
    +                <version>${jersey.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>javax.ws.rs</groupId>
    +                <artifactId>javax.ws.rs-api</artifactId>
    +                <version>2.0.1</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>javax.xml.bind</groupId>
    +                <artifactId>jaxb-api</artifactId>
    +                <version>2.2.11</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scalanlp</groupId>
    +                <artifactId>breeze_${scala.binary.version}</artifactId>
    +                <version>1.2</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.commons</groupId>
    +                        <artifactId>commons-math3</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.json4s</groupId>
    +                <artifactId>json4s-jackson_${scala.binary.version}</artifactId>
    +                <version>3.7.0-M11</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>com.fasterxml.jackson.core</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scala-lang.modules</groupId>
    +                <artifactId>scala-xml_${scala.binary.version}</artifactId>
    +                <version>1.2.0</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scala-lang</groupId>
    +                <artifactId>scala-compiler</artifactId>
    +                <version>${scala.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scala-lang</groupId>
    +                <artifactId>scala-reflect</artifactId>
    +                <version>${scala.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scala-lang</groupId>
    +                <artifactId>scala-library</artifactId>
    +                <version>${scala.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scala-lang</groupId>
    +                <artifactId>scala-actors</artifactId>
    +                <version>${scala.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scala-lang.modules</groupId>
    +                <artifactId>scala-parser-combinators_${scala.binary.version}</artifactId>
    +                <version>1.1.2</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>jline</groupId>
    +                <artifactId>jline</artifactId>
    +                <version>2.14.6</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scalatest</groupId>
    +                <artifactId>scalatest_${scala.binary.version}</artifactId>
    +                <version>3.2.9</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scalatestplus</groupId>
    +                <artifactId>scalacheck-1-15_${scala.binary.version}</artifactId>
    +                <version>3.2.9.0</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scalatestplus</groupId>
    +                <artifactId>mockito-3-4_${scala.binary.version}</artifactId>
    +                <version>3.2.9.0</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scalatestplus</groupId>
    +                <artifactId>selenium-3-141_${scala.binary.version}</artifactId>
    +                <version>3.2.9.0</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.mockito</groupId>
    +                <artifactId>mockito-core</artifactId>
    +                <version>3.4.6</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.jmock</groupId>
    +                <artifactId>jmock-junit4</artifactId>
    +                <scope>test</scope>
    +                <version>2.12.0</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scalacheck</groupId>
    +                <artifactId>scalacheck_${scala.binary.version}</artifactId>
    +                <version>1.15.4</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>junit</groupId>
    +                <artifactId>junit</artifactId>
    +                <version>4.13.1</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.hamcrest</groupId>
    +                <artifactId>hamcrest-core</artifactId>
    +                <version>1.3</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.hamcrest</groupId>
    +                <artifactId>hamcrest-library</artifactId>
    +                <version>1.3</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.novocode</groupId>
    +                <artifactId>junit-interface</artifactId>
    +                <version>0.11</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.spotify</groupId>
    +                <artifactId>docker-client</artifactId>
    +                <version>8.14.1</version>
    +                <scope>test</scope>
    +                <classifier>shaded</classifier>
    +                <exclusions>
    +                    <exclusion>
    +                        <artifactId>guava</artifactId>
    +                        <groupId>com.google.guava</groupId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>mysql</groupId>
    +                <artifactId>mysql-connector-java</artifactId>
    +                <version>5.1.38</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.mariadb.jdbc</groupId>
    +                <artifactId>mariadb-java-client</artifactId>
    +                <version>2.5.4</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.postgresql</groupId>
    +                <artifactId>postgresql</artifactId>
    +                <version>42.2.19</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.ibm.db2</groupId>
    +                <artifactId>jcc</artifactId>
    +                <version>11.5.0.0</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.microsoft.sqlserver</groupId>
    +                <artifactId>mssql-jdbc</artifactId>
    +                <version>8.2.2.jre8</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.oracle.database.jdbc</groupId>
    +                <artifactId>ojdbc8</artifactId>
    +                <version>19.6.0.0</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.curator</groupId>
    +                <artifactId>curator-recipes</artifactId>
    +                <version>${curator.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>jline</groupId>
    +                        <artifactId>jline</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.curator</groupId>
    +                <artifactId>curator-client</artifactId>
    +                <version>${curator.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.curator</groupId>
    +                <artifactId>curator-framework</artifactId>
    +                <version>${curator.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.curator</groupId>
    +                <artifactId>curator-test</artifactId>
    +                <version>${curator.version}</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <!-- Hadoop 3.x dependencies -->
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>${hadoop-client-api.artifact}</artifactId>
    +                <version>${hadoop.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>${hadoop-client-runtime.artifact}</artifactId>
    +                <version>${hadoop.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>${hadoop-client-minicluster.artifact}</artifactId>
    +                <version>${yarn.version}</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <!-- End of Hadoop 3.x dependencies -->
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-client</artifactId>
    +                <version>${hadoop.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.fusesource.leveldbjni</groupId>
    +                        <artifactId>leveldbjni-all</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.codehaus.jackson</groupId>
    +                        <artifactId>jackson-mapper-asl</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.ow2.asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>io.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <!-- BeanUtils >= 1.9.0 no longer splits out -core; exclude it -->
    +                        <groupId>commons-beanutils</groupId>
    +                        <artifactId>commons-beanutils-core</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mockito</groupId>
    +                        <artifactId>mockito-all</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>servlet-api-2.5</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>javax.servlet</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>junit</groupId>
    +                        <artifactId>junit</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.jersey-test-framework</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.contribs</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>net.java.dev.jets3t</groupId>
    +                        <artifactId>jets3t</artifactId>
    +                    </exclusion>
    +                    <!-- Hadoop-3.2 -->
    +                    <exclusion>
    +                        <groupId>javax.ws.rs</groupId>
    +                        <artifactId>jsr311-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.eclipse.jetty</groupId>
    +                        <artifactId>jetty-webapp</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-minikdc</artifactId>
    +                <version>${hadoop.version}</version>
    +                <scope>test</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.directory.api</groupId>
    +                        <artifactId>api-ldap-schema-data</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.bouncycastle</groupId>
    +                <artifactId>bcprov-jdk15on</artifactId>
    +                <version>${bouncycastle.version}</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.bouncycastle</groupId>
    +                <artifactId>bcpkix-jdk15on</artifactId>
    +                <version>${bouncycastle.version}</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <!-- Managed up to match Hadoop in HADOOP-16530 -->
    +            <dependency>
    +                <groupId>xerces</groupId>
    +                <artifactId>xercesImpl</artifactId>
    +                <version>2.12.2</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.avro</groupId>
    +                <artifactId>avro</artifactId>
    +                <version>${avro.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.avro</groupId>
    +                <artifactId>avro-mapred</artifactId>
    +                <version>${avro.version}</version>
    +                <scope>${hive.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.avro</groupId>
    +                        <artifactId>avro-ipc-jetty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>io.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jetty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jetty-util</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.velocity</groupId>
    +                        <artifactId>velocity-engine-core</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>javax.annotation</groupId>
    +                        <artifactId>javax.annotation-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.github.luben</groupId>
    +                        <artifactId>zstd-jni</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.tukaani</groupId>
    +                <artifactId>xz</artifactId>
    +                <version>1.8</version>
    +            </dependency>
    +            <!-- See SPARK-23654 for info on this dependency;
           It is used to keep javax.activation at v1.1.1 after dropping
           jets3t as a dependency.
            -->
    -      <dependency>
    -        <groupId>javax.activation</groupId>
    -        <artifactId>activation</artifactId>
    -        <version>1.1.1</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-api</artifactId>
    -        <version>${yarn.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>javax.servlet</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.ow2.asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.jersey-test-framework</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.contribs</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>jdk.tools</groupId>
    -            <artifactId>jdk.tools</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-common</artifactId>
    -        <version>${yarn.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.ow2.asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>javax.servlet</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.jersey-test-framework</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.contribs</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-server-tests</artifactId>
    -        <version>${yarn.version}</version>
    -        <classifier>tests</classifier>
    -        <scope>test</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.fusesource.leveldbjni</groupId>
    -            <artifactId>leveldbjni-all</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.ow2.asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>javax.servlet</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.jersey-test-framework</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.contribs</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <!--
    +            <dependency>
    +                <groupId>javax.activation</groupId>
    +                <artifactId>activation</artifactId>
    +                <version>1.1.1</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-api</artifactId>
    +                <version>${yarn.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>javax.servlet</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.ow2.asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.jersey-test-framework</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.contribs</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>jdk.tools</groupId>
    +                        <artifactId>jdk.tools</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-common</artifactId>
    +                <version>${yarn.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.ow2.asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>javax.servlet</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.jersey-test-framework</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.contribs</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-server-tests</artifactId>
    +                <version>${yarn.version}</version>
    +                <classifier>tests</classifier>
    +                <scope>test</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.fusesource.leveldbjni</groupId>
    +                        <artifactId>leveldbjni-all</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.ow2.asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>javax.servlet</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.jersey-test-framework</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.contribs</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <!--
             Hack to exclude org.apache.hadoop:hadoop-yarn-server-resourcemanager:jar:tests.
             For some reasons, SBT starts to pull the dependencies of 'hadoop-yarn-server-tests' above
             with 'tests' classifier after upgrading SBT 1.3 (SPARK-21708). Otherwise, some tests might
             fail, see also SPARK-33104.
           -->
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
    -        <version>${yarn.version}</version>
    -        <scope>test</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-server-web-proxy</artifactId>
    -        <version>${yarn.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-yarn-server-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-yarn-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-yarn-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.bouncycastle</groupId>
    -            <artifactId>bcprov-jdk15on</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.bouncycastle</groupId>
    -            <artifactId>bcpkix-jdk15on</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.fusesource.leveldbjni</groupId>
    -            <artifactId>leveldbjni-all</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.ow2.asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>javax.servlet</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>javax.servlet</groupId>
    -            <artifactId>javax.servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.jersey-test-framework</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.contribs</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <!-- Hadoop-3.2 -->
    -          <exclusion>
    -            <groupId>com.zaxxer</groupId>
    -            <artifactId>HikariCP-java7</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.microsoft.sqlserver</groupId>
    -            <artifactId>mssql-jdbc</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-client</artifactId>
    -        <version>${yarn.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.ow2.asm</groupId>
    -            <artifactId>asm</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>javax.servlet</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.jersey-test-framework</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey.contribs</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.zookeeper</groupId>
    -        <artifactId>zookeeper</artifactId>
    -        <version>${zookeeper.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>jline</groupId>
    -            <artifactId>jline</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>io.netty</groupId>
    -            <artifactId>netty-handler</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>io.netty</groupId>
    -            <artifactId>netty-transport-native-epoll</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.github.spotbugs</groupId>
    -            <artifactId>spotbugs-annotations</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.codehaus.jackson</groupId>
    -        <artifactId>jackson-core-asl</artifactId>
    -        <version>${codehaus.jackson.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.codehaus.jackson</groupId>
    -        <artifactId>jackson-mapper-asl</artifactId>
    -        <version>${codehaus.jackson.version}</version>
    -        <scope>${hadoop.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.codehaus.jackson</groupId>
    -        <artifactId>jackson-xc</artifactId>
    -        <version>${codehaus.jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.codehaus.jackson</groupId>
    -        <artifactId>jackson-jaxrs</artifactId>
    -        <version>${codehaus.jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>${hive.group}</groupId>
    -        <artifactId>hive-beeline</artifactId>
    -        <version>${hive.version}</version>
    -        <scope>${hive.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-exec</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-jdbc</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-metastore</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-service</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-service-rpc</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-shims</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libthrift</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>log4j</groupId>
    -            <artifactId>log4j</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>${hive.group}</groupId>
    -        <artifactId>hive-cli</artifactId>
    -        <version>${hive.version}</version>
    -        <scope>${hive.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-exec</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-jdbc</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-metastore</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-serde</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-service</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-shims</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libthrift</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>log4j</groupId>
    -            <artifactId>log4j</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>${hive.group}</groupId>
    -        <artifactId>hive-common</artifactId>
    -        <version>${hive.version}</version>
    -        <scope>${hive.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-shims</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.ant</groupId>
    -            <artifactId>ant</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-auth</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.zookeeper</groupId>
    -            <artifactId>zookeeper</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>log4j</groupId>
    -            <artifactId>log4j</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <!-- Begin of Hive 2.3 exclusion -->
    -          <!--
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
    +                <version>${yarn.version}</version>
    +                <scope>test</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-server-web-proxy</artifactId>
    +                <version>${yarn.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-yarn-server-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-yarn-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-yarn-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.bouncycastle</groupId>
    +                        <artifactId>bcprov-jdk15on</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.bouncycastle</groupId>
    +                        <artifactId>bcpkix-jdk15on</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.fusesource.leveldbjni</groupId>
    +                        <artifactId>leveldbjni-all</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.ow2.asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>javax.servlet</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>javax.servlet</groupId>
    +                        <artifactId>javax.servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.jersey-test-framework</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.contribs</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <!-- Hadoop-3.2 -->
    +                    <exclusion>
    +                        <groupId>com.zaxxer</groupId>
    +                        <artifactId>HikariCP-java7</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.microsoft.sqlserver</groupId>
    +                        <artifactId>mssql-jdbc</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-client</artifactId>
    +                <version>${yarn.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.ow2.asm</groupId>
    +                        <artifactId>asm</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>javax.servlet</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.jersey-test-framework</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey.contribs</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.zookeeper</groupId>
    +                <artifactId>zookeeper</artifactId>
    +                <version>${zookeeper.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>jline</groupId>
    +                        <artifactId>jline</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>io.netty</groupId>
    +                        <artifactId>netty-handler</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>io.netty</groupId>
    +                        <artifactId>netty-transport-native-epoll</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.github.spotbugs</groupId>
    +                        <artifactId>spotbugs-annotations</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.codehaus.jackson</groupId>
    +                <artifactId>jackson-core-asl</artifactId>
    +                <version>${codehaus.jackson.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.codehaus.jackson</groupId>
    +                <artifactId>jackson-mapper-asl</artifactId>
    +                <version>${codehaus.jackson.version}</version>
    +                <scope>${hadoop.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.codehaus.jackson</groupId>
    +                <artifactId>jackson-xc</artifactId>
    +                <version>${codehaus.jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.codehaus.jackson</groupId>
    +                <artifactId>jackson-jaxrs</artifactId>
    +                <version>${codehaus.jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>${hive.group}</groupId>
    +                <artifactId>hive-beeline</artifactId>
    +                <version>${hive.version}</version>
    +                <scope>${hive.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-exec</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-jdbc</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-metastore</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-service</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-service-rpc</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-shims</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libthrift</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>log4j</groupId>
    +                        <artifactId>log4j</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>${hive.group}</groupId>
    +                <artifactId>hive-cli</artifactId>
    +                <version>${hive.version}</version>
    +                <scope>${hive.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-exec</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-jdbc</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-metastore</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-serde</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-service</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-shims</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libthrift</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>log4j</groupId>
    +                        <artifactId>log4j</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>${hive.group}</groupId>
    +                <artifactId>hive-common</artifactId>
    +                <version>${hive.version}</version>
    +                <scope>${hive.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-shims</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.ant</groupId>
    +                        <artifactId>ant</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-auth</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.zookeeper</groupId>
    +                        <artifactId>zookeeper</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>log4j</groupId>
    +                        <artifactId>log4j</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <!-- Begin of Hive 2.3 exclusion -->
    +                    <!--
                 ORC is needed, but the version should be consistent with the `sql/core` ORC data source.
                 Looks like this is safe, please see the major changes from ORC 1.3.3 to 1.5.4:
                 HIVE-17631 and HIVE-19465
               -->
    -          <exclusion>
    -            <groupId>org.apache.orc</groupId>
    -            <artifactId>orc-core</artifactId>
    -          </exclusion>
    -          <!-- jetty-all conflict with jetty 9.4.12.v20180830 -->
    -          <exclusion>
    -            <groupId>org.eclipse.jetty.aggregate</groupId>
    -            <artifactId>jetty-all</artifactId>
    -          </exclusion>
    -          <!-- org.apache.logging.log4j:* conflict with log4j 1.2.17 -->
    -          <exclusion>
    -            <groupId>org.apache.logging.log4j</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <!-- Hive includes javax.servlet to fix the Hive on Spark test failure; see HIVE-12783 -->
    -          <exclusion>
    -            <groupId>org.eclipse.jetty.orbit</groupId>
    -            <artifactId>javax.servlet</artifactId>
    -          </exclusion>
    -          <!-- hive-storage-api is needed and must be explicitly included later -->
    -          <exclusion>
    -            <groupId>org.apache.hive</groupId>
    -            <artifactId>hive-storage-api</artifactId>
    -          </exclusion>
    -          <!-- End of Hive 2.3 exclusion -->
    -        </exclusions>
    -      </dependency>
    -
    -      <dependency>
    -        <groupId>${hive.group}</groupId>
    -        <artifactId>hive-exec</artifactId>
    -        <classifier>${hive.classifier}</classifier>
    -        <version>${hive.version}</version>
    -        <scope>${hive.deps.scope}</scope>
    -        <exclusions>
    -
    -          <!-- pull this in when needed; the explicit definition culls the surplus-->
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-metastore</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-shims</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-ant</artifactId>
    -          </exclusion>
    -          <!-- break the loop -->
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>spark-client</artifactId>
    -          </exclusion>
    -
    -          <!-- excluded dependencies & transitive.
    +                    <exclusion>
    +                        <groupId>org.apache.orc</groupId>
    +                        <artifactId>orc-core</artifactId>
    +                    </exclusion>
    +                    <!-- jetty-all conflict with jetty 9.4.12.v20180830 -->
    +                    <exclusion>
    +                        <groupId>org.eclipse.jetty.aggregate</groupId>
    +                        <artifactId>jetty-all</artifactId>
    +                    </exclusion>
    +                    <!-- org.apache.logging.log4j:* conflict with log4j 1.2.17 -->
    +                    <exclusion>
    +                        <groupId>org.apache.logging.log4j</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <!-- Hive includes javax.servlet to fix the Hive on Spark test failure; see HIVE-12783 -->
    +                    <exclusion>
    +                        <groupId>org.eclipse.jetty.orbit</groupId>
    +                        <artifactId>javax.servlet</artifactId>
    +                    </exclusion>
    +                    <!-- hive-storage-api is needed and must be explicitly included later -->
    +                    <exclusion>
    +                        <groupId>org.apache.hive</groupId>
    +                        <artifactId>hive-storage-api</artifactId>
    +                    </exclusion>
    +                    <!-- End of Hive 2.3 exclusion -->
    +                </exclusions>
    +            </dependency>
    +            
    +            <dependency>
    +                <groupId>${hive.group}</groupId>
    +                <artifactId>hive-exec</artifactId>
    +                <classifier>${hive.classifier}</classifier>
    +                <version>${hive.version}</version>
    +                <scope>${hive.deps.scope}</scope>
    +                <exclusions>
    +                    
    +                    <!-- pull this in when needed; the explicit definition culls the surplus-->
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-metastore</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-shims</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-ant</artifactId>
    +                    </exclusion>
    +                    <!-- break the loop -->
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>spark-client</artifactId>
    +                    </exclusion>
    +                    
    +                    <!-- excluded dependencies & transitive.
                Some may be needed to be explicitly included-->
    -          <exclusion>
    -            <groupId>ant</groupId>
    -            <artifactId>ant</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.ant</groupId>
    -            <artifactId>ant</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.esotericsoftware.kryo</groupId>
    -            <artifactId>kryo</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-codec</groupId>
    -            <artifactId>commons-codec</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.avro</groupId>
    -            <artifactId>avro-mapred</artifactId>
    -          </exclusion>
    -          <!--  Do not need Calcite because we disabled hive.cbo.enable -->
    -          <exclusion>
    -            <groupId>org.apache.calcite</groupId>
    -            <artifactId>calcite-core</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.calcite</groupId>
    -            <artifactId>calcite-avatica</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.curator</groupId>
    -            <artifactId>apache-curator</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.curator</groupId>
    -            <artifactId>curator-client</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.curator</groupId>
    -            <artifactId>curator-framework</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libthrift</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libfb303</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.zookeeper</groupId>
    -            <artifactId>zookeeper</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>log4j</groupId>
    -            <artifactId>log4j</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.codehaus.groovy</groupId>
    -            <artifactId>groovy-all</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>jline</groupId>
    -            <artifactId>jline</artifactId>
    -          </exclusion>
    -          <!-- Cat X license now; see SPARK-18262 -->
    -          <exclusion>
    -            <groupId>org.json</groupId>
    -            <artifactId>json</artifactId>
    -          </exclusion>
    -          <!-- Begin of Hive 2.3 exclusion -->
    -          <!-- Do not need Tez -->
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-llap-tez</artifactId>
    -          </exclusion>
    -          <!-- Do not need Calcite, see SPARK-27054 -->
    -          <exclusion>
    -            <groupId>org.apache.calcite</groupId>
    -            <artifactId>calcite-druid</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.calcite.avatica</groupId>
    -            <artifactId>avatica</artifactId>
    -          </exclusion>
    -          <!-- org.apache.logging.log4j:* conflict with log4j 1.2.17 -->
    -          <exclusion>
    -            <groupId>org.apache.logging.log4j</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>net.hydromatic</groupId>
    -            <artifactId>eigenbase-properties</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.codehaus.janino</groupId>
    -            <artifactId>commons-compiler</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.codehaus.janino</groupId>
    -            <artifactId>janino</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.pentaho</groupId>
    -            <artifactId>pentaho-aggdesigner-algorithm</artifactId>
    -          </exclusion>
    -          <!-- End of Hive 2.3 exclusion -->
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>${hive.group}</groupId>
    -        <artifactId>hive-jdbc</artifactId>
    -        <version>${hive.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-metastore</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-serde</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-service</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-service-rpc</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-shims</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.curator</groupId>
    -            <artifactId>curator-framework</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libthrift</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libfb303</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.zookeeper</groupId>
    -            <artifactId>zookeeper</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>log4j</groupId>
    -            <artifactId>log4j</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.codehaus.groovy</groupId>
    -            <artifactId>groovy-all</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -
    -      <dependency>
    -        <groupId>${hive.group}</groupId>
    -        <artifactId>hive-metastore</artifactId>
    -        <version>${hive.version}</version>
    -        <scope>${hive.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-serde</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-shims</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libfb303</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libthrift</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.google.guava</groupId>
    -            <artifactId>guava</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <!-- Begin of Hive 2.3 exclusion -->
    -          <!-- Hive removes the HBase Metastore; see HIVE-17234 -->
    -          <exclusion>
    -            <groupId>org.apache.hbase</groupId>
    -            <artifactId>hbase-client</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>co.cask.tephra</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -          <!-- End of Hive 2.3 exclusion -->
    -        </exclusions>
    -      </dependency>
    -
    -      <dependency>
    -        <groupId>${hive.group}</groupId>
    -        <artifactId>hive-serde</artifactId>
    -        <version>${hive.version}</version>
    -        <scope>${hive.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-shims</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-codec</groupId>
    -            <artifactId>commons-codec</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.google.code.findbugs</groupId>
    -            <artifactId>jsr305</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.avro</groupId>
    -            <artifactId>avro</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libthrift</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libfb303</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>log4j</groupId>
    -            <artifactId>log4j</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.codehaus.groovy</groupId>
    -            <artifactId>groovy-all</artifactId>
    -          </exclusion>
    -          <!-- Begin of Hive 2.3 exclusion -->
    -          <exclusion>
    -            <groupId>${hive.group}</groupId>
    -            <artifactId>hive-service-rpc</artifactId>
    -          </exclusion>
    -          <!-- parquet-hadoop-bundle:1.8.1 conflict with 1.12.0 -->
    -          <exclusion>
    -            <groupId>org.apache.parquet</groupId>
    -            <artifactId>parquet-hadoop-bundle</artifactId>
    -          </exclusion>
    -          <!-- Do not need Jasper, see HIVE-19799 -->
    -          <exclusion>
    -            <groupId>tomcat</groupId>
    -            <artifactId>jasper-compiler</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>tomcat</groupId>
    -            <artifactId>jasper-runtime</artifactId>
    -          </exclusion>
    -          <!-- End of Hive 2.3 exclusion -->
    -        </exclusions>
    -      </dependency>
    -
    -      <dependency>
    -        <groupId>${hive.group}</groupId>
    -        <artifactId>hive-service-rpc</artifactId>
    -        <version>3.1.2</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>*</groupId>
    -            <artifactId>*</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>net.sf.jpam</groupId>
    -        <artifactId>jpam</artifactId>
    -        <scope>${hive.deps.scope}</scope>
    -        <version>${jpam.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>javax.servlet</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -
    -      <!-- hive shims pulls in hive 0.23 and a transitive dependency of the Hadoop version
    +                    <exclusion>
    +                        <groupId>ant</groupId>
    +                        <artifactId>ant</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.ant</groupId>
    +                        <artifactId>ant</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.esotericsoftware.kryo</groupId>
    +                        <artifactId>kryo</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-codec</groupId>
    +                        <artifactId>commons-codec</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.avro</groupId>
    +                        <artifactId>avro-mapred</artifactId>
    +                    </exclusion>
    +                    <!--  Do not need Calcite because we disabled hive.cbo.enable -->
    +                    <exclusion>
    +                        <groupId>org.apache.calcite</groupId>
    +                        <artifactId>calcite-core</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.calcite</groupId>
    +                        <artifactId>calcite-avatica</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.curator</groupId>
    +                        <artifactId>apache-curator</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.curator</groupId>
    +                        <artifactId>curator-client</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.curator</groupId>
    +                        <artifactId>curator-framework</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libthrift</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libfb303</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.zookeeper</groupId>
    +                        <artifactId>zookeeper</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>log4j</groupId>
    +                        <artifactId>log4j</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.codehaus.groovy</groupId>
    +                        <artifactId>groovy-all</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>jline</groupId>
    +                        <artifactId>jline</artifactId>
    +                    </exclusion>
    +                    <!-- Cat X license now; see SPARK-18262 -->
    +                    <exclusion>
    +                        <groupId>org.json</groupId>
    +                        <artifactId>json</artifactId>
    +                    </exclusion>
    +                    <!-- Begin of Hive 2.3 exclusion -->
    +                    <!-- Do not need Tez -->
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-llap-tez</artifactId>
    +                    </exclusion>
    +                    <!-- Do not need Calcite, see SPARK-27054 -->
    +                    <exclusion>
    +                        <groupId>org.apache.calcite</groupId>
    +                        <artifactId>calcite-druid</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.calcite.avatica</groupId>
    +                        <artifactId>avatica</artifactId>
    +                    </exclusion>
    +                    <!-- org.apache.logging.log4j:* conflict with log4j 1.2.17 -->
    +                    <exclusion>
    +                        <groupId>org.apache.logging.log4j</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>net.hydromatic</groupId>
    +                        <artifactId>eigenbase-properties</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.codehaus.janino</groupId>
    +                        <artifactId>commons-compiler</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.codehaus.janino</groupId>
    +                        <artifactId>janino</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.pentaho</groupId>
    +                        <artifactId>pentaho-aggdesigner-algorithm</artifactId>
    +                    </exclusion>
    +                    <!-- End of Hive 2.3 exclusion -->
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>${hive.group}</groupId>
    +                <artifactId>hive-jdbc</artifactId>
    +                <version>${hive.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-metastore</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-serde</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-service</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-service-rpc</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-shims</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.curator</groupId>
    +                        <artifactId>curator-framework</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libthrift</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libfb303</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.zookeeper</groupId>
    +                        <artifactId>zookeeper</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>log4j</groupId>
    +                        <artifactId>log4j</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.codehaus.groovy</groupId>
    +                        <artifactId>groovy-all</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            
    +            <dependency>
    +                <groupId>${hive.group}</groupId>
    +                <artifactId>hive-metastore</artifactId>
    +                <version>${hive.version}</version>
    +                <scope>${hive.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-serde</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-shims</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libfb303</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libthrift</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.google.guava</groupId>
    +                        <artifactId>guava</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <!-- Begin of Hive 2.3 exclusion -->
    +                    <!-- Hive removes the HBase Metastore; see HIVE-17234 -->
    +                    <exclusion>
    +                        <groupId>org.apache.hbase</groupId>
    +                        <artifactId>hbase-client</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>co.cask.tephra</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                    <!-- End of Hive 2.3 exclusion -->
    +                </exclusions>
    +            </dependency>
    +            
    +            <dependency>
    +                <groupId>${hive.group}</groupId>
    +                <artifactId>hive-serde</artifactId>
    +                <version>${hive.version}</version>
    +                <scope>${hive.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-shims</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-codec</groupId>
    +                        <artifactId>commons-codec</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.google.code.findbugs</groupId>
    +                        <artifactId>jsr305</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.avro</groupId>
    +                        <artifactId>avro</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libthrift</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libfb303</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>log4j</groupId>
    +                        <artifactId>log4j</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.codehaus.groovy</groupId>
    +                        <artifactId>groovy-all</artifactId>
    +                    </exclusion>
    +                    <!-- Begin of Hive 2.3 exclusion -->
    +                    <exclusion>
    +                        <groupId>${hive.group}</groupId>
    +                        <artifactId>hive-service-rpc</artifactId>
    +                    </exclusion>
    +                    <!-- parquet-hadoop-bundle:1.8.1 conflict with 1.12.0 -->
    +                    <exclusion>
    +                        <groupId>org.apache.parquet</groupId>
    +                        <artifactId>parquet-hadoop-bundle</artifactId>
    +                    </exclusion>
    +                    <!-- Do not need Jasper, see HIVE-19799 -->
    +                    <exclusion>
    +                        <groupId>tomcat</groupId>
    +                        <artifactId>jasper-compiler</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>tomcat</groupId>
    +                        <artifactId>jasper-runtime</artifactId>
    +                    </exclusion>
    +                    <!-- End of Hive 2.3 exclusion -->
    +                </exclusions>
    +            </dependency>
    +            
    +            <dependency>
    +                <groupId>${hive.group}</groupId>
    +                <artifactId>hive-service-rpc</artifactId>
    +                <version>3.1.2</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>*</groupId>
    +                        <artifactId>*</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>net.sf.jpam</groupId>
    +                <artifactId>jpam</artifactId>
    +                <scope>${hive.deps.scope}</scope>
    +                <version>${jpam.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>javax.servlet</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            
    +            <!-- hive shims pulls in hive 0.23 and a transitive dependency of the Hadoop version
             Hive was built against. This dependency cuts out the YARN/hadoop dependency, which
             is needed by Hive to submit work to a YARN cluster.-->
    -      <dependency>
    -        <groupId>${hive.group}</groupId>
    -        <artifactId>hive-shims</artifactId>
    -        <version>${hive.version}</version>
    -        <scope>${hive.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>com.google.guava</groupId>
    -            <artifactId>guava</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.curator</groupId>
    -            <artifactId>curator-framework</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.thrift</groupId>
    -            <artifactId>libthrift</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.zookeeper</groupId>
    -            <artifactId>zookeeper</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>log4j</groupId>
    -            <artifactId>log4j</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.codehaus.groovy</groupId>
    -            <artifactId>groovy-all</artifactId>
    -          </exclusion>
    -          <!-- Begin of Hive 2.3 exclusion -->
    -          <!-- Exclude log4j-slf4j-impl, otherwise throw NCDFE when starting spark-shell -->
    -          <exclusion>
    -            <groupId>org.apache.logging.log4j</groupId>
    -            <artifactId>log4j-slf4j-impl</artifactId>
    -          </exclusion>
    -          <!-- End of Hive 2.3 exclusion -->
    -        </exclusions>
    -      </dependency>
    -
    -      <!-- hive-llap-common is needed when registering UDFs in Hive 2.3.
    +            <dependency>
    +                <groupId>${hive.group}</groupId>
    +                <artifactId>hive-shims</artifactId>
    +                <version>${hive.version}</version>
    +                <scope>${hive.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>com.google.guava</groupId>
    +                        <artifactId>guava</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.curator</groupId>
    +                        <artifactId>curator-framework</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.thrift</groupId>
    +                        <artifactId>libthrift</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.zookeeper</groupId>
    +                        <artifactId>zookeeper</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>log4j</groupId>
    +                        <artifactId>log4j</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.codehaus.groovy</groupId>
    +                        <artifactId>groovy-all</artifactId>
    +                    </exclusion>
    +                    <!-- Begin of Hive 2.3 exclusion -->
    +                    <!-- Exclude log4j-slf4j-impl, otherwise throw NCDFE when starting spark-shell -->
    +                    <exclusion>
    +                        <groupId>org.apache.logging.log4j</groupId>
    +                        <artifactId>log4j-slf4j-impl</artifactId>
    +                    </exclusion>
    +                    <!-- End of Hive 2.3 exclusion -->
    +                </exclusions>
    +            </dependency>
    +            
    +            <!-- hive-llap-common is needed when registering UDFs in Hive 2.3.
              We add it here, otherwise -Phive-provided won't work. -->
    -      <dependency>
    -        <groupId>org.apache.hive</groupId>
    -        <artifactId>hive-llap-common</artifactId>
    -        <version>${hive23.version}</version>
    -        <scope>${hive.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.hive</groupId>
    -            <artifactId>hive-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hive</groupId>
    -            <artifactId>hive-serde</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <!-- hive-llap-client is needed when run MapReduce test in Hive 2.3. -->
    -      <dependency>
    -        <groupId>org.apache.hive</groupId>
    -        <artifactId>hive-llap-client</artifactId>
    -        <version>${hive23.version}</version>
    -        <scope>test</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.hive</groupId>
    -            <artifactId>hive-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hive</groupId>
    -            <artifactId>hive-serde</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hive</groupId>
    -            <artifactId>hive-llap-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.curator</groupId>
    -            <artifactId>curator-framework</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.curator</groupId>
    -            <artifactId>apache-curator</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.zookeeper</groupId>
    -            <artifactId>zookeeper</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -
    -      <dependency>
    -        <groupId>org.apache.orc</groupId>
    -        <artifactId>orc-core</artifactId>
    -        <version>${orc.version}</version>
    -        <scope>${orc.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>javax.xml.bind</groupId>
    -            <artifactId>jaxb-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-hdfs</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hive</groupId>
    -            <artifactId>hive-storage-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.orc</groupId>
    -        <artifactId>orc-mapreduce</artifactId>
    -        <version>${orc.version}</version>
    -        <scope>${orc.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-mapreduce-client-core</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.orc</groupId>
    -            <artifactId>orc-core</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hive</groupId>
    -            <artifactId>hive-storage-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId> com.esotericsoftware</groupId>
    -            <artifactId>kryo-shaded</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.parquet</groupId>
    -        <artifactId>parquet-column</artifactId>
    -        <version>${parquet.version}</version>
    -        <scope>${parquet.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.parquet</groupId>
    -        <artifactId>parquet-hadoop</artifactId>
    -        <version>${parquet.version}</version>
    -        <scope>${parquet.deps.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>commons-pool</groupId>
    -            <artifactId>commons-pool</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>javax.annotation</groupId>
    -            <artifactId>javax.annotation-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.parquet</groupId>
    -        <artifactId>parquet-avro</artifactId>
    -        <version>${parquet.version}</version>
    -        <scope>${parquet.test.deps.scope}</scope>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.codehaus.janino</groupId>
    -        <artifactId>janino</artifactId>
    -        <version>${janino.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.codehaus.janino</groupId>
    -        <artifactId>commons-compiler</artifactId>
    -        <version>${janino.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>joda-time</groupId>
    -        <artifactId>joda-time</artifactId>
    -        <version>${joda.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.jodd</groupId>
    -        <artifactId>jodd-core</artifactId>
    -        <version>${jodd.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.datanucleus</groupId>
    -        <artifactId>datanucleus-core</artifactId>
    -        <version>${datanucleus-core.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.thrift</groupId>
    -        <artifactId>libthrift</artifactId>
    -        <version>${libthrift.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.thrift</groupId>
    -        <artifactId>libfb303</artifactId>
    -        <version>0.9.3</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.antlr</groupId>
    -        <artifactId>antlr4-runtime</artifactId>
    -        <version>${antlr4.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.commons</groupId>
    -        <artifactId>commons-crypto</artifactId>
    -        <version>${commons-crypto.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>net.java.dev.jna</groupId>
    -            <artifactId>jna</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.google.crypto.tink</groupId>
    -        <artifactId>tink</artifactId>
    -        <version>${tink.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.arrow</groupId>
    -        <artifactId>arrow-vector</artifactId>
    -        <version>${arrow.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>com.fasterxml.jackson.core</groupId>
    -            <artifactId>jackson-annotations</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.fasterxml.jackson.core</groupId>
    -            <artifactId>jackson-core</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>io.netty</groupId>
    -            <artifactId>netty-common</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.arrow</groupId>
    -        <artifactId>arrow-memory-netty</artifactId>
    -        <version>${arrow.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>io.netty</groupId>
    -            <artifactId>netty-buffer</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>io.netty</groupId>
    -            <artifactId>netty-common</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.univocity</groupId>
    -        <artifactId>univocity-parsers</artifactId>
    -        <version>2.9.1</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hive</groupId>
    -        <artifactId>hive-storage-api</artifactId>
    -        <version>${hive.storage.version}</version>
    -        <scope>${hive.storage.scope}</scope>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>commons-lang</groupId>
    -            <artifactId>commons-lang</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-cli</groupId>
    -        <artifactId>commons-cli</artifactId>
    -        <version>${commons-cli.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>dev.ludovic.netlib</groupId>
    -        <artifactId>blas</artifactId>
    -        <version>${netlib.ludovic.dev.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>dev.ludovic.netlib</groupId>
    -        <artifactId>lapack</artifactId>
    -        <version>${netlib.ludovic.dev.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>dev.ludovic.netlib</groupId>
    -        <artifactId>arpack</artifactId>
    -        <version>${netlib.ludovic.dev.version}</version>
    -      </dependency>
    -    </dependencies>
    -  </dependencyManagement>
    -
    -  <build>
    -    <pluginManagement>
    -      <plugins>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-enforcer-plugin</artifactId>
    -          <version>3.0.0-M2</version>
    -          <executions>
    -            <execution>
    -              <id>enforce-versions</id>
    -              <goals>
    -                <goal>enforce</goal>
    -              </goals>
    -              <configuration>
    -                <rules>
    -                  <requireMavenVersion>
    -                    <version>${maven.version}</version>
    -                  </requireMavenVersion>
    -                  <requireJavaVersion>
    -                    <version>${java.version}</version>
    -                  </requireJavaVersion>
    -                  <bannedDependencies>
    -                    <excludes>
    -                      <exclude>org.jboss.netty</exclude>
    -                      <exclude>org.codehaus.groovy</exclude>
    -                      <exclude>*:*_2.11</exclude>
    -                      <exclude>*:*_2.10</exclude>
    -                    </excludes>
    -                    <searchTransitive>true</searchTransitive>
    -                  </bannedDependencies>
    -                </rules>
    -              </configuration>
    -            </execution>
    -            <execution>
    -              <id>enforce-no-duplicate-dependencies</id>
    -              <goals>
    -                <goal>enforce</goal>
    -              </goals>
    -              <configuration>
    -                <rules>
    -                  <banDuplicatePomDependencyVersions/>
    -                </rules>
    -              </configuration>
    -            </execution>
    -          </executions>
    -        </plugin>
    -   <plugin>
    -     <groupId>org.codehaus.mojo</groupId>
    -     <artifactId>build-helper-maven-plugin</artifactId>
    -     <version>3.2.0</version>
    -     <executions>
    -       <execution>
    -         <id>module-timestamp-property</id>
    -         <phase>validate</phase>
    -         <goals>
    -       <goal>timestamp-property</goal>
    -         </goals>
    -         <configuration>
    -       <name>module.build.timestamp</name>
    -       <pattern>${maven.build.timestamp.format}</pattern>
    -       <timeSource>current</timeSource>
    -       <timeZone>America/Los_Angeles</timeZone>
    -         </configuration>
    -       </execution>
    -       <execution>
    -         <id>local-timestamp-property</id>
    -         <phase>validate</phase>
    -         <goals>
    -       <goal>timestamp-property</goal>
    -         </goals>
    -         <configuration>
    -       <name>local.build.timestamp</name>
    -       <pattern>${maven.build.timestamp.format}</pattern>
    -       <timeSource>build</timeSource>
    -       <timeZone>America/Los_Angeles</timeZone>
    -         </configuration>
    -       </execution>
    -     </executions>
    -   </plugin>
    -        <plugin>
    -          <groupId>net.alchim31.maven</groupId>
    -          <artifactId>scala-maven-plugin</artifactId>
    -          <!-- SPARK-36547: Please don't upgrade the version below, otherwise there will be an error on building Hadoop 2.7 package -->
    -          <version>4.3.0</version>
    -          <executions>
    -            <execution>
    -              <id>eclipse-add-source</id>
    -              <goals>
    -                <goal>add-source</goal>
    -              </goals>
    -            </execution>
    -            <execution>
    -              <id>scala-compile-first</id>
    -              <goals>
    -                <goal>compile</goal>
    -              </goals>
    -            </execution>
    -            <execution>
    -              <id>scala-test-compile-first</id>
    -              <goals>
    -                <goal>testCompile</goal>
    -              </goals>
    -            </execution>
    -            <execution>
    -              <id>attach-scaladocs</id>
    -              <phase>verify</phase>
    -              <goals>
    -                <goal>doc-jar</goal>
    -              </goals>
    -            </execution>
    -          </executions>
    -          <configuration>
    -            <scalaVersion>${scala.version}</scalaVersion>
    -            <checkMultipleScalaVersions>true</checkMultipleScalaVersions>
    -            <failOnMultipleScalaVersions>true</failOnMultipleScalaVersions>
    -            <recompileMode>incremental</recompileMode>
    -            <args>
    -              <arg>-unchecked</arg>
    -              <arg>-deprecation</arg>
    -              <arg>-feature</arg>
    -              <arg>-explaintypes</arg>
    -              <arg>-target:jvm-1.8</arg>
    -              <arg>-Xfatal-warnings</arg>
    -              <arg>-Ywarn-unused:imports</arg>
    -              <arg>-P:silencer:globalFilters=.*deprecated.*</arg>
    -            </args>
    -            <jvmArgs>
    -              <jvmArg>-Xss128m</jvmArg>
    -              <jvmArg>-Xms4g</jvmArg>
    -              <jvmArg>-Xmx4g</jvmArg>
    -              <jvmArg>-XX:MaxMetaspaceSize=2g</jvmArg>
    -              <jvmArg>-XX:ReservedCodeCacheSize=${CodeCacheSize}</jvmArg>
    -            </jvmArgs>
    -            <javacArgs>
    -              <javacArg>-source</javacArg>
    -              <javacArg>${java.version}</javacArg>
    -              <javacArg>-target</javacArg>
    -              <javacArg>${java.version}</javacArg>
    -              <javacArg>-Xlint:all,-serial,-path,-try</javacArg>
    -            </javacArgs>
    -            <compilerPlugins>
    -              <compilerPlugin>
    -                <groupId>com.github.ghik</groupId>
    -                <artifactId>silencer-plugin_${scala.version}</artifactId>
    -                <version>1.7.6</version>
    -              </compilerPlugin>
    -            </compilerPlugins>
    -          </configuration>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-compiler-plugin</artifactId>
    -          <version>3.8.1</version>
    -          <configuration>
    -            <source>${java.version}</source>
    -            <target>${java.version}</target>
    -            <skipMain>true</skipMain> <!-- skip compile -->
    -            <skip>true</skip> <!-- skip testCompile -->
    -          </configuration>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.antlr</groupId>
    -          <artifactId>antlr4-maven-plugin</artifactId>
    -          <version>${antlr4.version}</version>
    -        </plugin>
    -        <!-- Surefire runs all Java tests -->
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-surefire-plugin</artifactId>
    -          <version>3.0.0-M5</version>
    -          <!-- Note config is repeated in scalatest config -->
    -          <configuration>
    -            <includes>
    -              <include>**/Test*.java</include>
    -              <include>**/*Test.java</include>
    -              <include>**/*TestCase.java</include>
    -              <include>**/*Suite.java</include>
    -            </includes>
    -            <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
    -            <argLine>-ea -Xmx4g -Xss4m -XX:MaxMetaspaceSize=2g -XX:ReservedCodeCacheSize=${CodeCacheSize} -Dio.netty.tryReflectionSetAccessible=true</argLine>
    -            <environmentVariables>
    -              <!--
    +            <dependency>
    +                <groupId>org.apache.hive</groupId>
    +                <artifactId>hive-llap-common</artifactId>
    +                <version>${hive23.version}</version>
    +                <scope>${hive.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.hive</groupId>
    +                        <artifactId>hive-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hive</groupId>
    +                        <artifactId>hive-serde</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <!-- hive-llap-client is needed when run MapReduce test in Hive 2.3. -->
    +            <dependency>
    +                <groupId>org.apache.hive</groupId>
    +                <artifactId>hive-llap-client</artifactId>
    +                <version>${hive23.version}</version>
    +                <scope>test</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.hive</groupId>
    +                        <artifactId>hive-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hive</groupId>
    +                        <artifactId>hive-serde</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hive</groupId>
    +                        <artifactId>hive-llap-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.curator</groupId>
    +                        <artifactId>curator-framework</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.curator</groupId>
    +                        <artifactId>apache-curator</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.zookeeper</groupId>
    +                        <artifactId>zookeeper</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            
    +            <dependency>
    +                <groupId>org.apache.orc</groupId>
    +                <artifactId>orc-core</artifactId>
    +                <version>${orc.version}</version>
    +                <scope>${orc.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>javax.xml.bind</groupId>
    +                        <artifactId>jaxb-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-hdfs</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hive</groupId>
    +                        <artifactId>hive-storage-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.orc</groupId>
    +                <artifactId>orc-mapreduce</artifactId>
    +                <version>${orc.version}</version>
    +                <scope>${orc.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-mapreduce-client-core</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.orc</groupId>
    +                        <artifactId>orc-core</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hive</groupId>
    +                        <artifactId>hive-storage-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.esotericsoftware</groupId>
    +                        <artifactId>kryo-shaded</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.parquet</groupId>
    +                <artifactId>parquet-column</artifactId>
    +                <version>${parquet.version}</version>
    +                <scope>${parquet.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.parquet</groupId>
    +                <artifactId>parquet-hadoop</artifactId>
    +                <version>${parquet.version}</version>
    +                <scope>${parquet.deps.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>commons-pool</groupId>
    +                        <artifactId>commons-pool</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>javax.annotation</groupId>
    +                        <artifactId>javax.annotation-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.parquet</groupId>
    +                <artifactId>parquet-avro</artifactId>
    +                <version>${parquet.version}</version>
    +                <scope>${parquet.test.deps.scope}</scope>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.codehaus.janino</groupId>
    +                <artifactId>janino</artifactId>
    +                <version>${janino.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.codehaus.janino</groupId>
    +                <artifactId>commons-compiler</artifactId>
    +                <version>${janino.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>joda-time</groupId>
    +                <artifactId>joda-time</artifactId>
    +                <version>${joda.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.jodd</groupId>
    +                <artifactId>jodd-core</artifactId>
    +                <version>${jodd.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.datanucleus</groupId>
    +                <artifactId>datanucleus-core</artifactId>
    +                <version>${datanucleus-core.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.thrift</groupId>
    +                <artifactId>libthrift</artifactId>
    +                <version>${libthrift.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.thrift</groupId>
    +                <artifactId>libfb303</artifactId>
    +                <version>0.9.3</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.antlr</groupId>
    +                <artifactId>antlr4-runtime</artifactId>
    +                <version>${antlr4.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.commons</groupId>
    +                <artifactId>commons-crypto</artifactId>
    +                <version>${commons-crypto.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>net.java.dev.jna</groupId>
    +                        <artifactId>jna</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.google.crypto.tink</groupId>
    +                <artifactId>tink</artifactId>
    +                <version>${tink.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.arrow</groupId>
    +                <artifactId>arrow-vector</artifactId>
    +                <version>${arrow.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>com.fasterxml.jackson.core</groupId>
    +                        <artifactId>jackson-annotations</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.fasterxml.jackson.core</groupId>
    +                        <artifactId>jackson-core</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>io.netty</groupId>
    +                        <artifactId>netty-common</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.arrow</groupId>
    +                <artifactId>arrow-memory-netty</artifactId>
    +                <version>${arrow.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>io.netty</groupId>
    +                        <artifactId>netty-buffer</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>io.netty</groupId>
    +                        <artifactId>netty-common</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.univocity</groupId>
    +                <artifactId>univocity-parsers</artifactId>
    +                <version>2.9.1</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hive</groupId>
    +                <artifactId>hive-storage-api</artifactId>
    +                <version>${hive.storage.version}</version>
    +                <scope>${hive.storage.scope}</scope>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>commons-lang</groupId>
    +                        <artifactId>commons-lang</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-cli</groupId>
    +                <artifactId>commons-cli</artifactId>
    +                <version>${commons-cli.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>dev.ludovic.netlib</groupId>
    +                <artifactId>blas</artifactId>
    +                <version>${netlib.ludovic.dev.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>dev.ludovic.netlib</groupId>
    +                <artifactId>lapack</artifactId>
    +                <version>${netlib.ludovic.dev.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>dev.ludovic.netlib</groupId>
    +                <artifactId>arpack</artifactId>
    +                <version>${netlib.ludovic.dev.version}</version>
    +            </dependency>
    +        </dependencies>
    +    </dependencyManagement>
    +    
    +    <build>
    +        <pluginManagement>
    +            <plugins>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-enforcer-plugin</artifactId>
    +                    <version>3.0.0-M2</version>
    +                    <executions>
    +                        <execution>
    +                            <id>enforce-versions</id>
    +                            <goals>
    +                                <goal>enforce</goal>
    +                            </goals>
    +                            <configuration>
    +                                <rules>
    +                                    <requireMavenVersion>
    +                                        <version>${maven.version}</version>
    +                                    </requireMavenVersion>
    +                                    <requireJavaVersion>
    +                                        <version>${java.version}</version>
    +                                    </requireJavaVersion>
    +                                    <bannedDependencies>
    +                                        <excludes>
    +                                            <exclude>org.jboss.netty</exclude>
    +                                            <exclude>org.codehaus.groovy</exclude>
    +                                            <exclude>*:*_2.11</exclude>
    +                                            <exclude>*:*_2.10</exclude>
    +                                        </excludes>
    +                                        <searchTransitive>true</searchTransitive>
    +                                    </bannedDependencies>
    +                                </rules>
    +                            </configuration>
    +                        </execution>
    +                        <execution>
    +                            <id>enforce-no-duplicate-dependencies</id>
    +                            <goals>
    +                                <goal>enforce</goal>
    +                            </goals>
    +                            <configuration>
    +                                <rules>
    +                                    <banDuplicatePomDependencyVersions/>
    +                                </rules>
    +                            </configuration>
    +                        </execution>
    +                    </executions>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.codehaus.mojo</groupId>
    +                    <artifactId>build-helper-maven-plugin</artifactId>
    +                    <version>3.2.0</version>
    +                    <executions>
    +                        <execution>
    +                            <id>module-timestamp-property</id>
    +                            <phase>validate</phase>
    +                            <goals>
    +                                <goal>timestamp-property</goal>
    +                            </goals>
    +                            <configuration>
    +                                <name>module.build.timestamp</name>
    +                                <pattern>${maven.build.timestamp.format}</pattern>
    +                                <timeSource>current</timeSource>
    +                                <timeZone>America/Los_Angeles</timeZone>
    +                            </configuration>
    +                        </execution>
    +                        <execution>
    +                            <id>local-timestamp-property</id>
    +                            <phase>validate</phase>
    +                            <goals>
    +                                <goal>timestamp-property</goal>
    +                            </goals>
    +                            <configuration>
    +                                <name>local.build.timestamp</name>
    +                                <pattern>${maven.build.timestamp.format}</pattern>
    +                                <timeSource>build</timeSource>
    +                                <timeZone>America/Los_Angeles</timeZone>
    +                            </configuration>
    +                        </execution>
    +                    </executions>
    +                </plugin>
    +                <plugin>
    +                    <groupId>net.alchim31.maven</groupId>
    +                    <artifactId>scala-maven-plugin</artifactId>
    +                    <!-- SPARK-36547: Please don't upgrade the version below, otherwise there will be an error on building Hadoop 2.7 package -->
    +                    <version>4.3.0</version>
    +                    <executions>
    +                        <execution>
    +                            <id>eclipse-add-source</id>
    +                            <goals>
    +                                <goal>add-source</goal>
    +                            </goals>
    +                        </execution>
    +                        <execution>
    +                            <id>scala-compile-first</id>
    +                            <goals>
    +                                <goal>compile</goal>
    +                            </goals>
    +                        </execution>
    +                        <execution>
    +                            <id>scala-test-compile-first</id>
    +                            <goals>
    +                                <goal>testCompile</goal>
    +                            </goals>
    +                        </execution>
    +                        <execution>
    +                            <id>attach-scaladocs</id>
    +                            <phase>verify</phase>
    +                            <goals>
    +                                <goal>doc-jar</goal>
    +                            </goals>
    +                        </execution>
    +                    </executions>
    +                    <configuration>
    +                        <scalaVersion>${scala.version}</scalaVersion>
    +                        <checkMultipleScalaVersions>true</checkMultipleScalaVersions>
    +                        <failOnMultipleScalaVersions>true</failOnMultipleScalaVersions>
    +                        <recompileMode>incremental</recompileMode>
    +                        <args>
    +                            <arg>-unchecked</arg>
    +                            <arg>-deprecation</arg>
    +                            <arg>-feature</arg>
    +                            <arg>-explaintypes</arg>
    +                            <arg>-target:jvm-1.8</arg>
    +                            <arg>-Xfatal-warnings</arg>
    +                            <arg>-Ywarn-unused:imports</arg>
    +                            <arg>-P:silencer:globalFilters=.*deprecated.*</arg>
    +                        </args>
    +                        <jvmArgs>
    +                            <jvmArg>-Xss128m</jvmArg>
    +                            <jvmArg>-Xms4g</jvmArg>
    +                            <jvmArg>-Xmx4g</jvmArg>
    +                            <jvmArg>-XX:MaxMetaspaceSize=2g</jvmArg>
    +                            <jvmArg>-XX:ReservedCodeCacheSize=${CodeCacheSize}</jvmArg>
    +                        </jvmArgs>
    +                        <javacArgs>
    +                            <javacArg>-source</javacArg>
    +                            <javacArg>${java.version}</javacArg>
    +                            <javacArg>-target</javacArg>
    +                            <javacArg>${java.version}</javacArg>
    +                            <javacArg>-Xlint:all,-serial,-path,-try</javacArg>
    +                        </javacArgs>
    +                        <compilerPlugins>
    +                            <compilerPlugin>
    +                                <groupId>com.github.ghik</groupId>
    +                                <artifactId>silencer-plugin_${scala.version}</artifactId>
    +                                <version>1.7.6</version>
    +                            </compilerPlugin>
    +                        </compilerPlugins>
    +                    </configuration>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-compiler-plugin</artifactId>
    +                    <version>3.8.1</version>
    +                    <configuration>
    +                        <source>${java.version}</source>
    +                        <target>${java.version}</target>
    +                        <skipMain>true</skipMain> <!-- skip compile -->
    +                        <skip>true</skip> <!-- skip testCompile -->
    +                    </configuration>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.antlr</groupId>
    +                    <artifactId>antlr4-maven-plugin</artifactId>
    +                    <version>${antlr4.version}</version>
    +                </plugin>
    +                <!-- Surefire runs all Java tests -->
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-surefire-plugin</artifactId>
    +                    <version>3.0.0-M5</version>
    +                    <!-- Note config is repeated in scalatest config -->
    +                    <configuration>
    +                        <includes>
    +                            <include>**/Test*.java</include>
    +                            <include>**/*Test.java</include>
    +                            <include>**/*TestCase.java</include>
    +                            <include>**/*Suite.java</include>
    +                        </includes>
    +                        <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
    +                        <argLine>-ea -Xmx4g -Xss4m -XX:MaxMetaspaceSize=2g -XX:ReservedCodeCacheSize=${CodeCacheSize}
    +                            -Dio.netty.tryReflectionSetAccessible=true
    +                        </argLine>
    +                        <environmentVariables>
    +                            <!--
                     Setting SPARK_DIST_CLASSPATH is a simple way to make sure any child processes
                     launched by the tests have access to the correct test-time classpath.
                   -->
    -              <SPARK_DIST_CLASSPATH>${test_classpath}</SPARK_DIST_CLASSPATH>
    -              <SPARK_PREPEND_CLASSES>1</SPARK_PREPEND_CLASSES>
    -              <SPARK_SCALA_VERSION>${scala.binary.version}</SPARK_SCALA_VERSION>
    -              <SPARK_TESTING>1</SPARK_TESTING>
    -              <JAVA_HOME>${test.java.home}</JAVA_HOME>
    -            </environmentVariables>
    -            <systemProperties>
    -              <log4j.configuration>file:src/test/resources/log4j.properties</log4j.configuration>
    -              <derby.system.durability>test</derby.system.durability>
    -              <java.awt.headless>true</java.awt.headless>
    -              <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
    -              <spark.test.home>${spark.test.home}</spark.test.home>
    -              <spark.testing>1</spark.testing>
    -              <spark.master.rest.enabled>false</spark.master.rest.enabled>
    -              <spark.ui.enabled>false</spark.ui.enabled>
    -              <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
    -              <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
    -              <spark.memory.debugFill>true</spark.memory.debugFill>
    -              <spark.hadoop.hadoop.security.key.provider.path>test:///</spark.hadoop.hadoop.security.key.provider.path>
    -              <!-- Needed by sql/hive tests. -->
    -              <test.src.tables>src</test.src.tables>
    -            </systemProperties>
    -            <failIfNoTests>false</failIfNoTests>
    -            <excludedGroups>${test.exclude.tags}</excludedGroups>
    -            <groups>${test.include.tags}</groups>
    -          </configuration>
    -          <executions>
    -            <execution>
    -              <id>test</id>
    -              <goals>
    -                <goal>test</goal>
    -              </goals>
    -            </execution>
    -          </executions>
    -        </plugin>
    -        <!-- Scalatest runs all Scala tests -->
    -        <plugin>
    -          <groupId>org.scalatest</groupId>
    -          <artifactId>scalatest-maven-plugin</artifactId>
    -          <version>${scalatest-maven-plugin.version}</version>
    -          <!-- Note config is repeated in surefire config -->
    -          <configuration>
    -            <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
    -            <junitxml>.</junitxml>
    -            <filereports>SparkTestSuite.txt</filereports>
    -            <argLine>-ea -Xmx4g -Xss4m -XX:MaxMetaspaceSize=2g -XX:ReservedCodeCacheSize=${CodeCacheSize} -Dio.netty.tryReflectionSetAccessible=true</argLine>
    -            <stderr/>
    -            <environmentVariables>
    -              <!--
    +                            <SPARK_DIST_CLASSPATH>${test_classpath}</SPARK_DIST_CLASSPATH>
    +                            <SPARK_PREPEND_CLASSES>1</SPARK_PREPEND_CLASSES>
    +                            <SPARK_SCALA_VERSION>${scala.binary.version}</SPARK_SCALA_VERSION>
    +                            <SPARK_TESTING>1</SPARK_TESTING>
    +                            <JAVA_HOME>${test.java.home}</JAVA_HOME>
    +                        </environmentVariables>
    +                        <systemProperties>
    +                            <log4j.configuration>file:src/test/resources/log4j.properties</log4j.configuration>
    +                            <derby.system.durability>test</derby.system.durability>
    +                            <java.awt.headless>true</java.awt.headless>
    +                            <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
    +                            <spark.test.home>${spark.test.home}</spark.test.home>
    +                            <spark.testing>1</spark.testing>
    +                            <spark.master.rest.enabled>false</spark.master.rest.enabled>
    +                            <spark.ui.enabled>false</spark.ui.enabled>
    +                            <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
    +                            <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
    +                            <spark.memory.debugFill>true</spark.memory.debugFill>
    +                            <spark.hadoop.hadoop.security.key.provider.path>test:///
    +                            </spark.hadoop.hadoop.security.key.provider.path>
    +                            <!-- Needed by sql/hive tests. -->
    +                            <test.src.tables>src</test.src.tables>
    +                        </systemProperties>
    +                        <failIfNoTests>false</failIfNoTests>
    +                        <excludedGroups>${test.exclude.tags}</excludedGroups>
    +                        <groups>${test.include.tags}</groups>
    +                    </configuration>
    +                    <executions>
    +                        <execution>
    +                            <id>test</id>
    +                            <goals>
    +                                <goal>test</goal>
    +                            </goals>
    +                        </execution>
    +                    </executions>
    +                </plugin>
    +                <!-- Scalatest runs all Scala tests -->
    +                <plugin>
    +                    <groupId>org.scalatest</groupId>
    +                    <artifactId>scalatest-maven-plugin</artifactId>
    +                    <version>${scalatest-maven-plugin.version}</version>
    +                    <!-- Note config is repeated in surefire config -->
    +                    <configuration>
    +                        <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
    +                        <junitxml>.</junitxml>
    +                        <filereports>SparkTestSuite.txt</filereports>
    +                        <argLine>-ea -Xmx4g -Xss4m -XX:MaxMetaspaceSize=2g -XX:ReservedCodeCacheSize=${CodeCacheSize}
    +                            -Dio.netty.tryReflectionSetAccessible=true
    +                        </argLine>
    +                        <stderr/>
    +                        <environmentVariables>
    +                            <!--
                     Setting SPARK_DIST_CLASSPATH is a simple way to make sure any child processes
                     launched by the tests have access to the correct test-time classpath.
                   -->
    -              <SPARK_DIST_CLASSPATH>${test_classpath}</SPARK_DIST_CLASSPATH>
    -              <SPARK_PREPEND_CLASSES>1</SPARK_PREPEND_CLASSES>
    -              <SPARK_SCALA_VERSION>${scala.binary.version}</SPARK_SCALA_VERSION>
    -              <SPARK_TESTING>1</SPARK_TESTING>
    -              <JAVA_HOME>${test.java.home}</JAVA_HOME>
    -            </environmentVariables>
    -            <systemProperties>
    -              <log4j.configuration>file:src/test/resources/log4j.properties</log4j.configuration>
    -              <derby.system.durability>test</derby.system.durability>
    -              <java.awt.headless>true</java.awt.headless>
    -              <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
    -              <spark.test.home>${spark.test.home}</spark.test.home>
    -              <spark.testing>1</spark.testing>
    -              <spark.ui.enabled>false</spark.ui.enabled>
    -              <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
    -              <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
    -              <spark.test.webdriver.chrome.driver>${spark.test.webdriver.chrome.driver}</spark.test.webdriver.chrome.driver>
    -              <spark.test.docker.keepContainer>${spark.test.docker.keepContainer}</spark.test.docker.keepContainer>
    -              <spark.test.docker.removePulledImage>${spark.test.docker.removePulledImage}</spark.test.docker.removePulledImage>
    -              <!-- Needed by sql/hive tests. -->
    -              <test.src.tables>__not_used__</test.src.tables>
    -            </systemProperties>
    -            <tagsToExclude>${test.exclude.tags},${test.default.exclude.tags}</tagsToExclude>
    -            <tagsToInclude>${test.include.tags}</tagsToInclude>
    -          </configuration>
    -          <executions>
    -            <execution>
    -              <id>test</id>
    -              <goals>
    -                <goal>test</goal>
    -              </goals>
    -            </execution>
    -          </executions>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-jar-plugin</artifactId>
    -          <version>3.1.2</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-antrun-plugin</artifactId>
    -          <version>${maven-antrun.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-source-plugin</artifactId>
    -          <version>3.1.0</version>
    -          <configuration>
    -            <attach>true</attach>
    -          </configuration>
    -          <executions>
    -            <execution>
    -              <id>create-source-jar</id>
    -              <goals>
    -                <goal>jar-no-fork</goal>
    -                <goal>test-jar-no-fork</goal>
    -              </goals>
    -            </execution>
    -          </executions>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-clean-plugin</artifactId>
    -          <version>3.1.0</version>
    -          <configuration>
    -            <filesets>
    -              <fileset>
    -                <directory>work</directory>
    -              </fileset>
    -              <fileset>
    -                <directory>checkpoint</directory>
    -              </fileset>
    -              <fileset>
    -                <directory>lib_managed</directory>
    -              </fileset>
    -              <fileset>
    -                <directory>metastore_db</directory>
    -              </fileset>
    -              <fileset>
    -                <directory>spark-warehouse</directory>
    -              </fileset>
    -            </filesets>
    -          </configuration>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-javadoc-plugin</artifactId>
    -          <version>3.1.1</version>
    -          <configuration>
    -            <additionalJOptions>
    -              <additionalJOption>-Xdoclint:all</additionalJOption>
    -              <additionalJOption>-Xdoclint:-missing</additionalJOption>
    -            </additionalJOptions>
    -            <tags>
    -              <tag>
    -                <name>example</name>
    -                <placement>a</placement>
    -                <head>Example:</head>
    -              </tag>
    -              <tag>
    -                <name>note</name>
    -                <placement>a</placement>
    -                <head>Note:</head>
    -              </tag>
    -              <tag>
    -                <name>group</name>
    -                <placement>X</placement>
    -              </tag>
    -              <tag>
    -                <name>tparam</name>
    -                <placement>X</placement>
    -              </tag>
    -              <tag>
    -                <name>constructor</name>
    -                <placement>X</placement>
    -              </tag>
    -              <tag>
    -                <name>todo</name>
    -                <placement>X</placement>
    -              </tag>
    -              <tag>
    -                <name>groupname</name>
    -                <placement>X</placement>
    -              </tag>
    -            </tags>
    -          </configuration>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.codehaus.mojo</groupId>
    -          <artifactId>exec-maven-plugin</artifactId>
    -          <version>${exec-maven-plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-assembly-plugin</artifactId>
    -          <version>3.1.0</version>
    -          <configuration>
    -            <tarLongFileMode>posix</tarLongFileMode>
    -          </configuration>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-shade-plugin</artifactId>
    -          <version>3.2.4</version>
    -          <dependencies>
    -            <dependency>
    -              <groupId>org.ow2.asm</groupId>
    -              <artifactId>asm</artifactId>
    -              <version>9.1</version>
    -            </dependency>
    -            <dependency>
    -              <groupId>org.ow2.asm</groupId>
    -              <artifactId>asm-commons</artifactId>
    -              <version>9.1</version>
    -            </dependency>
    -          </dependencies>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-install-plugin</artifactId>
    -          <version>3.0.0-M1</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-deploy-plugin</artifactId>
    -          <version>3.0.0-M1</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-dependency-plugin</artifactId>
    -          <version>3.1.1</version>
    -          <executions>
    -            <execution>
    -              <id>default-cli</id>
    -              <goals>
    -                 <goal>build-classpath</goal>
    -              </goals>
    -              <configuration>
    -                <!-- This includes dependencies with 'runtime' and 'compile' scopes;
    +                            <SPARK_DIST_CLASSPATH>${test_classpath}</SPARK_DIST_CLASSPATH>
    +                            <SPARK_PREPEND_CLASSES>1</SPARK_PREPEND_CLASSES>
    +                            <SPARK_SCALA_VERSION>${scala.binary.version}</SPARK_SCALA_VERSION>
    +                            <SPARK_TESTING>1</SPARK_TESTING>
    +                            <JAVA_HOME>${test.java.home}</JAVA_HOME>
    +                        </environmentVariables>
    +                        <systemProperties>
    +                            <log4j.configuration>file:src/test/resources/log4j.properties</log4j.configuration>
    +                            <derby.system.durability>test</derby.system.durability>
    +                            <java.awt.headless>true</java.awt.headless>
    +                            <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
    +                            <spark.test.home>${spark.test.home}</spark.test.home>
    +                            <spark.testing>1</spark.testing>
    +                            <spark.ui.enabled>false</spark.ui.enabled>
    +                            <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
    +                            <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
    +                            <spark.test.webdriver.chrome.driver>${spark.test.webdriver.chrome.driver}
    +                            </spark.test.webdriver.chrome.driver>
    +                            <spark.test.docker.keepContainer>${spark.test.docker.keepContainer}
    +                            </spark.test.docker.keepContainer>
    +                            <spark.test.docker.removePulledImage>${spark.test.docker.removePulledImage}
    +                            </spark.test.docker.removePulledImage>
    +                            <!-- Needed by sql/hive tests. -->
    +                            <test.src.tables>__not_used__</test.src.tables>
    +                        </systemProperties>
    +                        <tagsToExclude>${test.exclude.tags},${test.default.exclude.tags}</tagsToExclude>
    +                        <tagsToInclude>${test.include.tags}</tagsToInclude>
    +                    </configuration>
    +                    <executions>
    +                        <execution>
    +                            <id>test</id>
    +                            <goals>
    +                                <goal>test</goal>
    +                            </goals>
    +                        </execution>
    +                    </executions>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-jar-plugin</artifactId>
    +                    <version>3.1.2</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-antrun-plugin</artifactId>
    +                    <version>${maven-antrun.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-source-plugin</artifactId>
    +                    <version>3.1.0</version>
    +                    <configuration>
    +                        <attach>true</attach>
    +                    </configuration>
    +                    <executions>
    +                        <execution>
    +                            <id>create-source-jar</id>
    +                            <goals>
    +                                <goal>jar-no-fork</goal>
    +                                <goal>test-jar-no-fork</goal>
    +                            </goals>
    +                        </execution>
    +                    </executions>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-clean-plugin</artifactId>
    +                    <version>3.1.0</version>
    +                    <configuration>
    +                        <filesets>
    +                            <fileset>
    +                                <directory>work</directory>
    +                            </fileset>
    +                            <fileset>
    +                                <directory>checkpoint</directory>
    +                            </fileset>
    +                            <fileset>
    +                                <directory>lib_managed</directory>
    +                            </fileset>
    +                            <fileset>
    +                                <directory>metastore_db</directory>
    +                            </fileset>
    +                            <fileset>
    +                                <directory>spark-warehouse</directory>
    +                            </fileset>
    +                        </filesets>
    +                    </configuration>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-javadoc-plugin</artifactId>
    +                    <version>3.1.1</version>
    +                    <configuration>
    +                        <additionalJOptions>
    +                            <additionalJOption>-Xdoclint:all</additionalJOption>
    +                            <additionalJOption>-Xdoclint:-missing</additionalJOption>
    +                        </additionalJOptions>
    +                        <tags>
    +                            <tag>
    +                                <name>example</name>
    +                                <placement>a</placement>
    +                                <head>Example:</head>
    +                            </tag>
    +                            <tag>
    +                                <name>note</name>
    +                                <placement>a</placement>
    +                                <head>Note:</head>
    +                            </tag>
    +                            <tag>
    +                                <name>group</name>
    +                                <placement>X</placement>
    +                            </tag>
    +                            <tag>
    +                                <name>tparam</name>
    +                                <placement>X</placement>
    +                            </tag>
    +                            <tag>
    +                                <name>constructor</name>
    +                                <placement>X</placement>
    +                            </tag>
    +                            <tag>
    +                                <name>todo</name>
    +                                <placement>X</placement>
    +                            </tag>
    +                            <tag>
    +                                <name>groupname</name>
    +                                <placement>X</placement>
    +                            </tag>
    +                        </tags>
    +                    </configuration>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.codehaus.mojo</groupId>
    +                    <artifactId>exec-maven-plugin</artifactId>
    +                    <version>${exec-maven-plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-assembly-plugin</artifactId>
    +                    <version>3.1.0</version>
    +                    <configuration>
    +                        <tarLongFileMode>posix</tarLongFileMode>
    +                    </configuration>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-shade-plugin</artifactId>
    +                    <version>3.2.4</version>
    +                    <dependencies>
    +                        <dependency>
    +                            <groupId>org.ow2.asm</groupId>
    +                            <artifactId>asm</artifactId>
    +                            <version>9.1</version>
    +                        </dependency>
    +                        <dependency>
    +                            <groupId>org.ow2.asm</groupId>
    +                            <artifactId>asm-commons</artifactId>
    +                            <version>9.1</version>
    +                        </dependency>
    +                    </dependencies>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-install-plugin</artifactId>
    +                    <version>3.0.0-M1</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-deploy-plugin</artifactId>
    +                    <version>3.0.0-M1</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-dependency-plugin</artifactId>
    +                    <version>3.1.1</version>
    +                    <executions>
    +                        <execution>
    +                            <id>default-cli</id>
    +                            <goals>
    +                                <goal>build-classpath</goal>
    +                            </goals>
    +                            <configuration>
    +                                <!-- This includes dependencies with 'runtime' and 'compile' scopes;
                          see the docs for includeScope for more details -->
    -                <includeScope>runtime</includeScope>
    -              </configuration>
    -            </execution>
    -          </executions>
    -        </plugin>
    -        <!-- This plugin's configuration is used to store Eclipse m2e settings only. -->
    -        <!-- It has no influence on the Maven build itself. -->
    -        <plugin>
    -          <groupId>org.eclipse.m2e</groupId>
    -          <artifactId>lifecycle-mapping</artifactId>
    -          <version>1.0.0</version>
    -          <configuration>
    -            <lifecycleMappingMetadata>
    -              <pluginExecutions>
    -                <pluginExecution>
    -                  <pluginExecutionFilter>
    -                    <groupId>org.apache.maven.plugins</groupId>
    -                    <artifactId>maven-dependency-plugin</artifactId>
    -                    <versionRange>[2.8,)</versionRange>
    -                    <goals>
    -                      <goal>build-classpath</goal>
    -                    </goals>
    -                  </pluginExecutionFilter>
    -                  <action>
    -                    <ignore></ignore>
    -                  </action>
    -                </pluginExecution>
    -                <pluginExecution>
    -                  <pluginExecutionFilter>
    -                    <groupId>org.apache.maven.plugins</groupId>
    -                    <artifactId>maven-jar-plugin</artifactId>
    -                    <versionRange>3.1.2</versionRange>
    -                    <goals>
    -                      <goal>test-jar</goal>
    -                    </goals>
    -                  </pluginExecutionFilter>
    -                  <action>
    -                    <ignore></ignore>
    -                  </action>
    -                </pluginExecution>
    -                <pluginExecution>
    -                  <pluginExecutionFilter>
    -                    <groupId>org.apache.maven.plugins</groupId>
    -                    <artifactId>maven-antrun-plugin</artifactId>
    -                    <versionRange>[${maven-antrun.version},)</versionRange>
    -                    <goals>
    -                      <goal>run</goal>
    -                    </goals>
    -                  </pluginExecutionFilter>
    -                  <action>
    -                    <ignore></ignore>
    -                  </action>
    -                </pluginExecution>
    -              </pluginExecutions>
    -            </lifecycleMappingMetadata>
    -          </configuration>
    -        </plugin>
    -      </plugins>
    -    </pluginManagement>
    -
    -    <plugins>
    -      <!-- This plugin dumps the test classpath into a file -->
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-dependency-plugin</artifactId>
    -        <executions>
    -          <execution>
    -            <id>generate-test-classpath</id>
    -            <phase>test-compile</phase>
    -            <goals>
    -              <goal>build-classpath</goal>
    -            </goals>
    -            <configuration>
    -              <includeScope>test</includeScope>
    -              <outputProperty>test_classpath</outputProperty>
    -            </configuration>
    -          </execution>
    -          <execution>
    -            <id>copy-module-dependencies</id>
    -            <phase>${build.copyDependenciesPhase}</phase>
    -            <goals>
    -              <goal>copy-dependencies</goal>
    -            </goals>
    -            <configuration>
    -              <includeScope>runtime</includeScope>
    -              <outputDirectory>${jars.target.dir}</outputDirectory>
    -            </configuration>
    -          </execution>
    -        </executions>
    -      </plugin>
    -
    -      <!--
    +                                <includeScope>runtime</includeScope>
    +                            </configuration>
    +                        </execution>
    +                    </executions>
    +                </plugin>
    +                <!-- This plugin's configuration is used to store Eclipse m2e settings only. -->
    +                <!-- It has no influence on the Maven build itself. -->
    +                <plugin>
    +                    <groupId>org.eclipse.m2e</groupId>
    +                    <artifactId>lifecycle-mapping</artifactId>
    +                    <version>1.0.0</version>
    +                    <configuration>
    +                        <lifecycleMappingMetadata>
    +                            <pluginExecutions>
    +                                <pluginExecution>
    +                                    <pluginExecutionFilter>
    +                                        <groupId>org.apache.maven.plugins</groupId>
    +                                        <artifactId>maven-dependency-plugin</artifactId>
    +                                        <versionRange>[2.8,)</versionRange>
    +                                        <goals>
    +                                            <goal>build-classpath</goal>
    +                                        </goals>
    +                                    </pluginExecutionFilter>
    +                                    <action>
    +                                        <ignore></ignore>
    +                                    </action>
    +                                </pluginExecution>
    +                                <pluginExecution>
    +                                    <pluginExecutionFilter>
    +                                        <groupId>org.apache.maven.plugins</groupId>
    +                                        <artifactId>maven-jar-plugin</artifactId>
    +                                        <versionRange>3.1.2</versionRange>
    +                                        <goals>
    +                                            <goal>test-jar</goal>
    +                                        </goals>
    +                                    </pluginExecutionFilter>
    +                                    <action>
    +                                        <ignore></ignore>
    +                                    </action>
    +                                </pluginExecution>
    +                                <pluginExecution>
    +                                    <pluginExecutionFilter>
    +                                        <groupId>org.apache.maven.plugins</groupId>
    +                                        <artifactId>maven-antrun-plugin</artifactId>
    +                                        <versionRange>[${maven-antrun.version},)</versionRange>
    +                                        <goals>
    +                                            <goal>run</goal>
    +                                        </goals>
    +                                    </pluginExecutionFilter>
    +                                    <action>
    +                                        <ignore></ignore>
    +                                    </action>
    +                                </pluginExecution>
    +                            </pluginExecutions>
    +                        </lifecycleMappingMetadata>
    +                    </configuration>
    +                </plugin>
    +            </plugins>
    +        </pluginManagement>
    +        
    +        <plugins>
    +            <!-- This plugin dumps the test classpath into a file -->
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-dependency-plugin</artifactId>
    +                <executions>
    +                    <execution>
    +                        <id>generate-test-classpath</id>
    +                        <phase>test-compile</phase>
    +                        <goals>
    +                            <goal>build-classpath</goal>
    +                        </goals>
    +                        <configuration>
    +                            <includeScope>test</includeScope>
    +                            <outputProperty>test_classpath</outputProperty>
    +                        </configuration>
    +                    </execution>
    +                    <execution>
    +                        <id>copy-module-dependencies</id>
    +                        <phase>${build.copyDependenciesPhase}</phase>
    +                        <goals>
    +                            <goal>copy-dependencies</goal>
    +                        </goals>
    +                        <configuration>
    +                            <includeScope>runtime</includeScope>
    +                            <outputDirectory>${jars.target.dir}</outputDirectory>
    +                        </configuration>
    +                    </execution>
    +                </executions>
    +            </plugin>
    +            
    +            <!--
             The shade plug-in is used here to create effective pom's (see SPARK-3812), and also
             remove references from the shaded libraries from artifacts published by Spark.
           -->
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-shade-plugin</artifactId>
    -        <configuration>
    -          <shadedArtifactAttached>false</shadedArtifactAttached>
    -          <artifactSet>
    -            <includes>
    -              <include>org.spark-project.spark:unused</include>
    -              <include>org.eclipse.jetty:jetty-io</include>
    -              <include>org.eclipse.jetty:jetty-http</include>
    -              <include>org.eclipse.jetty:jetty-proxy</include>
    -              <include>org.eclipse.jetty:jetty-client</include>
    -              <include>org.eclipse.jetty:jetty-continuation</include>
    -              <include>org.eclipse.jetty:jetty-servlet</include>
    -              <include>org.eclipse.jetty:jetty-servlets</include>
    -              <include>org.eclipse.jetty:jetty-plus</include>
    -              <include>org.eclipse.jetty:jetty-security</include>
    -              <include>org.eclipse.jetty:jetty-util</include>
    -              <include>org.eclipse.jetty:jetty-server</include>
    -              <include>com.google.guava:guava</include>
    -              <include>org.jpmml:*</include>
    -            </includes>
    -          </artifactSet>
    -          <relocations>
    -            <relocation>
    -              <pattern>org.eclipse.jetty</pattern>
    -              <shadedPattern>${spark.shade.packageName}.jetty</shadedPattern>
    -              <includes>
    -                <include>org.eclipse.jetty.**</include>
    -              </includes>
    -            </relocation>
    -            <relocation>
    -              <pattern>com.google.common</pattern>
    -              <shadedPattern>${spark.shade.packageName}.guava</shadedPattern>
    -            </relocation>
    -            <relocation>
    -              <pattern>org.dmg.pmml</pattern>
    -              <shadedPattern>${spark.shade.packageName}.dmg.pmml</shadedPattern>
    -            </relocation>
    -            <relocation>
    -              <pattern>org.jpmml</pattern>
    -              <shadedPattern>${spark.shade.packageName}.jpmml</shadedPattern>
    -            </relocation>
    -          </relocations>
    -        </configuration>
    -        <executions>
    -          <execution>
    -            <phase>package</phase>
    -            <goals>
    -              <goal>shade</goal>
    -            </goals>
    -          </execution>
    -        </executions>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-enforcer-plugin</artifactId>
    -      </plugin>
    -      <plugin>
    -        <groupId>net.alchim31.maven</groupId>
    -        <artifactId>scala-maven-plugin</artifactId>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-source-plugin</artifactId>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.scalastyle</groupId>
    -        <artifactId>scalastyle-maven-plugin</artifactId>
    -        <version>1.0.0</version>
    -        <configuration>
    -          <verbose>false</verbose>
    -          <failOnViolation>true</failOnViolation>
    -          <includeTestSourceDirectory>false</includeTestSourceDirectory>
    -          <failOnWarning>false</failOnWarning>
    -          <sourceDirectory>${basedir}/src/main/scala</sourceDirectory>
    -          <testSourceDirectory>${basedir}/src/test/scala</testSourceDirectory>
    -          <configLocation>scalastyle-config.xml</configLocation>
    -          <outputFile>${basedir}/target/scalastyle-output.xml</outputFile>
    -          <inputEncoding>${project.build.sourceEncoding}</inputEncoding>
    -          <outputEncoding>${project.reporting.outputEncoding}</outputEncoding>
    -        </configuration>
    -        <executions>
    -          <execution>
    -            <goals>
    -              <goal>check</goal>
    -            </goals>
    -          </execution>
    -        </executions>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-checkstyle-plugin</artifactId>
    -        <version>3.1.1</version>
    -        <configuration>
    -          <failOnViolation>false</failOnViolation>
    -          <includeTestSourceDirectory>true</includeTestSourceDirectory>
    -          <sourceDirectories>
    -            <directory>${basedir}/src/main/java</directory>
    -            <directory>${basedir}/src/main/scala</directory>
    -          </sourceDirectories>
    -          <testSourceDirectories>
    -            <directory>${basedir}/src/test/java</directory>
    -          </testSourceDirectories>
    -          <configLocation>dev/checkstyle.xml</configLocation>
    -          <outputFile>${basedir}/target/checkstyle-output.xml</outputFile>
    -          <inputEncoding>${project.build.sourceEncoding}</inputEncoding>
    -          <outputEncoding>${project.reporting.outputEncoding}</outputEncoding>
    -        </configuration>
    -        <dependencies>
    -          <dependency>
    -            <groupId>com.puppycrawl.tools</groupId>
    -            <artifactId>checkstyle</artifactId>
    -            <version>8.39</version>
    -          </dependency>
    -        </dependencies>
    -        <executions>
    -          <execution>
    -            <goals>
    -              <goal>check</goal>
    -            </goals>
    -          </execution>
    -        </executions>
    -      </plugin>
    -
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-antrun-plugin</artifactId>
    -        <executions>
    -          <execution>
    -            <id>create-tmp-dir</id>
    -            <phase>generate-test-resources</phase>
    -            <goals>
    -              <goal>run</goal>
    -            </goals>
    -            <configuration>
    -              <target>
    -                <mkdir dir="${project.build.directory}/tmp" />
    -              </target>
    -            </configuration>
    -          </execution>
    -        </executions>
    -      </plugin>
    -
    -      <!-- Enable surefire and scalatest in all children, in one place: -->
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-surefire-plugin</artifactId>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.scalatest</groupId>
    -        <artifactId>scalatest-maven-plugin</artifactId>
    -      </plugin>
    -      <!-- Build test-jar's for all projects, since some projects depend on tests from others -->
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-jar-plugin</artifactId>
    -        <executions>
    -          <execution>
    -            <id>prepare-test-jar</id>
    -            <phase>${build.testJarPhase}</phase>
    -            <goals>
    -              <goal>test-jar</goal>
    -            </goals>
    -            <configuration>
    -              <excludes>
    -                <exclude>log4j.properties</exclude>
    -              </excludes>
    -            </configuration>
    -          </execution>
    -        </executions>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.antipathy</groupId>
    -        <artifactId>mvn-scalafmt_${scala.binary.version}</artifactId>
    -        <version>1.0.4</version>
    -        <configuration>
    -          <parameters>${scalafmt.parameters}</parameters> <!-- (Optional) Additional command line arguments -->
    -          <skip>${scalafmt.skip}</skip> <!-- (Optional) skip formatting -->
    -          <skipSources>${scalafmt.skip}</skipSources>
    -          <skipTestSources>${scalafmt.skip}</skipTestSources>
    -          <configLocation>dev/.scalafmt.conf</configLocation> <!-- (Optional) config location -->
    -          <onlyChangedFiles>true</onlyChangedFiles>
    -        </configuration>
    -        <executions>
    -          <execution>
    -            <phase>validate</phase>
    -            <goals>
    -              <goal>format</goal>
    -            </goals>
    -          </execution>
    -        </executions>
    -      </plugin>
    -      <!--
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-shade-plugin</artifactId>
    +                <configuration>
    +                    <shadedArtifactAttached>false</shadedArtifactAttached>
    +                    <artifactSet>
    +                        <includes>
    +                            <include>org.spark-project.spark:unused</include>
    +                            <include>org.eclipse.jetty:jetty-io</include>
    +                            <include>org.eclipse.jetty:jetty-http</include>
    +                            <include>org.eclipse.jetty:jetty-proxy</include>
    +                            <include>org.eclipse.jetty:jetty-client</include>
    +                            <include>org.eclipse.jetty:jetty-continuation</include>
    +                            <include>org.eclipse.jetty:jetty-servlet</include>
    +                            <include>org.eclipse.jetty:jetty-servlets</include>
    +                            <include>org.eclipse.jetty:jetty-plus</include>
    +                            <include>org.eclipse.jetty:jetty-security</include>
    +                            <include>org.eclipse.jetty:jetty-util</include>
    +                            <include>org.eclipse.jetty:jetty-server</include>
    +                            <include>com.google.guava:guava</include>
    +                            <include>org.jpmml:*</include>
    +                        </includes>
    +                    </artifactSet>
    +                    <relocations>
    +                        <relocation>
    +                            <pattern>org.eclipse.jetty</pattern>
    +                            <shadedPattern>${spark.shade.packageName}.jetty</shadedPattern>
    +                            <includes>
    +                                <include>org.eclipse.jetty.**</include>
    +                            </includes>
    +                        </relocation>
    +                        <relocation>
    +                            <pattern>com.google.common</pattern>
    +                            <shadedPattern>${spark.shade.packageName}.guava</shadedPattern>
    +                        </relocation>
    +                        <relocation>
    +                            <pattern>org.dmg.pmml</pattern>
    +                            <shadedPattern>${spark.shade.packageName}.dmg.pmml</shadedPattern>
    +                        </relocation>
    +                        <relocation>
    +                            <pattern>org.jpmml</pattern>
    +                            <shadedPattern>${spark.shade.packageName}.jpmml</shadedPattern>
    +                        </relocation>
    +                    </relocations>
    +                </configuration>
    +                <executions>
    +                    <execution>
    +                        <phase>package</phase>
    +                        <goals>
    +                            <goal>shade</goal>
    +                        </goals>
    +                    </execution>
    +                </executions>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-enforcer-plugin</artifactId>
    +            </plugin>
    +            <plugin>
    +                <groupId>net.alchim31.maven</groupId>
    +                <artifactId>scala-maven-plugin</artifactId>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-source-plugin</artifactId>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.scalastyle</groupId>
    +                <artifactId>scalastyle-maven-plugin</artifactId>
    +                <version>1.0.0</version>
    +                <configuration>
    +                    <verbose>false</verbose>
    +                    <failOnViolation>true</failOnViolation>
    +                    <includeTestSourceDirectory>false</includeTestSourceDirectory>
    +                    <failOnWarning>false</failOnWarning>
    +                    <sourceDirectory>${basedir}/src/main/scala</sourceDirectory>
    +                    <testSourceDirectory>${basedir}/src/test/scala</testSourceDirectory>
    +                    <configLocation>scalastyle-config.xml</configLocation>
    +                    <outputFile>${basedir}/target/scalastyle-output.xml</outputFile>
    +                    <inputEncoding>${project.build.sourceEncoding}</inputEncoding>
    +                    <outputEncoding>${project.reporting.outputEncoding}</outputEncoding>
    +                </configuration>
    +                <executions>
    +                    <execution>
    +                        <goals>
    +                            <goal>check</goal>
    +                        </goals>
    +                    </execution>
    +                </executions>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-checkstyle-plugin</artifactId>
    +                <version>3.1.1</version>
    +                <configuration>
    +                    <failOnViolation>false</failOnViolation>
    +                    <includeTestSourceDirectory>true</includeTestSourceDirectory>
    +                    <sourceDirectories>
    +                        <directory>${basedir}/src/main/java</directory>
    +                        <directory>${basedir}/src/main/scala</directory>
    +                    </sourceDirectories>
    +                    <testSourceDirectories>
    +                        <directory>${basedir}/src/test/java</directory>
    +                    </testSourceDirectories>
    +                    <configLocation>dev/checkstyle.xml</configLocation>
    +                    <outputFile>${basedir}/target/checkstyle-output.xml</outputFile>
    +                    <inputEncoding>${project.build.sourceEncoding}</inputEncoding>
    +                    <outputEncoding>${project.reporting.outputEncoding}</outputEncoding>
    +                </configuration>
    +                <dependencies>
    +                    <dependency>
    +                        <groupId>com.puppycrawl.tools</groupId>
    +                        <artifactId>checkstyle</artifactId>
    +                        <version>8.39</version>
    +                    </dependency>
    +                </dependencies>
    +                <executions>
    +                    <execution>
    +                        <goals>
    +                            <goal>check</goal>
    +                        </goals>
    +                    </execution>
    +                </executions>
    +            </plugin>
    +            
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-antrun-plugin</artifactId>
    +                <executions>
    +                    <execution>
    +                        <id>create-tmp-dir</id>
    +                        <phase>generate-test-resources</phase>
    +                        <goals>
    +                            <goal>run</goal>
    +                        </goals>
    +                        <configuration>
    +                            <target>
    +                                <mkdir dir="${project.build.directory}/tmp"/>
    +                            </target>
    +                        </configuration>
    +                    </execution>
    +                </executions>
    +            </plugin>
    +            
    +            <!-- Enable surefire and scalatest in all children, in one place: -->
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-surefire-plugin</artifactId>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.scalatest</groupId>
    +                <artifactId>scalatest-maven-plugin</artifactId>
    +            </plugin>
    +            <!-- Build test-jar's for all projects, since some projects depend on tests from others -->
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-jar-plugin</artifactId>
    +                <executions>
    +                    <execution>
    +                        <id>prepare-test-jar</id>
    +                        <phase>${build.testJarPhase}</phase>
    +                        <goals>
    +                            <goal>test-jar</goal>
    +                        </goals>
    +                        <configuration>
    +                            <excludes>
    +                                <exclude>log4j.properties</exclude>
    +                            </excludes>
    +                        </configuration>
    +                    </execution>
    +                </executions>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.antipathy</groupId>
    +                <artifactId>mvn-scalafmt_${scala.binary.version}</artifactId>
    +                <version>1.0.4</version>
    +                <configuration>
    +                    <parameters>${scalafmt.parameters}
    +                    </parameters> <!-- (Optional) Additional command line arguments -->
    +                    <skip>${scalafmt.skip}</skip> <!-- (Optional) skip formatting -->
    +                    <skipSources>${scalafmt.skip}</skipSources>
    +                    <skipTestSources>${scalafmt.skip}</skipTestSources>
    +                    <configLocation>dev/.scalafmt.conf</configLocation> <!-- (Optional) config location -->
    +                    <onlyChangedFiles>true</onlyChangedFiles>
    +                </configuration>
    +                <executions>
    +                    <execution>
    +                        <phase>validate</phase>
    +                        <goals>
    +                            <goal>format</goal>
    +                        </goals>
    +                    </execution>
    +                </executions>
    +            </plugin>
    +            <!--
             Couple of dependencies are coming in bundle format (bundle is just a normal jar which
             contains OSGi metadata in the manifest). If one don't use OSGi, then a bundle will work as
             any other jar. Since maven doesn't have native bundle support it needs an external plugin
             handle it. If the plugin is not added then the build can't resolve bundle dependencies.
           -->
    -      <plugin>
    -        <groupId>org.apache.felix</groupId>
    -        <artifactId>maven-bundle-plugin</artifactId>
    -        <version>4.2.0</version>
    -        <extensions>true</extensions>
    -      </plugin>
    -    </plugins>
    -  </build>
    -
    -  <profiles>
    -
    -    <!--
    +            <plugin>
    +                <groupId>org.apache.felix</groupId>
    +                <artifactId>maven-bundle-plugin</artifactId>
    +                <version>4.2.0</version>
    +                <extensions>true</extensions>
    +            </plugin>
    +        </plugins>
    +    </build>
    +    
    +    <profiles>
    +        
    +        <!--
           This profile is enabled automatically by the sbt build. It changes the scope for shaded
           dependencies, since we don't shade it in the artifacts generated by the sbt build.
         -->
    -    <profile>
    -      <id>sbt</id>
    -      <dependencies>
    -        <dependency>
    -          <groupId>com.google.guava</groupId>
    -          <artifactId>guava</artifactId>
    -          <scope>compile</scope>
    -        </dependency>
    -        <dependency>
    -          <groupId>org.jpmml</groupId>
    -          <artifactId>pmml-model</artifactId>
    -          <scope>compile</scope>
    -        </dependency>
    -      </dependencies>
    -    </profile>
    -
    -    <!-- Ganglia integration is not included by default due to LGPL-licensed code -->
    -    <profile>
    -      <id>spark-ganglia-lgpl</id>
    -      <modules>
    -        <module>external/spark-ganglia-lgpl</module>
    -      </modules>
    -    </profile>
    -
    -    <!-- Kinesis integration is not included by default due to ASL-licensed code -->
    -    <profile>
    -      <id>kinesis-asl</id>
    -      <modules>
    -        <module>external/kinesis-asl</module>
    -        <module>external/kinesis-asl-assembly</module>
    -      </modules>
    -    </profile>
    -
    -    <profile>
    -      <id>docker-integration-tests</id>
    -      <modules>
    -        <module>external/docker-integration-tests</module>
    -      </modules>
    -    </profile>
    -
    -    <!-- A series of build profiles where customizations for particular Hadoop releases can be made -->
    -
    -    <!-- Hadoop-a.b.c dependencies can be found at
    +        <profile>
    +            <id>sbt</id>
    +            <dependencies>
    +                <dependency>
    +                    <groupId>com.google.guava</groupId>
    +                    <artifactId>guava</artifactId>
    +                    <scope>compile</scope>
    +                </dependency>
    +                <dependency>
    +                    <groupId>org.jpmml</groupId>
    +                    <artifactId>pmml-model</artifactId>
    +                    <scope>compile</scope>
    +                </dependency>
    +            </dependencies>
    +        </profile>
    +        
    +        <!-- Ganglia integration is not included by default due to LGPL-licensed code -->
    +        <profile>
    +            <id>spark-ganglia-lgpl</id>
    +            <modules>
    +                <module>external/spark-ganglia-lgpl</module>
    +            </modules>
    +        </profile>
    +        
    +        <!-- Kinesis integration is not included by default due to ASL-licensed code -->
    +        <profile>
    +            <id>kinesis-asl</id>
    +            <modules>
    +                <module>external/kinesis-asl</module>
    +                <module>external/kinesis-asl-assembly</module>
    +            </modules>
    +        </profile>
    +        
    +        <profile>
    +            <id>docker-integration-tests</id>
    +            <modules>
    +                <module>external/docker-integration-tests</module>
    +            </modules>
    +        </profile>
    +        
    +        <!-- A series of build profiles where customizations for particular Hadoop releases can be made -->
    +        
    +        <!-- Hadoop-a.b.c dependencies can be found at
         http://hadoop.apache.org/docs/ra.b.c/hadoop-project-dist/hadoop-common/dependency-analysis.html
         -->
    -
    -    <profile>
    -      <id>hadoop-2.7</id>
    -      <properties>
    -        <hadoop.version>2.7.4</hadoop.version>
    -        <curator.version>2.7.1</curator.version>
    -        <commons-io.version>2.4</commons-io.version>
    -        <!--
    +        
    +        <profile>
    +            <id>hadoop-2.7</id>
    +            <properties>
    +                <hadoop.version>2.7.4</hadoop.version>
    +                <curator.version>2.7.1</curator.version>
    +                <commons-io.version>2.4</commons-io.version>
    +                <!--
               the declaration site above of these variables explains why we need to re-assign them here
             -->
    -        <hadoop-client-api.artifact>hadoop-client</hadoop-client-api.artifact>
    -        <hadoop-client-runtime.artifact>hadoop-yarn-api</hadoop-client-runtime.artifact>
    -        <hadoop-client-minicluster.artifact>hadoop-client</hadoop-client-minicluster.artifact>
    -      </properties>
    -    </profile>
    -
    -    <profile>
    -      <id>hadoop-3.2</id>
    -      <!-- Default hadoop profile. Uses global properties. -->
    -    </profile>
    -
    -    <profile>
    -      <id>hive-2.3</id>
    -      <!-- Default hive profile. Uses global properties. -->
    -    </profile>
    -
    -    <profile>
    -      <id>yarn</id>
    -      <modules>
    -        <module>resource-managers/yarn</module>
    -        <module>common/network-yarn</module>
    -      </modules>
    -    </profile>
    -
    -    <profile>
    -      <id>mesos</id>
    -      <modules>
    -        <module>resource-managers/mesos</module>
    -      </modules>
    -    </profile>
    -
    -    <profile>
    -      <id>kubernetes</id>
    -      <modules>
    -        <module>resource-managers/kubernetes/core</module>
    -      </modules>
    -    </profile>
    -
    -    <!-- generally not enabled for automated builds, but will run k8s tests -->
    -    <profile>
    -      <id>kubernetes-integration-tests</id>
    -      <modules>
    -        <module>resource-managers/kubernetes/integration-tests</module>
    -      </modules>
    -    </profile>
    -
    -    <profile>
    -      <id>hive-thriftserver</id>
    -      <modules>
    -        <module>sql/hive-thriftserver</module>
    -      </modules>
    -    </profile>
    -
    -    <profile>
    -      <id>hadoop-cloud</id>
    -      <modules>
    -        <module>hadoop-cloud</module>
    -      </modules>
    -    </profile>
    -
    -    <profile>
    -      <id>test-java-home</id>
    -      <activation>
    -        <property><name>env.JAVA_HOME</name></property>
    -      </activation>
    -      <properties>
    -        <test.java.home>${env.JAVA_HOME}</test.java.home>
    -      </properties>
    -    </profile>
    -
    -    <profile>
    -      <id>scala-2.12</id>
    -      <properties>
    -        <!-- 
    +                <hadoop-client-api.artifact>hadoop-client</hadoop-client-api.artifact>
    +                <hadoop-client-runtime.artifact>hadoop-yarn-api</hadoop-client-runtime.artifact>
    +                <hadoop-client-minicluster.artifact>hadoop-client</hadoop-client-minicluster.artifact>
    +            </properties>
    +        </profile>
    +        
    +        <profile>
    +            <id>hadoop-3.2</id>
    +            <!-- Default hadoop profile. Uses global properties. -->
    +        </profile>
    +        
    +        <profile>
    +            <id>hive-2.3</id>
    +            <!-- Default hive profile. Uses global properties. -->
    +        </profile>
    +        
    +        <profile>
    +            <id>yarn</id>
    +            <modules>
    +                <module>resource-managers/yarn</module>
    +                <module>common/network-yarn</module>
    +            </modules>
    +        </profile>
    +        
    +        <profile>
    +            <id>mesos</id>
    +            <modules>
    +                <module>resource-managers/mesos</module>
    +            </modules>
    +        </profile>
    +        
    +        <profile>
    +            <id>kubernetes</id>
    +            <modules>
    +                <module>resource-managers/kubernetes/core</module>
    +            </modules>
    +        </profile>
    +        
    +        <!-- generally not enabled for automated builds, but will run k8s tests -->
    +        <profile>
    +            <id>kubernetes-integration-tests</id>
    +            <modules>
    +                <module>resource-managers/kubernetes/integration-tests</module>
    +            </modules>
    +        </profile>
    +        
    +        <profile>
    +            <id>hive-thriftserver</id>
    +            <modules>
    +                <module>sql/hive-thriftserver</module>
    +            </modules>
    +        </profile>
    +        
    +        <profile>
    +            <id>hadoop-cloud</id>
    +            <modules>
    +                <module>hadoop-cloud</module>
    +            </modules>
    +        </profile>
    +        
    +        <profile>
    +            <id>test-java-home</id>
    +            <activation>
    +                <property>
    +                    <name>env.JAVA_HOME</name>
    +                </property>
    +            </activation>
    +            <properties>
    +                <test.java.home>${env.JAVA_HOME}</test.java.home>
    +            </properties>
    +        </profile>
    +        
    +        <profile>
    +            <id>scala-2.12</id>
    +            <properties>
    +                <!-- 
              SPARK-34774 Add this property to ensure change-scala-version.sh can replace the public `scala.version`
              property correctly. 
             -->
    -        <scala.version>2.12.15</scala.version>
    -      </properties>
    -      <build>
    -        <pluginManagement>
    -          <plugins>
    -          </plugins>
    -        </pluginManagement>
    -      </build>
    -    </profile>
    -
    -    <profile>
    -      <id>scala-2.13</id>
    -      <properties>
    -        <scala.version>2.13.5</scala.version>
    -        <scala.binary.version>2.13</scala.binary.version>
    -      </properties>
    -      <build>
    -        <pluginManagement>
    -          <plugins>
    -            <plugin>
    -              <groupId>net.alchim31.maven</groupId>
    -              <artifactId>scala-maven-plugin</artifactId>
    -              <configuration>
    -                <args>
    -                  <arg>-unchecked</arg>
    -                  <arg>-deprecation</arg>
    -                  <arg>-feature</arg>
    -                  <arg>-explaintypes</arg>
    -                  <arg>-target:jvm-1.8</arg>
    -                  <arg>-Wconf:cat=deprecation:wv,any:e</arg>
    -                  <!--
    +                <scala.version>2.12.15</scala.version>
    +            </properties>
    +            <build>
    +                <pluginManagement>
    +                    <plugins>
    +                    </plugins>
    +                </pluginManagement>
    +            </build>
    +        </profile>
    +        
    +        <profile>
    +            <id>scala-2.13</id>
    +            <properties>
    +                <scala.version>2.13.5</scala.version>
    +                <scala.binary.version>2.13</scala.binary.version>
    +            </properties>
    +            <build>
    +                <pluginManagement>
    +                    <plugins>
    +                        <plugin>
    +                            <groupId>net.alchim31.maven</groupId>
    +                            <artifactId>scala-maven-plugin</artifactId>
    +                            <configuration>
    +                                <args>
    +                                    <arg>-unchecked</arg>
    +                                    <arg>-deprecation</arg>
    +                                    <arg>-feature</arg>
    +                                    <arg>-explaintypes</arg>
    +                                    <arg>-target:jvm-1.8</arg>
    +                                    <arg>-Wconf:cat=deprecation:wv,any:e</arg>
    +                                    <!--
                         TODO(SPARK-33805): Undo the corresponding deprecated usage suppression rule after fixed
                         <arg>-Wunused:imports</arg>
                       -->
    -                  <arg>-Wconf:cat=scaladoc:wv</arg>
    -                  <arg>-Wconf:cat=lint-multiarg-infix:wv</arg>
    -                  <arg>-Wconf:cat=other-nullary-override:wv</arg>
    -                  <arg>-Wconf:cat=other-match-analysis&amp;site=org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupFunction.catalogFunction:wv</arg>
    -                  <arg>-Wconf:cat=other-pure-statement&amp;site=org.apache.spark.streaming.util.FileBasedWriteAheadLog.readAll.readFile:wv</arg>
    -                  <arg>-Wconf:cat=other-pure-statement&amp;site=org.apache.spark.scheduler.OutputCommitCoordinatorSuite.&lt;local OutputCommitCoordinatorSuite&gt;.futureAction:wv</arg>
    -                  <!--
    +                                    <arg>-Wconf:cat=scaladoc:wv</arg>
    +                                    <arg>-Wconf:cat=lint-multiarg-infix:wv</arg>
    +                                    <arg>-Wconf:cat=other-nullary-override:wv</arg>
    +                                    <arg>-Wconf:cat=other-match-analysis&amp;site=org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupFunction.catalogFunction:wv</arg>
    +                                    <arg>-Wconf:cat=other-pure-statement&amp;site=org.apache.spark.streaming.util.FileBasedWriteAheadLog.readAll.readFile:wv</arg>
    +                                    <arg>-Wconf:cat=other-pure-statement&amp;site=org.apache.spark.scheduler.OutputCommitCoordinatorSuite.&lt;local
    +                                        OutputCommitCoordinatorSuite&gt;.futureAction:wv
    +                                    </arg>
    +                                    <!--
                         SPARK-33775 Suppress compilation warnings that contain the following contents.
                         TODO(SPARK-33805): Undo the corresponding deprecated usage suppression rule after fixed.
                       -->
    -                  <arg>-Wconf:msg=^(?=.*?method|value|type|object|trait|inheritance)(?=.*?deprecated)(?=.*?since 2.13).+$:s</arg>
    -                  <arg>-Wconf:msg=^(?=.*?Widening conversion from)(?=.*?is deprecated because it loses precision).+$:s</arg>
    -                  <arg>-Wconf:msg=Auto-application to \`\(\)\` is deprecated:s</arg>
    -                  <arg>-Wconf:msg=method with a single empty parameter list overrides method without any parameter list:s</arg>
    -                  <arg>-Wconf:msg=method without a parameter list overrides a method with a single empty one:s</arg>
    -                  <!--
    +                                    <arg>
    +                                        -Wconf:msg=^(?=.*?method|value|type|object|trait|inheritance)(?=.*?deprecated)(?=.*?since
    +                                        2.13).+$:s
    +                                    </arg>
    +                                    <arg>-Wconf:msg=^(?=.*?Widening conversion from)(?=.*?is deprecated because it loses
    +                                        precision).+$:s
    +                                    </arg>
    +                                    <arg>-Wconf:msg=Auto-application to \`\(\)\` is deprecated:s</arg>
    +                                    <arg>-Wconf:msg=method with a single empty parameter list overrides method without
    +                                        any parameter list:s
    +                                    </arg>
    +                                    <arg>-Wconf:msg=method without a parameter list overrides a method with a single
    +                                        empty one:s
    +                                    </arg>
    +                                    <!--
                         SPARK-35574 Prevent the recurrence of compilation warnings related to
                         `procedure syntax is deprecated`
                       -->
    -                  <arg>-Wconf:cat=deprecation&amp;msg=procedure syntax is deprecated:e</arg>
    -                </args>
    -                <compilerPlugins combine.self="override">
    -                </compilerPlugins>
    -              </configuration>
    -            </plugin>
    -          </plugins>
    -        </pluginManagement>
    -      </build>
    -
    -    </profile>
    -
    -    <!--
    +                                    <arg>-Wconf:cat=deprecation&amp;msg=procedure syntax is deprecated:e</arg>
    +                                </args>
    +                                <compilerPlugins combine.self="override">
    +                                </compilerPlugins>
    +                            </configuration>
    +                        </plugin>
    +                    </plugins>
    +                </pluginManagement>
    +            </build>
    +        
    +        </profile>
    +        
    +        <!--
          This is a profile to enable the use of the ASF snapshot and staging repositories
          during a build. It is useful when testing against nightly or RC releases of dependencies.
          It MUST NOT be used when building copies of Spark to use in production of for distribution,
          -->
    -    <profile>
    -      <id>snapshots-and-staging</id>
    -      <properties>
    -        <!-- override point for ASF staging/snapshot repos -->
    -        <asf.staging>https://repository.apache.org/content/groups/staging/</asf.staging>
    -        <asf.snapshots>https://repository.apache.org/content/repositories/snapshots/</asf.snapshots>
    -      </properties>
    -
    -      <pluginRepositories>
    -        <pluginRepository>
    -          <id>ASF Staging</id>
    -          <url>${asf.staging}</url>
    -        </pluginRepository>
    -        <pluginRepository>
    -          <id>ASF Snapshots</id>
    -          <url>${asf.snapshots}</url>
    -          <snapshots>
    -            <enabled>true</enabled>
    -          </snapshots>
    -          <releases>
    -            <enabled>false</enabled>
    -          </releases>
    -        </pluginRepository>
    -
    -      </pluginRepositories>
    -      <repositories>
    -        <repository>
    -          <id>ASF Staging</id>
    -          <url>${asf.staging}</url>
    -        </repository>
    -        <repository>
    -          <id>ASF Snapshots</id>
    -          <url>${asf.snapshots}</url>
    -          <snapshots>
    -            <enabled>true</enabled>
    -          </snapshots>
    -          <releases>
    -            <enabled>false</enabled>
    -          </releases>
    -        </repository>
    -      </repositories>
    -    </profile>
    -
    -    <!--
    +        <profile>
    +            <id>snapshots-and-staging</id>
    +            <properties>
    +                <!-- override point for ASF staging/snapshot repos -->
    +                <asf.staging>https://repository.apache.org/content/groups/staging/</asf.staging>
    +                <asf.snapshots>https://repository.apache.org/content/repositories/snapshots/</asf.snapshots>
    +            </properties>
    +            
    +            <pluginRepositories>
    +                <pluginRepository>
    +                    <id>ASF Staging</id>
    +                    <url>${asf.staging}</url>
    +                </pluginRepository>
    +                <pluginRepository>
    +                    <id>ASF Snapshots</id>
    +                    <url>${asf.snapshots}</url>
    +                    <snapshots>
    +                        <enabled>true</enabled>
    +                    </snapshots>
    +                    <releases>
    +                        <enabled>false</enabled>
    +                    </releases>
    +                </pluginRepository>
    +            
    +            </pluginRepositories>
    +            <repositories>
    +                <repository>
    +                    <id>ASF Staging</id>
    +                    <url>${asf.staging}</url>
    +                </repository>
    +                <repository>
    +                    <id>ASF Snapshots</id>
    +                    <url>${asf.snapshots}</url>
    +                    <snapshots>
    +                        <enabled>true</enabled>
    +                    </snapshots>
    +                    <releases>
    +                        <enabled>false</enabled>
    +                    </releases>
    +                </repository>
    +            </repositories>
    +        </profile>
    +        
    +        <!--
           These empty profiles are available in some sub-modules. Declare them here so that
           maven does not complain when they're provided on the command line for a sub-module
           that does not have them.
         -->
    -    <profile>
    -      <id>hadoop-provided</id>
    -      <properties>
    -        <spark.yarn.isHadoopProvided>true</spark.yarn.isHadoopProvided>
    -      </properties>
    -    </profile>
    -    <profile>
    -      <id>hive-provided</id>
    -    </profile>
    -    <profile>
    -      <id>orc-provided</id>
    -    </profile>
    -    <profile>
    -      <id>parquet-provided</id>
    -    </profile>
    -    <profile>
    -      <id>sparkr</id>
    -    </profile>
    -    <!-- use org.openlabtesting.leveldbjni on aarch64 platform -->
    -    <profile>
    -      <id>aarch64</id>
    -      <properties>
    -        <leveldbjni.group>org.openlabtesting.leveldbjni</leveldbjni.group>
    -      </properties>
    -      <activation>
    -        <os>
    -          <family>linux</family>
    -          <arch>aarch64</arch>
    -        </os>
    -      </activation>
    -    </profile>
    -    <profile>
    -      <id>jdwp-test-debug</id>
    -      <properties>
    -        <jdwp.arg.line>-agentlib:jdwp=transport=dt_socket,suspend=${test.jdwp.suspend},server=${test.jdwp.server},address=${test.jdwp.address}</jdwp.arg.line>
    -        <debugArgLine>${jdwp.arg.line}</debugArgLine>
    -        <maven.surefire.debug>${jdwp.arg.line}</maven.surefire.debug>
    -        <debugForkedProcess>${test.debug.suite}</debugForkedProcess>
    -      </properties>
    -    </profile>
    -
    -    <!--
    +        <profile>
    +            <id>hadoop-provided</id>
    +            <properties>
    +                <spark.yarn.isHadoopProvided>true</spark.yarn.isHadoopProvided>
    +            </properties>
    +        </profile>
    +        <profile>
    +            <id>hive-provided</id>
    +        </profile>
    +        <profile>
    +            <id>orc-provided</id>
    +        </profile>
    +        <profile>
    +            <id>parquet-provided</id>
    +        </profile>
    +        <profile>
    +            <id>sparkr</id>
    +        </profile>
    +        <!-- use org.openlabtesting.leveldbjni on aarch64 platform -->
    +        <profile>
    +            <id>aarch64</id>
    +            <properties>
    +                <leveldbjni.group>org.openlabtesting.leveldbjni</leveldbjni.group>
    +            </properties>
    +            <activation>
    +                <os>
    +                    <family>linux</family>
    +                    <arch>aarch64</arch>
    +                </os>
    +            </activation>
    +        </profile>
    +        <profile>
    +            <id>jdwp-test-debug</id>
    +            <properties>
    +                <jdwp.arg.line>
    +                    -agentlib:jdwp=transport=dt_socket,suspend=${test.jdwp.suspend},server=${test.jdwp.server},address=${test.jdwp.address}
    +                </jdwp.arg.line>
    +                <debugArgLine>${jdwp.arg.line}</debugArgLine>
    +                <maven.surefire.debug>${jdwp.arg.line}</maven.surefire.debug>
    +                <debugForkedProcess>${test.debug.suite}</debugForkedProcess>
    +            </properties>
    +        </profile>
    +        
    +        <!--
           Deprecated: com.github.fommil.netlib has been replaced by dev.ludovic.netlib which
           doesn't package or distribute any GPL/LGPL dependencies. There should now be hardware
           acceleration out-of-the-box without enabling any additional profile.
    @@ -3537,16 +3561,16 @@
           org.scalanlp:breeze still depends on `com.github.fommil.netlib`. It's been updated with
           https://github.com/scalanlp/breeze/pull/811 but it hasn't been released yet.
         -->
    -    <profile>
    -      <id>netlib-lgpl</id>
    -      <dependencies>
    -        <dependency>
    -          <groupId>com.github.fommil.netlib</groupId>
    -          <artifactId>all</artifactId>
    -          <version>${netlib.java.version}</version>
    -          <type>pom</type>
    -        </dependency>
    -      </dependencies>
    -    </profile>
    -  </profiles>
    +        <profile>
    +            <id>netlib-lgpl</id>
    +            <dependencies>
    +                <dependency>
    +                    <groupId>com.github.fommil.netlib</groupId>
    +                    <artifactId>all</artifactId>
    +                    <version>${netlib.java.version}</version>
    +                    <type>pom</type>
    +                </dependency>
    +            </dependencies>
    +        </profile>
    +    </profiles>
     </project>
```

### 3.3 解压安装 Spark

```bash
    cd /home/issac/coding/java/src/spark/ || exit                              # 切换到解压目录
    tar -zxf spark-3.2.3-bin-build.tgz -C /opt/apache/                         # 解压编译后的压缩包
    mv /opt/apache/spark-3.2.3-bin-build/ /opt/apache/spark                    # 修改目录名称
    mkdir -p /opt/apache/spark/logs/                                           # 创建日志目录
```

### 3.4 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Spark 3.2.3 ====================================== #
        export SPARK_HOME=/opt/apache/spark
        export PATH=$PATH:${SPARK_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 3.5 修改配置文件

#### 3.5.1 添加如下配置文件

```bash
    cd /opt/apache/spark/                                                      # 切换到 spark 安装目录
    touch ${SPARK_HOME}/conf/spark-env.sh
    touch ${SPARK_HOME}/conf/spark-defaults.conf
    touch ${SPARK_HOME}/spark/conf/workers
    cp ${SPARK_HOME}/conf/log4j.properties.template ${SPARK_HOME}/conf/log4j.properties
```

#### 3.5.2 修改 ${SPARK_HOME}/conf/spark-env.sh 

```bash
    export JAVA_HOME=/opt/java/jdk-08
    export SCALA_HOME=/opt/java/scala-212
    export HADOOP_HOME=/opt/apache/hadoop
    export SPARK_HOME=/opt/apache/spark
    
    export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    export YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    # export SPARK_MASTER_HOST=issac
    export SPARK_CONF_DIR=${SPARK_HOME}/conf
    
    export SPARK_HISTORY_OPTS="
        -Dspark.history.ui.port=18080 
        -Dspark.history.fs.logDirectory=hdfs://issac:9000/spark/logs 
        -Dspark.history.retainedApplications=30"
    
    export SPARK_DIST_CLASSPATH=$(${HADOOP_HOME}/bin/hadoop classpath)
```

#### 3.5.3 修改 ${SPARK_HOME}/conf/spark-defaults.conf 

```bash
    spark.master                      yarn
    spark.eventLog.enabled            true
    spark.eventLog.dir                hdfs://issac:9000/spark/logs
    spark.yarn.jars                   hdfs://issac:9000/spark/jars/*
    spark.history.fs.logDirectory     hdfs://issac:9000/spark/history
    
    spark.serializer                  org.apache.spark.serializer.KryoSerializer
    spark.driver.memory               2g
    spark.executor.memory             2g
    
    spark.yarn.historyServer.address  http://issac:18080
    spark.history.ui.port             18080
```

#### 3.5.4 修改 ${SPARK_HOME}/conf/works

```bash
    slaver1
    slaver2
    slaver3
```

### 3.6 上传 jar 到 hdfs

```bash
    # 在 hdfs 上创建 spark 需要的目录
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/                             # hdfs 上的 spark 工作目录
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/jars                         # 不带 hadoop 的 spark 依赖
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/logs                         # spark 日志存放目录
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/history                      # spark 历史服务器目录
    
    # 上传不带 hadoop 的 spark 依赖
    tar -zxvf spark-3.2.3-bin-without-hadoop.tgz                               # 解压下载的不带 hadoop 的 spark 安装包
    ${HADOOP_HOME}/bin/hadoop fs -put spark-3.2.3-bin-without-hadoop/jars/* /spark/jars/ # 上传文件到 hdfs
```

### 3.7 编写 spark 启停脚本

#### 3.7.1 创建 spark.sh

```bash
    cd /opt/apache/spark/bin/                                                  # 切换到 spark 安装目录的 bin
    touch ${SPARK_HOME}/bin/spark.sh                                           # 脚本内容，如：3.7.2
    chmod +x ${SPARK_HOME}/bin/spark.sh                                        # 添加可执行权限
```

#### 3.7.2 ${SPARK_HOME}/bin/spark.sh

```bashpro shell script
    #!/usr/bin/env bash
    
    
    SERVICE_DIR=$(cd "$(dirname "$0")/../" || exit; pwd)
    SERVICE_NAME=Spark
    JUDGE_NAME=org.apache.spark.deploy
    
    MASTER_PORT=7077
    MASTER_UI_PORT=8080
    WORKER_PORT=8081
    WORKER_RPC_PORT=34003
    HISTORY_SERVER_PORT=18080
    
    MASTER_NODE=org.apache.spark.deploy.master.Master
    WORKER_NODE=org.apache.spark.deploy.worker.Worker
    HISTORY_SERVER=org.apache.spark.deploy.history.HistoryServer
    
    
    printf "\n=========================================================================\n"
    #  匹配输入参数
    case "$1" in
        #  1. 运行程序
        start)
            # 1.1 查找程序的 pid
            pid_list=$(ps -aux | grep -i "${JUDGE_NAME}" | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}')
            
            #  1.2 若 pid 不存在，则运行程序，否则打印程序运行状态
            if [ ! "${pid_list}" ]; then
                echo "    程序（${SERVICE_NAME}）正在加载中 ......"
                "${SERVICE_DIR}/sbin/start-all.sh" > /dev/null 2>&1
                sleep 1
                "${SERVICE_DIR}/sbin/start-history-server.sh" > /dev/null 2>&1
                sleep 1
                
                # 1.3 判断程序 Master 启动是否成功
                master_pid=$(ps -aux | grep -i ${MASTER_NODE} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${master_pid}" -ne 1 ]; then
                    echo "    程序 SparkMaster 启动失败 ...... "
                fi
                
                # 1.4 判断程序 Worker 启动是否成功
                worker_pid=$(ps -aux | grep -i ${WORKER_NODE} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${worker_pid}" -ne 1 ]; then
                    echo "    程序 SparkWorker 启动失败 ...... "
                fi
                
                # 1.5 判断程序 HistoryServer 启动是否成功
                history_pid=$(ps -aux | grep -i ${HISTORY_SERVER} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${history_pid}" -ne 1 ]; then
                    echo "    程序 HistoryServer 启动失败 ...... "
                fi
                
                # 1.6 判断所有程序启动是否成功
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${pid_count}" -ge 3 ]; then
                    echo "    程序（${SERVICE_NAME}）启动成功 ...... "
                else
                    echo "    程序（${SERVICE_NAME}）启动失败 ...... "
                fi
                
            else
                echo "    程序（${SERVICE_NAME}）正在运行当中 ...... "
            fi
        ;;
        
        #  2. 停止
        stop)
            # 2.1 根据程序的 pid 查询程序运行状态
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            if [ "${pid_count}" -eq 0 ]; then
                echo "    程序（${SERVICE_NAME}）的进程不存在，程序没有运行 ...... "
            elif [ "${pid_count}" -eq 3 ]; then
                # 2.2 杀死进程，关闭程序
                "${SERVICE_DIR}/sbin/stop-history-server.sh" > /dev/null 2>&1
                sleep 1
                echo "    程序（${SERVICE_NAME}）正在停止中 ...... "
                "${SERVICE_DIR}/sbin/stop-all.sh" > /dev/null 2>&1
                sleep 4
    
                # 2.3 若还未关闭，则强制杀死进程，关闭程序
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | wc -l)
                if [ "${pid_count}" -ge 1 ]; then
                    temp=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | xargs kill -9)
                fi
                
                echo "    程序（${SERVICE_NAME}）已经停止成功 ......"            
            else
                echo "    程序（${SERVICE_NAME}）运行出现问题 ......"
            fi
        ;;
        
        #  3. 状态查询
        status)
            # 3.1 查看正在运行程序的 pid
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            #  3.2 判断 ES 运行状态
            if [ "${pid_count}" -eq 0 ]; then
                echo "    程序（${SERVICE_NAME}） 已经停止 ...... "
            elif [ "${pid_count}" -eq 3 ]; then
                echo "    程序（${SERVICE_NAME}）正在运行中 ...... "
            else
                echo "    程序（${SERVICE_NAME}）运行出现问题 ...... "
            fi
        ;;
        
        #  4. 重启程序
        restart)
            "$0" stop
            sleep 3
            "$0" start
        ;;
        
        #  5. 其它情况
        *)
            echo "    脚本可传入一个参数，如下所示：              "
            echo "        +-----------------------------------+ "
            echo "        |  start | stop | restart | status  | "
            echo "        +-----------------------------------+ "
            echo "        |        start    ：  启动服务      | "
            echo "        |        stop     ：  关闭服务      | "
            echo "        |        restart  ：  重启服务      | "
            echo "        |        status   ：  查看状态      | "
            echo "        +-----------------------------------+ "
        ;;
    esac
    printf "=========================================================================\n\n"
```

### 3.8 同步 spark 安装路径，分发到其他节点 

```bash
    cd /opt/apache/                                                            # 切换到 spark 安装父路径
    ~/shell/xync.sh spark                                                      # 同步 spark 到其它节点
````

### 3.9 启动集群并测试

#### 3.9.1 启动集群

```bash
    # 单独启动
    ${SPARK_HOME}/sbin/start-master.sh                                         # 单独启动 master 主节点
    ${SPARK_HOME}/sbin/start-workers.sh spark://master:7077                    # 启动所有的 slave（worker） 节点
    ${SPARK_HOME}/sbin/start-worker.sh  spark://master:7077                    # 启动单台的 slave（worker） 节点
    ${SPARK_HOME}/sbin/start-history-server.sh                                 # 启动历史服务器
    
    # 全部启动
    ${SPARK_HOME}/sbin/start-all.sh                                            # 启动 master 和 worker 节点
    
    # 自定义脚本启动
    ${SPARK_HOME}/bin/spark.sh start                                           # 启动 master、worker、history-server
```

#### 3.9.2 集群测试

```bash
    jps -l                                                                     # 查看 spark 启动的 jvm 进程
    http://master:8080                                                         # 浏览器访问 SparkMaster
    http://master:4040                                                         # 浏览器访问 SparkUI
    http://master:18080                                                        # 浏览器访问 历史服务器
    
    # 计算 pi 
    ${SPARK_HOME}/bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[*] ${SPARK_HOME}/examples/jars/spark-examples_2.12-3.2.3.jar 100
    ${SPARK_HOME}/bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://issac:7077 --deploy-mode cluster ${SPARK_HOME}/examples/jars/spark-examples_2.12-3.2.3.jar 100
    ${SPARK_HOME}/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --num-executors 3 --executor-cores 2 ${SPARK_HOME}/examples/jars/spark-examples_2.12-3.2.3.jar
    ${SPARK_HOME}/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client  --driver-memory 1G --executor-memory 1G --num-executors 3 --executor-cores 2 ${SPARK_HOME}/examples/jars/spark-examples_2.12-3.2.3.jar
    
    # 计算 wc
    ${HADOOP_HOME}/bin/hadoop fs -mkdir -p /spark/test/wc/input                # 创建 hdfs 数据输入目录
    ${HADOOP_HOME}/bin/hadoop fs -ls /hadoop/test/wc/input                     # 查看创建的目录
    ${HADOOP_HOME}/bin/hadoop fs -put ${SPARK_HOME}/RELEASE /spark/test/wc/input    # 上传文件到 hdfs
    ${SPARK_HOME}/bin/spark-submit --class com.example.spark.ScalaWordCount --master yarn --deploy-mode client --driver-memory 1G --executor-memory 1G --total-executor-cores 2 /home/issac/coding/java/jar/wc-1.0.jar hdfs:///spark/test/input
```



## 4. Zookeeper 安装

### 4.1 zookeeper-3.6.4 下载

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[zookeeper-3.6.4](https://mirrors.aliyun.com/apache/zookeeper/zookeeper-3.6.4/apache-zookeeper-3.6.4-bin.tar.gz)** 安装包到本地

### 4.2 解压安装 zookeeper

```bash
    tar -zxvf apache-zookeeper-3.6.4-bin.tar.gz -C /opt/apache/                # 解压编译后的压缩包
    mv /opt/apache/apache-zookeeper-3.6.4-bin /opt/apache/zookeeper            # 修改目录名称
    mkdir -p /opt/apache/zookeeper/data/                                       # 创建数据存储目录
    mkdir -p /opt/apache/zookeeper/logs/                                       # 创建日志目录
```

### 4.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Zookeeper 3.6.4 ====================================== #
        export ZOOKEEPER_HOME=/opt/apache/zookeeper
        export PATH=${PATH}:${ZOOKEEPER_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 4.4 修改配置文件

#### 4.4.1 创建配置文件

```bash
    cd ${ZOOKEEPER_HOME}/conf/ || exit                                         # 切换到 zookeeper 配置文件目录
    touch /opt/apache/zookeeper/conf/zoo.cfg                                   # zookeeper 主要配置文件
    cp ${ZOOKEEPER_HOME}/conf/log4j.properties.template ${ZOOKEEPER_HOME}/conf/log4j.properties    # 日志配置文件
    mkdir -p ${ZOOKEEPER_HOME}/data                                            # 创建 ZK 数据存储目录
    mkdir -p ${ZOOKEEPER_HOME}/logs                                            # 创建 ZK 日志存储目录
```

#### 4.4.2 修改 ${ZOOKEEPER_HOME}/conf/zoo.cfg 

```bash
    # 用来调节心跳和超时, 默认的会话超时时间是两倍的 tickTime
    tickTime=2000
    
    # 用于配置允许 followers 连接并同步到 leader 的最大时间
    initLimit=10
    
    # 配置leader 和 followers 间进行心跳检测的最大延迟时间
    syncLimit=5
    
    # 存储内存数据库快照目录, 并且除非指定其它目录, 否则数据库更新的事务日志也将会存储在该目录下
    dataDir=/opt/apache/zookeeper/data
    
    # 配置 dataLogDir 参数来指定 ZooKeeper 事务日志的存储目录
    dataLogDir=/opt/apache/zookeeper/logs
    
    # 服务器监听客户端连接的端口, 也即客户端尝试连接的端口, 默认值是 2181 
    clientPort=2181
    
    # 不然会出现端口被占用的情况，因为默认是和 Apache.Tomcat 使用的 8080 端口
    admin.serverPort=8180
    
    # 限制单个客户端与单台服务器之前的并发连接数量, 可以通过 IP 地址来区分不同的客户端，它用来防止某种类型的 DoS 攻击, 将其设置为 0 将完全移除并发连接数的限制
    # maxClientCnxns=60
    
    # ZooKeeper 自动清理时需要保留的数据文件快照的数量和对应的事务日志文件, 默认值是 3
    # autopurge.snapRetainCount=3
    
    # 和 autopurge.snapRetainCount 配套使用, 用于配置 ZooKeeper 自动清理文件的频率，默认值是 1, 即默认开启自动清理功能, 设置为 0 则表示禁用自动清理功能。
    # autopurge.purgeInterval=1
    
    
    ## Metrics Providers
    # https://prometheus.io Metrics Exporter
    #metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
    #metricsProvider.httpPort=7000
    #metricsProvider.exportJvmInfo=true
    
    # 服务器唯一标识
    server.0=master:2888:3888
    server.1=slaver1:2888:3888
    server.2=slaver2:2888:3888
    server.2=slaver3:2888:3888
```

#### 4.4.3 创建 ${ZOOKEEPER_HOME}/data/myid

```bash
    # 创建每个节点独有的版本号
    echo "0" >> ${ZOOKEEPER_HOME}/data/myid                                    # master
    echo "1" >> ${ZOOKEEPER_HOME}/data/myid                                    # slaver1
    echo "2" >> ${ZOOKEEPER_HOME}/data/myid                                    # slaver2
    echo "3" >> ${ZOOKEEPER_HOME}/data/myid                                    # slaver3
```

### 4.6 编写 zookeeper 启停脚本

#### 4.6.1 创建 zookeeper.sh

```bash
    cd /opt/apache/zookeeper/ || exit                                          # 切换到 zookeeper 安装目录的 bin
    touch ${ZOOKEEPER_HOME}/bin/zookeeper.sh                                   # 脚本内容，如：4.6.2
    touch ${ISSAC_HOME}/shell/zookeeper-cluster.sh                             # 脚本内容，如：4.6.3
    chmod +x ${ZOOKEEPER_HOME}/bin/zookeeper.sh                                # 添加可执行权限
    chmod +x ${ISSAC_HOME}/shell/zookeeper-cluster.sh                          # 添加可执行权限
```

#### 4.6.2 ${ZOOKEEPER_HOME}/bin/zookeeper.sh

```bashpro shell script
    #!/usr/bin/env bash
    
    
    SERVICE_DIR=$(cd "$(dirname "$0")/../" || exit; pwd)
    SERVICE_NAME=Zookeeper
    JUDGE_NAME=org.apache.zookeeper
    SERVICE_PORT=2181
    SERVER_PORT=8180
    
    
    printf "\n=========================================================================\n"
    #  匹配输入参数
    case "$1" in
        #  1. 运行程序
        start)
            # 1.1 查找程序的 pid
            pid_list=$(ps -aux | grep -i "${JUDGE_NAME}" | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}')
            
            #  1.2 若 pid 不存在，则运行程序，否则打印程序运行状态
            if [ ! "${pid_list}" ]; then
                echo "    程序 ${SERVICE_NAME} 正在加载中 ......"
                "${SERVICE_DIR}/bin/zkServer.sh" start > /dev/null 2>&1
                sleep 5
                
                # 1.9 判断所有程序启动是否成功
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${pid_count}" -ge 1 ]; then
                    echo "    程序 ${SERVICE_NAME} 启动成功 ...... "
                else
                    echo "    程序 ${SERVICE_NAME} 启动失败 ...... "
                fi
                
            else
                echo "    程序 ${SERVICE_NAME} 正在运行当中 ...... "
            fi
        ;;
        
        #  2. 停止
        stop)
            # 2.1 根据程序的 pid 查询程序运行状态
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            if [ "${pid_count}" -eq 0 ]; then
                echo "    ${SERVICE_NAME} 的进程不存在，程序没有运行 ...... "
            else
                # 2.2 关闭进程，关闭程序
                echo "    程序 ${SERVICE_NAME} 正在停止中 ...... "
                "${SERVICE_DIR}/bin/zkServer.sh" stop > /dev/null 2>&1
                sleep 5
    
                # 2.3 若还未关闭，则强制杀死进程，关闭程序
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | wc -l)
                if [ "${pid_count}" -ge 1 ]; then
                    temp=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | xargs kill -9)
                fi
                
                echo "    程序 ${SERVICE_NAME} 已经停止成功 ......"
            fi
        ;;
        
        #  3. 状态查询
        status)
            # 3.1 查看正在运行程序的 pid
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            #  3.2 判断 ES 运行状态
            if [ "${pid_count}" -eq 0 ]; then
                echo "    程序 ${SERVICE_NAME} 已经停止 ...... "
            else
                echo "    程序 ${SERVICE_NAME} 正在运行中 ...... "
            fi
        ;;
        
        #  4. 重启程序
        restart)
            "$0" stop
            sleep 3
            "$0" start
        ;;
        
        #  5. 其它情况
        *)
            echo "    脚本可传入一个参数，如下所示：              "
            echo "        +-----------------------------------+ "
            echo "        |  start | stop | restart | status  | "
            echo "        +-----------------------------------+ "
            echo "        |        start    ：  启动服务      | "
            echo "        |        stop     ：  关闭服务      | "
            echo "        |        restart  ：  重启服务      | "
            echo "        |        status   ：  查看状态      | "
            echo "        +-----------------------------------+ "
        ;;
    esac
    printf "=========================================================================\n\n"
```

#### 4.6.3 ${ISSAC_HOME}/shell/zookeeper-cluster.sh

```bashpro shell script
    #!/usr/bin/env bash
        
    ZOOKEEPER_HOME=/opt/apache/zookeeper
    HOST_LIST=(master slaver1 slaver2 slaver3)
    
    for host_name in "${HOST_LIST[@]}"
    do
        echo "-------------------- zookeeper ${host_name} 启动 --------------------"
        ssh ${host_name} "source /etc/profile; nohup ${ZOOKEEPER_HOME}/bin/zookeeper.sh $1 >> ${ZOOKEEPER_HOME}/logs/cluster.log 2>&1 &"
    done
```

### 4.7 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 zookeeper 安装父路径
    ~/shell/xync.sh zookeeper                                                  # 同步 zookeeper 到其它节点
````

### 4.8 启动 Zookeeper 并验证

```bash
    # 启动 zookeeper 集群
   ${ZOOKEEPER_HOME}/bin/zkServer.sh start                                     # 分别在 master、slaver1、slaver2、slaver3 执行 
   ${ZOOKEEPER_HOME}/bin/zookeeper.sh start                                    # 分别在 master、slaver1、slaver2、slaver3 执行 
   ${ISSAC_HOME}/shell/zookeeper-cluster.sh start                              # 一次性启动集群
   
   # 查看各节点状态
   jps -l 
   ${ZOOKEEPER_HOME}/bin/zkServer.sh status                                    # 在每个节点执行查看
   ${ISSAC_HOME}/shell/xcall.sh ${ZOOKEEPER_HOME}/bin/zkServer.sh status       # 在主节点 master 执行查看
```



## 5. Kafka 安装配置

### 5.1 下载 kafka-3.2.3 安装包

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[kafka-3.2.3](https://mirrors.aliyun.com/apache/kafka/3.2.3/kafka_2.12-3.2.3.tgz)** 安装包到本地

### 5.2 解压安装 kafka

```bash
    tar -zxvf kafka_2.12-3.2.3.tgz -C /opt/apache/                             # 解压 压缩包
    mv /opt/apache/kafka_2.12-3.2.3 /opt/apache/kafka                          # 修改目录名称
    mkdir -p /opt/apache/kafka/data/                                           # 创建数据存储目录
    mkdir -p /opt/apache/kafka/logs/                                           # 创建日志目录
```

### 5.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Kafka 3.2.3 ====================================== #
        export KAFKA_HOME=/opt/apache/kafka
        export PATH=${PATH}:${KAFKA_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 5.4 修改配置文件

#### 5.4.1 修改 ${KAFKA_HOME}/config/zookeeper.properties

```properties
    # kafka 的数据在 zookeeper 中存储位置
    dataDir=/opt/apache/zookeeper/data
    
    # zookeeper 的端口号
    clientPort=2181
    
    # disable the per-ip limit on the number of connections since this is a non-production config
    maxClientCnxns=0
    
    # Disable the adminserver by default to avoid port conflicts.
    # Set the port to something non-conflicting if choosing to enable this
    admin.enableServer=false
    # admin.serverPort=808
```

#### 5.4.2 修改 ${KAFKA_HOME}/config/producer.properties

```properties
    bootstrap.servers=slaver1:9092,slaver2:9092,slaver3:9092
    
    # specify the compression codec for all data generated: none, gzip, snappy, lz4, zstd
    compression.type=gzip
```

#### 5.4.2 修改 ${KAFKA_HOME}/config/server.properties

```properties
    ############################# Server Basics #############################
    # 配合 broker 的 id：对于每个 broker.id 来说，必须设置为唯一的整数，且从 0 开始
    broker.id=0
    # 删除 topic 功能
    delete.topic.enable=true
    
    ############################# Socket Server Settings #############################
    listeners=PLAINTEXT://slaver1:9092,slaver2:9092,slaver3:9092
    advertised.listeners=PLAINTEXT://slaver1:9092,slaver2:9092,slaver3:9092
    # listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
    
    num.network.threads=3
    num.io.threads=8
    socket.send.buffer.bytes=102400
    socket.receive.buffer.bytes=102400
    socket.request.max.bytes=104857600
    
    ############################# Log Basics #############################
    # 数据日志文件存储路径（用逗号分隔的目录列表）
    log.dirs=/opt/apache/kafka/data
    num.partitions=3
    num.recovery.threads.per.data.dir=1
    
    ############################# Internal Topic Settings  #############################
    offsets.topic.replication.factor=3
    transaction.state.log.replication.factor=3
    transaction.state.log.min.isr=1
    
    ############################# Log Flush Policy #############################
    # log.flush.interval.messages=10000
    # log.flush.interval.ms=1000
    
    ############################# Log Retention Policy #############################
    log.retention.hours=168
    log.segment.bytes=1073741824
    log.retention.check.interval.ms=300000
    
    ############################# Zookeeper #############################
    # 配置连接 zookeeper 的集群地址（用逗号分隔的目录列表）
    zookeeper.connect=issac:2181/kafka
    
    # 连接 zookeeper 超时
    zookeeper.connection.timeout.ms=20000
    
    group.initial.rebalance.delay.ms=0
```

#### 5.4.2 修改 ${KAFKA_HOME}/config/consumer.properties

```properties
    bootstrap.servers=slaver1:9092,slaver2:9092,slaver3:9092
    group.id=test-consumer-group
    # auto.offset.reset=
```

### 5.5 安装 kafka 的可视化工具 efak 

#### 5.5.1 下载 efak-3.0.1 安装包

从 [**kafka-eagle 官网**](http://www.kafka-eagle.org/) 找到下载按钮，并 **[点击按钮](https://github.com/smartloli/kafka-eagle-bin/blob/master/efak-web-3.0.2-bin.tar.gz)** 下载安装包到本地

#### 5.5.2 解压安装 efak

```bash
    tar -zxvf kafka-eagle-bin-3.0.1.tgz                                        # 解压 压缩包
    cd kafka_2.12-3.2.3/                                                       # 进入到解压路径
    tar -zxvf efak-web-3.0.1-bin.tar.gz -C ${KAFKA_HOME}/                      # 解压 压缩包
    mv ${KAFKA_HOME}/efak-web-3.0.1 ${KAFKA_HOME}/efak                         # 修改目录名称
```

#### 5.5.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== efak 3.2.3 ====================================== #
        export KE_HOME=/opt/apache/kafka/efak
        export PATH=${PATH}:${KE_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile
```

#### 5.5.3 修改配置文件 ${KE_HOME}/conf/system-config.properties

```properties
    cluster1.zk.list=master:2181/kafka,slaver1:2181/kafka,slaver2:2181/kafka,slaver3:2181/kafka    # 7 行
    efak.webui.port=8048                                                                           # 31 行
    efak.worknode.port=8085                                                                        # 39 行
    cluster1.efak.offset.storage=kafka                                                             # 54 行
    
    # 元数据存储路径（125-128 行）
    efak.driver=com.mysql.cj.jdbc.Driver
    efak.url=jdbc:mysql://master:3306/other?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
    efak.username=issac
    efak.password=111111
```

#### 5.5.4 修改配置文件 ${KE_HOME}/conf/works

```bash
    echo "slaver1" >> ${KE_HOME}/conf/works 
    echo "slaver2" >> ${KE_HOME}/conf/works 
    echo "slaver3" >> ${KE_HOME}/conf/works 
```

### 5.6 编写 kafka 启停脚本： ${KAFKA_HOME}/bin/kafka.sh

```bashpro shell script
    #!/usr/bin/env bash
    
    
    SERVICE_DIR=$(cd "$(dirname "$0")/../" || exit; pwd)
    SERVICE_NAME=Kafka
    JUDGE_NAME=kafka.Kafka
    CONF_FILE=config/server.properties
    SERVICE_PORT=9092
    
    
    printf "\n=========================================================================\n"
    #  匹配输入参数
    case "$1" in
        #  1. 运行程序
        start)
            # 1.1 查找程序的 pid
            pid_list=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v "$0" | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}')
            
            #  1.2 若 pid 不存在，则运行程序，否则打印程序运行状态
            if [ ! "${pid_list}" ]; then
                "${SERVICE_DIR}/bin/kafka-server-start.sh" -daemon "${SERVICE_DIR}/${CONF_FILE}" > /dev/null 2>&1 & 
                
                echo "    程序（${SERVICE_NAME}）正在加载中 ......"
                sleep 2
                echo "    程序（${SERVICE_NAME}）启动验证中 ...... "
                sleep 3
                
                # 1.3 判断所有程序启动是否成功
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v "$0" | grep -v grep | awk '{print $2}' | wc -l)
                if [ "${pid_count}" -ge 1 ]; then
                    echo "    程序（${SERVICE_NAME}）启动成功 ...... "
                else
                    echo "    程序（${SERVICE_NAME}）启动失败 ...... "
                fi
            else
                echo "    程序（${SERVICE_NAME}）正在运行当中 ...... "
            fi
        ;;
        
        
        #  2. 停止
        stop)
            # 2.1 根据程序的 pid 查询程序运行状态
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v "$0" | grep -v grep | awk '{print $2}' | wc -l)
            if [ "${pid_count}" -eq 0 ]; then
                echo "    程序（${SERVICE_NAME}）的进程不存在，程序没有运行 ...... "
            else
                # 2.2 杀死进程，关闭程序
                "${SERVICE_DIR}/bin/kafka-server-stop.sh" > /dev/null 2>&1
                sleep 2
                echo "    程序（${SERVICE_NAME}）正在停止中 ...... "
                sleep 3
    
                # 2.3 若还未关闭，则强制杀死进程，关闭程序
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v "$0" | grep -v grep | awk '{print $2}' | wc -l)
                if [ "${pid_count}" -ge 1 ]; then
                    temp=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v "$0" | grep -v grep | awk '{print $2}' | xargs kill -9)
                fi
                
                echo "    程序（${SERVICE_NAME}）已经停止成功 ......"
            fi
        ;;
        
        
        #  3. 状态查询
        status)
            # 3.1 查看正在运行程序的 pid
            pid_list=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v "$0" | grep -v grep | awk '{print $2}')
            #  3.2 判断 ES 运行状态
            
            if [ ! "${pid_list}" ]; then
                echo "    程序（${SERVICE_NAME}）已经停止 ...... "
            else
                echo "    程序（${SERVICE_NAME}）正在运行中 ...... "
            fi
        ;;
        
        
        #  4. 重启程序
        restart)
            "$0" stop
            sleep 3 
            "$0" start
        ;;
        
        
        #  5. 其它情况
        *)
            echo "    脚本可传入一个参数，如下所示：              "
            echo "        +-----------------------------------+ "
            echo "        |  start | stop | restart | status  | "
            echo "        +-----------------------------------+ "
            echo "        |        start    ：  启动服务      | "
            echo "        |        stop     ：  关闭服务      | "
            echo "        |        restart  ：  重启服务      | "
            echo "        |        status   ：  查看状态      | "
            echo "        +-----------------------------------+ "
        ;;
    esac
    printf "=========================================================================\n\n"
```

### 5.7 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 kafka 安装父路径
    ~/shell/xync.sh kafka                                                      # 同步 kafka 到其它节点
````

### 5.8 启动 kafka 和 efak 集群 启停脚本：kafka-cluster.sh

```bashpro shell script
    #!/usr/bin/env bash
        
    KAFKA_HOME=/opt/apache/kafka
    HOST_LIST=(slaver1 slaver2 slaver3)
    
    for host_name in "${HOST_LIST[@]}"
    do
        echo "-------------------- kafka（${host_name}）启动 --------------------"
        ssh ${host_name} "source /etc/profile; nohup ${KAFKA_HOME}/bin/kafka.sh $1 >> ${KAFKA_HOME}/logs/cluster.log 2>&1 &; nohup ${KE_HOME}/bin/ke.sh $1 >> ${KE_HOME}/logs/cluster.log 2>&1 &"
    done
```

### 5.9 启动 kafka 和 efak 

```bash
    ${KAFKA_HOME}/bin/kafka-server-start.sh -daemon ${KAFKA_HOME}/config/server.properties > /dev/null 2>&1 &     # 每个节点单独启动 
    ${KAFKA_HOME}/bin/kafka.sh start                                           # 每个节点单独启动 
    ${ISSAC_HOME}/shell/kafka-cluster.sh start                                 # 集群启动
    
    jps -l                                                                     # 查看 kafka 启动的 jvm 进程
    http://issac:8048/                                                         # EFAK UI
```


### 5.10 kafka 测试

```bash
    # 查看当前服务器中的所有 topic
    ${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server slaver1:9092,slaver2:9092,slaver3:9092 --list     
    
    # 创建 topic
    ${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server slaver1:9092,slaver2:9092,slaver3:9092 --create --topic test --replication-factor 3 --partitions 3
    
    # 发送消息
    ${KAFKA_HOME}/bin/kafka-console-producer.sh --broker-list slaver1:9092,slaver2:9092,slaver3:9092 --topic test   
    
    # 1.5 消费消息
    ${KAFKA_HOME}/bin/kafka-console-consumer.sh --bootstrap-server slaver1:9092,slaver2:9092,slaver3:9092 --from-beginning --topic test
```



## 6. Hive 安装配置

### 6.1 下载 Hive-3.1.3 源码

使用 git 从 [**github**](https://github.com/) 下载 **[Hive-3.1.3](https://github.com/apache/hive.git)** 源码到安装有图形化界面的 linux 系统

### 6.2 编译 Hive（hive-3.1.3 和 hadoop-3.2.4、spark-3.2.3 在兼容性上存在问题，需要修改源码重新编译）

#### 6.2.1 clone hive-3.1.3 源码，测试编译

```bash
    # 下载源码，并编译
    git clone https://github.com/apache/hive.git                               # clone 源码
    cd hive || exit                                                            # 进入解压路

    # 切换分支
    git branch                                                                 # 查看本地分支
    git branch -r                                                              # 查看远程分支，-r 表示 remote
    git branch -a                                                              # 查看所有分支
    git checkout rel/release-3.1.3                                             # 切换到 3.1.3 分支
    
    mvn clean -DskipTests package -Pdist                                       # 跳过测试，对 hive 进行编译打包
```

### 6.2 修改源码

#### 6.2.1 创建 issac.patch 补丁

```bash
    touch issac-hive.patch                                                     # 创建补丁，补丁内容如：6.2.2
    
    git apply --check ./issac-hive.patch                                       # 检查 patch 是否可用
    git apply ./issac-hive.patch                                               # 应用补丁，不包含 commit 内容
    git am ./issac-hive.patch                                                  # 应用补丁，包含 commit 内容
    
    mvn clean -DskipTests package -Pdist                                       # 跳过测试，对 hive 进行编译打包
```

#### 6.2.2 issac-hive.patch

```java
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DoubleColumnStatsAggregator.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DoubleColumnStatsAggregator.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DoubleColumnStatsAggregator.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DoubleColumnStatsAggregator.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DoubleColumnStatsAggregator.java	(date 1666966498503)
    @@ -37,6 +37,8 @@
     import org.slf4j.Logger;
     import org.slf4j.LoggerFactory;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.doubleInspectorFromStats;
    +
     public class DoubleColumnStatsAggregator extends ColumnStatsAggregator implements
         IExtrapolatePartStatus {
     
    @@ -63,7 +65,7 @@
                 doAllPartitionContainStats);
           }
           DoubleColumnStatsDataInspector doubleColumnStatsData =
    -          (DoubleColumnStatsDataInspector) cso.getStatsData().getDoubleStats();
    +          doubleInspectorFromStats(cso);
           if (doubleColumnStatsData.getNdvEstimator() == null) {
             ndvEstimator = null;
             break;
    @@ -95,8 +97,7 @@
           double densityAvgSum = 0.0;
           for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {
             ColumnStatisticsObj cso = csp.getColStatsObj();
    -        DoubleColumnStatsDataInspector newData =
    -            (DoubleColumnStatsDataInspector) cso.getStatsData().getDoubleStats();
    +        DoubleColumnStatsDataInspector newData = doubleInspectorFromStats(cso);
             lowerBound = Math.max(lowerBound, newData.getNumDVs());
             higherBound += newData.getNumDVs();
             densityAvgSum += (newData.getHighValue() - newData.getLowValue()) / newData.getNumDVs();
    @@ -173,7 +174,7 @@
               ColumnStatisticsObj cso = csp.getColStatsObj();
               String partName = csp.getPartName();
               DoubleColumnStatsDataInspector newData =
    -              (DoubleColumnStatsDataInspector) cso.getStatsData().getDoubleStats();
    +              doubleInspectorFromStats(cso);
               // newData.isSetBitVectors() should be true for sure because we
               // already checked it before.
               if (indexMap.get(partName) != curIndex) {
    Index: pom.xml
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/pom.xml b/pom.xml
    --- a/pom.xml	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/pom.xml	(date 1674057235771)
    @@ -12,1533 +12,1545 @@
       See the License for the specific language governing permissions and
       limitations under the License.
     -->
    -<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    -  <modelVersion>4.0.0</modelVersion>
    -  <parent>
    -    <groupId>org.apache</groupId>
    -    <artifactId>apache</artifactId>
    -    <version>18</version>
    -  </parent>
    -  <groupId>org.apache.hive</groupId>
    -  <artifactId>hive</artifactId>
    -  <version>3.1.3</version>
    -  <packaging>pom</packaging>
    -
    -  <name>Hive</name>
    -  <url>https://hive.apache.org</url>
    -  <prerequisites>
    -    <maven>2.2.1</maven>
    -  </prerequisites>
    -
    -  <modules>
    -    <module>accumulo-handler</module>
    -    <module>vector-code-gen</module>
    -    <module>beeline</module>
    -    <module>classification</module>
    -    <module>cli</module>
    -    <module>common</module>
    -    <module>contrib</module>
    -    <module>druid-handler</module>
    -    <module>hbase-handler</module>
    -    <module>jdbc-handler</module>
    -    <module>hcatalog</module>
    -    <module>hplsql</module>
    -    <module>jdbc</module>
    -    <module>metastore</module>
    -    <module>ql</module>
    -    <module>serde</module>
    -    <module>service-rpc</module>
    -    <module>service</module>
    -    <module>streaming</module>
    -    <module>llap-common</module>
    -    <module>llap-client</module>
    -    <module>llap-ext-client</module>
    -    <module>llap-tez</module>
    -    <module>llap-server</module>
    -    <module>shims</module>
    -    <module>spark-client</module>
    -    <module>kryo-registrator</module>
    -    <module>testutils</module>
    -    <module>packaging</module>
    -    <module>standalone-metastore</module>
    -    <module>upgrade-acid</module>
    -  </modules>
    -
    -  <properties>
    -    <hive.version.shortname>3.1.0</hive.version.shortname>
    -
    -    <!-- Build Properties -->
    -    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    -    <maven.compiler.source>1.8</maven.compiler.source>
    -    <maven.compiler.target>1.8</maven.compiler.target>
    -    <maven.compiler.useIncrementalCompilation>false</maven.compiler.useIncrementalCompilation>
    -    <maven.repo.local>${settings.localRepository}</maven.repo.local>
    -    <hive.path.to.root>.</hive.path.to.root>
    -    <hive.jdbc.driver.classifier>standalone</hive.jdbc.driver.classifier>
    -    <checkstyle.conf.dir>${basedir}/${hive.path.to.root}/checkstyle</checkstyle.conf.dir>
    -
    -    <!-- Test Properties -->
    -    <test.extra.path></test.extra.path>
    -    <test.hive.hadoop.classpath>${maven.test.classpath}</test.hive.hadoop.classpath>
    -    <test.log4j.scheme>file://</test.log4j.scheme>
    -    <test.tmp.dir>${project.build.directory}/tmp</test.tmp.dir>
    -    <test.conf.dir>${project.build.directory}/testconf</test.conf.dir>
    -    <test.tmp.dir.uri>file://${test.tmp.dir}</test.tmp.dir.uri>
    -    <!-- Determines the log level of the console logger, hive.log is independent of this-->
    -    <test.console.log.level>INFO</test.console.log.level>
    -    <test.warehouse.dir>${project.build.directory}/warehouse</test.warehouse.dir>
    -    <test.warehouse.scheme>pfile://</test.warehouse.scheme>
    -
    -    <!-- To add additional exclude patterns set this property -->
    -    <test.excludes.additional></test.excludes.additional>
    -    <skip.spark.files></skip.spark.files>
    -
    -    <!-- Plugin and Plugin Dependency Versions -->
    -    <ant.contrib.version>1.0b3</ant.contrib.version>
    -    <datanucleus.maven.plugin.version>3.3.0-release</datanucleus.maven.plugin.version>
    -    <maven.test.jvm.args>-Xmx2048m</maven.test.jvm.args>
    -    <maven.antrun.plugin.version>1.7</maven.antrun.plugin.version>
    -    <maven.assembly.plugin.version>2.3</maven.assembly.plugin.version>
    -    <maven.checkstyle.plugin.version>2.17</maven.checkstyle.plugin.version>
    -    <maven.compiler.plugin.version>3.6.1</maven.compiler.plugin.version>
    -    <maven.enforcer.plugin.version>1.3.1</maven.enforcer.plugin.version>
    -    <maven.install.plugin.version>2.4</maven.install.plugin.version>
    -    <maven.jar.plugin.version>2.4</maven.jar.plugin.version>
    -    <maven.javadoc.plugin.version>2.4</maven.javadoc.plugin.version>
    -    <maven.shade.plugin.version>3.1.0</maven.shade.plugin.version>
    -    <maven.surefire.plugin.version>2.21.0</maven.surefire.plugin.version>
    -    <maven.war.plugin.version>2.4</maven.war.plugin.version>
    -    <maven.dependency.plugin.version>2.8</maven.dependency.plugin.version>
    -    <maven.eclipse.plugin.version>2.9</maven.eclipse.plugin.version>
    -    <maven.build-helper.plugin.version>1.8</maven.build-helper.plugin.version>
    -
    -    <!-- Library Dependency Versions -->
    -    <accumulo.version>1.7.3</accumulo.version>
    -    <activemq.version>5.5.0</activemq.version>
    -    <ant.version>1.9.1</ant.version>
    -    <antlr.version>3.5.2</antlr.version>
    -    <apache-directory-server.version>1.5.6</apache-directory-server.version>
    -    <apache-directory-clientapi.version>0.1</apache-directory-clientapi.version>
    -    <!-- Include arrow for LlapOutputFormatService -->
    -    <arrow.version>0.8.0</arrow.version>
    -    <avatica.version>1.11.0</avatica.version>
    -    <avro.version>1.8.2</avro.version>
    -    <bonecp.version>0.8.0.RELEASE</bonecp.version>
    -    <calcite.version>1.16.0</calcite.version>
    -    <datanucleus-api-jdo.version>4.2.4</datanucleus-api-jdo.version>
    -    <datanucleus-core.version>4.1.17</datanucleus-core.version>
    -    <datanucleus-rdbms.version>4.1.19</datanucleus-rdbms.version>
    -    <datanucleus-jdo.version>3.2.0-m3</datanucleus-jdo.version>
    -    <commons-cli.version>1.2</commons-cli.version>
    -    <commons-codec.version>1.15</commons-codec.version>
    -    <commons-collections.version>3.2.2</commons-collections.version>
    -    <commons-compress.version>1.19</commons-compress.version>
    -    <commons-exec.version>1.1</commons-exec.version>
    -    <commons-io.version>2.6</commons-io.version>
    -    <commons-lang.version>2.6</commons-lang.version>
    -    <commons-lang3.version>3.9</commons-lang3.version>
    -    <commons-pool.version>1.5.4</commons-pool.version>
    -    <commons-dbcp.version>1.4</commons-dbcp.version>
    -    <derby.version>10.14.1.0</derby.version>
    -    <dropwizard.version>3.1.0</dropwizard.version>
    -    <dropwizard-metrics-hadoop-metrics2-reporter.version>0.1.2</dropwizard-metrics-hadoop-metrics2-reporter.version>
    -    <druid.version>0.12.0</druid.version>
    -    <flatbuffers.version>1.2.0-3f79e055</flatbuffers.version>
    -    <guava.version>19.0</guava.version>
    -    <groovy.version>2.4.11</groovy.version>
    -    <h2database.version>1.3.166</h2database.version>
    -    <hadoop.version>3.1.0</hadoop.version>
    -    <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop</hadoop.bin.path>
    -    <hamcrest.version>1.3</hamcrest.version>
    -    <hbase.version>2.0.0-alpha4</hbase.version>
    -    <hppc.version>0.7.2</hppc.version>
    -    <!-- required for logging test to avoid including hbase which pulls disruptor transitively -->
    -    <disruptor.version>3.3.7</disruptor.version>
    -    <hikaricp.version>2.6.1</hikaricp.version>
    -    <!-- httpcomponents are not always in version sync -->
    -    <httpcomponents.client.version>4.5.13</httpcomponents.client.version>
    -    <httpcomponents.core.version>4.4.13</httpcomponents.core.version>
    -    <ivy.version>2.4.0</ivy.version>
    -    <jackson.version>2.12.0</jackson.version>
    -    <jamon.plugin.version>2.3.4</jamon.plugin.version>
    -    <jamon-runtime.version>2.3.1</jamon-runtime.version>
    -    <javaewah.version>0.3.2</javaewah.version>
    -    <javax-servlet.version>3.1.0</javax-servlet.version>
    -    <javax-servlet-jsp.version>2.3.1</javax-servlet-jsp.version>
    -    <javolution.version>5.5.1</javolution.version>
    -    <jdo-api.version>3.0.1</jdo-api.version>
    -    <jettison.version>1.1</jettison.version>
    -    <jetty.version>9.3.20.v20170531</jetty.version>
    -    <jersey.version>1.19</jersey.version>
    -    <!-- Glassfish jersey is included for Spark client test only -->
    -    <glassfish.jersey.version>2.22.2</glassfish.jersey.version>
    -    <jline.version>2.12</jline.version>
    -    <jms.version>2.0.2</jms.version>
    -    <joda.version>2.9.9</joda.version>
    -    <jodd.version>3.5.2</jodd.version>
    -    <json.version>1.8</json.version>
    -    <junit.version>4.11</junit.version>
    -    <kryo.version>3.0.3</kryo.version>
    -    <libfb303.version>0.9.3</libfb303.version>
    -    <libthrift.version>0.9.3</libthrift.version>
    -    <log4j2.version>2.17.1</log4j2.version>
    -    <opencsv.version>2.3</opencsv.version>
    -    <orc.version>1.5.8</orc.version>
    -    <mockito-all.version>1.10.19</mockito-all.version>
    -    <mina.version>2.0.0-M5</mina.version>
    -    <netty.version>4.1.17.Final</netty.version>
    -    <netty3.version>3.10.5.Final</netty3.version>
    -    <parquet.version>1.10.0</parquet.version>
    -    <pig.version>0.16.0</pig.version>
    -    <plexus.version>1.5.6</plexus.version>
    -    <protobuf.version>2.5.0</protobuf.version>
    -    <stax.version>1.0.1</stax.version>
    -    <slf4j.version>1.7.10</slf4j.version>
    -    <ST4.version>4.0.4</ST4.version>
    -    <storage-api.version>2.7.0</storage-api.version>
    -    <tez.version>0.9.1</tez.version>
    -    <super-csv.version>2.2.0</super-csv.version>
    -    <spark.version>2.3.0</spark.version>
    -    <scala.binary.version>2.11</scala.binary.version>
    -    <scala.version>2.11.8</scala.version>
    -    <tempus-fugit.version>1.1</tempus-fugit.version>
    -    <snappy.version>1.1.4</snappy.version>
    -    <wadl-resourcedoc-doclet.version>1.4</wadl-resourcedoc-doclet.version>
    -    <velocity.version>2.3</velocity.version>
    -    <xerces.version>2.9.1</xerces.version>
    -    <zookeeper.version>3.4.6</zookeeper.version>
    -    <jpam.version>1.1</jpam.version>
    -    <felix.version>2.4.0</felix.version>
    -    <curator.version>2.12.0</curator.version>
    -    <jsr305.version>3.0.0</jsr305.version>
    -    <tephra.version>0.6.0</tephra.version>
    -    <gson.version>2.2.4</gson.version>
    -  </properties>
    -
    -  <repositories>
    -    <repository>
    -      <id>central</id>
    -      <name>central</name>
    -      <url>https://repo.maven.apache.org/maven2</url>
    -      <layout>default</layout>
    -      <releases>
    -        <enabled>true</enabled>
    -        <checksumPolicy>warn</checksumPolicy>
    -      </releases>
    -    </repository>
    -  </repositories>
    -
    -  <dependencyManagement>
    -    <dependencies>
    -      <!-- dependencies are always listed in sorted order by groupId, artifectId -->
    -      <dependency>
    -        <groupId>com.esotericsoftware</groupId>
    -        <artifactId>kryo-shaded</artifactId>
    -        <version>${kryo.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.google.guava</groupId>
    -        <artifactId>guava</artifactId>
    -        <version>${guava.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.google.protobuf</groupId>
    -        <artifactId>protobuf-java</artifactId>
    -        <version>${protobuf.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.google.code.tempus-fugit</groupId>
    -        <artifactId>tempus-fugit</artifactId>
    -        <version>${tempus-fugit.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.hamcrest</groupId>
    -            <artifactId>hamcrest-core</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.googlecode.javaewah</groupId>
    -        <artifactId>JavaEWAH</artifactId>
    -        <version>${javaewah.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.jolbox</groupId>
    -        <artifactId>bonecp</artifactId>
    -        <version>${bonecp.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.zaxxer</groupId>
    -        <artifactId>HikariCP</artifactId>
    -        <version>${hikaricp.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.parquet</groupId>
    -        <artifactId>parquet</artifactId>
    -        <version>${parquet.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.parquet</groupId>
    -        <artifactId>parquet-column</artifactId>
    -        <version>${parquet.version}</version>
    -        <classifier>tests</classifier>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.sun.jersey</groupId>
    -        <artifactId>jersey-core</artifactId>
    -        <version>${jersey.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.sun.jersey</groupId>
    -        <artifactId>jersey-json</artifactId>
    -        <version>${jersey.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.sun.jersey</groupId>
    -        <artifactId>jersey-server</artifactId>
    -        <version>${jersey.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.sun.jersey.contribs</groupId>
    -        <artifactId>wadl-resourcedoc-doclet</artifactId>
    -        <version>${wadl-resourcedoc-doclet.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.sun.jersey</groupId>
    -        <artifactId>jersey-servlet</artifactId>
    -        <version>${jersey.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-cli</groupId>
    -        <artifactId>commons-cli</artifactId>
    -        <version>${commons-cli.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-codec</groupId>
    -        <artifactId>commons-codec</artifactId>
    -        <version>${commons-codec.version}</version>
    -      </dependency>
    -       <dependency>
    -        <groupId>commons-collections</groupId>
    -        <artifactId>commons-collections</artifactId>
    -        <version>${commons-collections.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-io</groupId>
    -        <artifactId>commons-io</artifactId>
    -        <version>${commons-io.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>commons-lang</groupId>
    -        <artifactId>commons-lang</artifactId>
    -        <version>${commons-lang.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>io.netty</groupId>
    -        <artifactId>netty-all</artifactId>
    -        <version>${netty.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>javax.jdo</groupId>
    -        <artifactId>jdo-api</artifactId>
    -        <version>${jdo-api.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>javax.jms</groupId>
    -        <artifactId>jms</artifactId>
    -        <version>${jms.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>javolution</groupId>
    -        <artifactId>javolution</artifactId>
    -        <version>${javolution.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>jline</groupId>
    -        <artifactId>jline</artifactId>
    -        <version>${jline.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>junit</groupId>
    -        <artifactId>junit</artifactId>
    -        <version>${junit.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.logging.log4j</groupId>
    -        <artifactId>log4j-1.2-api</artifactId>
    -        <version>${log4j2.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.logging.log4j</groupId>
    -        <artifactId>log4j-web</artifactId>
    -        <version>${log4j2.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.logging.log4j</groupId>
    -        <artifactId>log4j-slf4j-impl</artifactId>
    -        <version>${log4j2.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.antlr</groupId>
    -        <artifactId>antlr-runtime</artifactId>
    -        <version>${antlr.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.antlr</groupId>
    -        <artifactId>ST4</artifactId>
    -        <version>${ST4.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.commons</groupId>
    -        <artifactId>commons-compress</artifactId>
    -        <version>${commons-compress.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.commons</groupId>
    -        <artifactId>commons-exec</artifactId>
    -        <version>${commons-exec.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.accumulo</groupId>
    -        <artifactId>accumulo-core</artifactId>
    -        <version>${accumulo.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.accumulo</groupId>
    -        <artifactId>accumulo-fate</artifactId>
    -        <version>${accumulo.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.accumulo</groupId>
    -        <artifactId>accumulo-minicluster</artifactId>
    -        <version>${accumulo.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.accumulo</groupId>
    -        <artifactId>accumulo-start</artifactId>
    -        <version>${accumulo.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.accumulo</groupId>
    -        <artifactId>accumulo-trace</artifactId>
    -        <version>${accumulo.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.activemq</groupId>
    -        <artifactId>activemq-core</artifactId>
    -        <version>${activemq.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.springframework</groupId>
    -            <artifactId>spring-context</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.activemq</groupId>
    -        <artifactId>kahadb</artifactId>
    -        <version>${activemq.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.avro</groupId>
    -        <artifactId>avro</artifactId>
    -        <version>${avro.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.avro</groupId>
    -        <artifactId>avro-mapred</artifactId>
    -        <classifier>hadoop2</classifier>
    -        <version>${avro.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>jetty-util</artifactId>
    -          </exclusion>
    -        <exclusion>
    -          <groupId>org.mortbay.jetty</groupId>
    -          <artifactId>jetty</artifactId>
    -        </exclusion>
    -        </exclusions>
    -     </dependency>
    -      <dependency>
    -        <groupId>org.apache.derby</groupId>
    -        <artifactId>derby</artifactId>
    -        <version>${derby.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.httpcomponents</groupId>
    -        <artifactId>httpclient</artifactId>
    -        <version>${httpcomponents.client.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.httpcomponents</groupId>
    -        <artifactId>httpcore</artifactId>
    -        <version>${httpcomponents.core.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.velocity</groupId>
    -        <artifactId>velocity-engine-core</artifactId>
    -        <version>${velocity.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>stax</groupId>
    -        <artifactId>stax-api</artifactId>
    -        <version>${stax.version}</version>
    -      </dependency>
    -       <dependency>
    -        <groupId>org.apache.orc</groupId>
    -        <artifactId>orc-core</artifactId>
    -        <version>${orc.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-common</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.hive</groupId>
    -            <artifactId>hive-storage-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hive</groupId>
    -        <artifactId>hive-storage-api</artifactId>
    -        <version>${storage-api.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.pig</groupId>
    -        <artifactId>pig</artifactId>
    -        <version>${pig.version}</version>
    -         <exclusions>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>jetty-util</artifactId>
    -          </exclusion>
    -        <exclusion>
    -          <groupId>org.mortbay.jetty</groupId>
    -          <artifactId>jetty</artifactId>
    -        </exclusion>
    -        </exclusions>
    -     </dependency>
    -      <dependency>
    -        <groupId>org.apache.thrift</groupId>
    -        <artifactId>libfb303</artifactId>
    -        <version>${libfb303.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.thrift</groupId>
    -        <artifactId>libthrift</artifactId>
    -        <version>${libthrift.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.zookeeper</groupId>
    -        <artifactId>zookeeper</artifactId>
    -        <version>${zookeeper.version}</version>
    -        <exclusions>
    -            <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commmons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.httpcomponents</groupId>
    -            <artifactId>httpcore</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.httpcomponents</groupId>
    -            <artifactId>httpclient</artifactId>
    -          </exclusion>
    -         <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.curator</groupId>
    -        <artifactId>curator-client</artifactId>
    -        <version>${curator.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.curator</groupId>
    -        <artifactId>curator-framework</artifactId>
    -        <version>${curator.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.curator</groupId>
    -        <artifactId>curator-recipes</artifactId>
    -        <version>${curator.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.codehaus.groovy</groupId>
    -        <artifactId>groovy-all</artifactId>
    -        <version>${groovy.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.core</groupId>
    -        <artifactId>jackson-annotations</artifactId>
    -        <version>${jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.core</groupId>
    -        <artifactId>jackson-core</artifactId>
    -        <version>${jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.core</groupId>
    -        <artifactId>jackson-databind</artifactId>
    -        <version>${jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.module</groupId>
    -        <artifactId>jackson-module-scala_${scala.binary.version}</artifactId>
    -        <version>${jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.dataformat</groupId>
    -        <artifactId>jackson-dataformat-smile</artifactId>
    -        <version>${jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.jaxrs</groupId>
    -        <artifactId>jackson-jaxrs-json-provider</artifactId>
    -        <version>${jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.fasterxml.jackson.module</groupId>
    -        <artifactId>jackson-module-jaxb-annotations</artifactId>
    -        <version>${jackson.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.codehaus.jettison</groupId>
    -        <artifactId>jettison</artifactId>
    -        <version>${jettison.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>stax</groupId>
    -            <artifactId>stax-api</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-rewrite</artifactId>
    -        <version>${jetty.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-server</artifactId>
    -        <version>${jetty.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-servlet</artifactId>
    -        <version>${jetty.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.eclipse.jetty</groupId>
    -        <artifactId>jetty-webapp</artifactId>
    -        <version>${jetty.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>javax.servlet</groupId>
    -        <artifactId>javax.servlet-api</artifactId>
    -        <version>${javax-servlet.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.datanucleus</groupId>
    -        <artifactId>datanucleus-api-jdo</artifactId>
    -        <version>${datanucleus-api-jdo.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.datanucleus</groupId>
    -        <artifactId>datanucleus-core</artifactId>
    -        <version>${datanucleus-core.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.datanucleus</groupId>
    -        <artifactId>datanucleus-rdbms</artifactId>
    -        <version>${datanucleus-rdbms.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.datanucleus</groupId>
    -        <artifactId>javax.jdo</artifactId>
    -        <version>${datanucleus-jdo.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>com.tdunning</groupId>
    -        <artifactId>json</artifactId>
    -        <version>${json.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.mockito</groupId>
    -        <artifactId>mockito-all</artifactId>
    -        <version>${mockito-all.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.slf4j</groupId>
    -        <artifactId>slf4j-api</artifactId>
    -        <version>${slf4j.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>xerces</groupId>
    -        <artifactId>xercesImpl</artifactId>
    -        <version>${xerces.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-client</artifactId>
    -        <version>${hadoop.version}</version>
    -         <exclusions>
    -           <exclusion>
    -            <groupId>commmons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -         </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-auth</artifactId>
    -        <version>${hadoop.version}</version>
    -         <exclusions>
    -           <exclusion>
    -            <groupId>commmons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -         </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-common</artifactId>
    -        <version>${hadoop.version}</version>
    -        <exclusions>
    -           <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>jetty-sslengine</artifactId>
    -          </exclusion>
    -         <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commmons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.httpcomponents</groupId>
    -            <artifactId>httpcore</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.apache.httpcomponents</groupId>
    -            <artifactId>httpclient</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-hdfs</artifactId>
    -        <version>${hadoop.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
    -        <version>${hadoop.version}</version>
    -          <exclusions>
    -            <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commmons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.codahale.metrics</groupId>
    -            <artifactId>metrics-core</artifactId>
    -          </exclusion>
    -        </exclusions>
    -     </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-mapreduce-client-common</artifactId>
    -        <version>${hadoop.version}</version>
    -         <exclusions>
    -            <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commmons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -        </exclusions>
    -     </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-mapreduce-client-core</artifactId>
    -        <version>${hadoop.version}</version>
    -         <exclusions>
    -            <exclusion>
    -            <groupId>org.slf4j</groupId>
    -            <artifactId>slf4j-log4j12</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>commmons-logging</groupId>
    -            <artifactId>commons-logging</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-minikdc</artifactId>
    -        <version>${hadoop.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-api</artifactId>
    -        <version>${hadoop.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-client</artifactId>
    -        <version>${hadoop.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-common</artifactId>
    -        <version>${hadoop.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-registry</artifactId>
    -        <version>${hadoop.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-server-web-common</artifactId>
    -        <version>${hadoop.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-yarn-server-web-proxy</artifactId>
    -        <version>${hadoop.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hbase</groupId>
    -        <artifactId>hbase-common</artifactId>
    -        <version>${hbase.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hbase</groupId>
    -        <artifactId>hbase-hadoop-compat</artifactId>
    -        <version>${hbase.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hbase</groupId>
    -        <artifactId>hbase-hadoop2-compat</artifactId>
    -        <version>${hbase.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>javax.servlet</groupId>
    -            <artifactId>servlet-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>javax.servlet.jsp</groupId>
    -            <artifactId>jsp-api</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.jruby</groupId>
    -            <artifactId>jruby-complete</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.jboss.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>io.netty</groupId>
    -            <artifactId>netty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>jsp-2.1</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>jsp-api-2.1</artifactId>
    -         </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>servlet-api-2.5</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>servlet-api-2.5</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey</groupId>
    -            <artifactId>jersey-core</artifactId>
    -         </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey</groupId>
    -            <artifactId>jersey-json</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.sun.jersey</groupId>
    -            <artifactId>jersey-server</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>jetty</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>org.mortbay.jetty</groupId>
    -            <artifactId>jetty-util</artifactId>
    -          </exclusion>
    -          <exclusion>
    -            <groupId>com.codahale.metrics</groupId>
    -            <artifactId>metrics-core</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hbase</groupId>
    -        <artifactId>hbase-server</artifactId>
    -        <version>${hbase.version}</version>
    -	<exclusions>
    -            <exclusion>
    -        	<groupId>org.glassfish</groupId>
    -		<artifactId>javax.el</artifactId>
    -            </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hbase</groupId>
    -        <artifactId>hbase-mapreduce</artifactId>
    -        <version>${hbase.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.hadoop</groupId>
    -        <artifactId>hadoop-minicluster</artifactId>
    -        <version>${hadoop.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.scala-lang</groupId>
    -        <artifactId>scala-library</artifactId>
    -        <version>${scala.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.apache.spark</groupId>
    -        <artifactId>spark-core_${scala.binary.version}</artifactId>
    -        <version>${spark.version}</version>
    -        <exclusions>
    -          <exclusion>
    -            <groupId>org.apache.hadoop</groupId>
    -            <artifactId>hadoop-core</artifactId>
    -          </exclusion>
    -        </exclusions>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.jamon</groupId>
    -        <artifactId>jamon-runtime</artifactId>
    -        <version>${jamon-runtime.version}</version>
    -      </dependency>
    -      <dependency>
    -        <groupId>org.xerial.snappy</groupId>
    -        <artifactId>snappy-java</artifactId>
    -        <version>${snappy.version}</version>
    -      </dependency>
    -    </dependencies>
    -  </dependencyManagement>
    -
    -  <dependencies>
    -    <!-- dependencies are always listed in sorted order by groupId, artifectId -->
    -    <!-- global dependencies -->
    -    <dependency>
    -      <groupId>org.slf4j</groupId>
    -      <artifactId>slf4j-api</artifactId>
    -      <version>${slf4j.version}</version>
    -    </dependency>
    -    <dependency>
    -      <groupId>org.apache.hive</groupId>
    -      <artifactId>hive-upgrade-acid</artifactId>
    -      <version>${project.version}</version>
    -    </dependency>
    -    <dependency>
    -      <groupId>org.mockito</groupId>
    -      <artifactId>mockito-all</artifactId>
    -      <scope>test</scope>
    -    </dependency>
    -  </dependencies>
    -
    -  <build>
    -    <pluginManagement>
    -      <plugins>
    -        <!-- plugins are always listed in sorted order by groupId, artifectId -->
    -        <plugin>
    -          <groupId>org.antlr</groupId>
    -          <artifactId>antlr3-maven-plugin</artifactId>
    -          <version>${antlr.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-compiler-plugin</artifactId>
    -          <version>${maven.compiler.plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-antrun-plugin</artifactId>
    -          <version>${maven.antrun.plugin.version}</version>
    -          <dependencies>
    -            <dependency>
    -              <groupId>ant-contrib</groupId>
    -              <artifactId>ant-contrib</artifactId>
    -              <version>${ant.contrib.version}</version>
    -              <exclusions>
    -                <exclusion>
    -                  <groupId>ant</groupId>
    -                  <artifactId>ant</artifactId>
    -                </exclusion>
    -              </exclusions>
    -            </dependency>
    -          </dependencies>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-assembly-plugin</artifactId>
    -          <version>${maven.assembly.plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-eclipse-plugin</artifactId>
    -          <version>${maven.eclipse.plugin.version}</version>
    -          <configuration>
    -            <downloadJavadocs>true</downloadJavadocs>
    -            <downloadSources>true</downloadSources>
    -            <buildOutputDirectory>target/eclipse/classes</buildOutputDirectory>
    -            <workspaceActiveCodeStyleProfileName>Hive</workspaceActiveCodeStyleProfileName>
    -            <workspaceCodeStylesURL>${basedir}/dev-support/eclipse-styles.xml</workspaceCodeStylesURL>
    -          </configuration>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-checkstyle-plugin</artifactId>
    -          <version>${maven.checkstyle.plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-enforcer-plugin</artifactId>
    -          <version>${maven.enforcer.plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-install-plugin</artifactId>
    -          <version>${maven.install.plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-shade-plugin</artifactId>
    -          <version>${maven.shade.plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-surefire-plugin</artifactId>
    -          <version>${maven.surefire.plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-jar-plugin</artifactId>
    -          <version>${maven.jar.plugin.version}</version>
    -          <configuration>
    -            <archive>
    -              <manifest>
    -                <addDefaultImplementationEntries>true</addDefaultImplementationEntries>
    -              </manifest>
    -            </archive>
    -          </configuration>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-war-plugin</artifactId>
    -          <version>${maven.war.plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.datanucleus</groupId>
    -          <artifactId>datanucleus-maven-plugin</artifactId>
    -          <version>${datanucleus.maven.plugin.version}</version>
    -          <dependencies>
    -            <dependency>
    -              <groupId>org.datanucleus</groupId>
    -              <artifactId>datanucleus-core</artifactId>
    -              <version>${datanucleus-core.version}</version>
    -            </dependency>
    -          </dependencies>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.felix</groupId>
    -          <artifactId>maven-bundle-plugin</artifactId>
    -          <version>${felix.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.apache.maven.plugins</groupId>
    -          <artifactId>maven-dependency-plugin</artifactId>
    -          <version>${maven.dependency.plugin.version}</version>
    -        </plugin>
    -        <plugin>
    -          <groupId>org.codehaus.mojo</groupId>
    -          <artifactId>build-helper-maven-plugin</artifactId>
    -          <version>${maven.build-helper.plugin.version}</version>
    -        </plugin>
    -      </plugins>
    -    </pluginManagement>
    -
    -    <plugins>
    -      <!-- plugins are always listed in sorted order by groupId, artifectId -->
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-antrun-plugin</artifactId>
    -        <executions>
    -          <execution>
    -            <id>define-classpath</id>
    -            <phase>process-resources</phase>
    -            <goals>
    -              <goal>run</goal>
    -            </goals>
    -            <configuration>
    -              <exportAntProperties>true</exportAntProperties>
    -              <target>
    -                <property name="maven.test.classpath" refid="maven.test.classpath"/>
    -              </target>
    -            </configuration>
    -          </execution>
    -          <execution>
    -            <id>setup-test-dirs</id>
    -            <phase>process-test-resources</phase>
    -            <goals>
    -              <goal>run</goal>
    -            </goals>
    -            <configuration>
    -              <target>
    -                <delete dir="${test.tmp.dir}" />
    -                <delete dir="${test.conf.dir}" />
    -                <delete dir="${test.warehouse.dir}" />
    -                <mkdir dir="${test.tmp.dir}" />
    -                <mkdir dir="${test.warehouse.dir}" />
    -                <mkdir dir="${test.conf.dir}" />
    -                <!-- copies hive-site.xml so it can be modified -->
    -                <copy todir="${test.conf.dir}">
    -                  <fileset dir="${basedir}/${hive.path.to.root}/data/conf/"/>
    -                </copy>
    -              </target>
    -            </configuration>
    -          </execution>
    -        </executions>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-clean-plugin</artifactId>
    -        <version>2.5</version>
    -        <configuration>
    -          <filesets>
    -            <fileset>
    -              <directory>./</directory>
    -              <includes>
    -                <include>datanucleus.log</include>
    -                <include>derby.log</include>
    -              </includes>
    -              <followSymlinks>false</followSymlinks>
    -            </fileset>
    -            <fileset>
    -              <directory>build</directory>
    -              <followSymlinks>false</followSymlinks>
    -            </fileset>
    -          </filesets>
    -        </configuration>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-checkstyle-plugin</artifactId>
    -        <configuration>
    -          <configLocation>${checkstyle.conf.dir}/checkstyle.xml</configLocation>
    -          <propertyExpansion>config_loc=${checkstyle.conf.dir}</propertyExpansion>
    -          <includeTestSourceDirectory>true</includeTestSourceDirectory>
    -        </configuration>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-enforcer-plugin</artifactId>
    -        <executions>
    -          <execution>
    -            <id>enforce-no-snapshots</id>
    -            <goals>
    -              <goal>enforce</goal>
    -            </goals>
    -            <configuration>
    -              <rules>
    -                <requireReleaseDeps>
    -                  <message>Release builds are not allowed to have SNAPSHOT depenendencies</message>
    -                  <searchTransitive>true</searchTransitive>
    -                  <onlyWhenRelease>true</onlyWhenRelease>
    -                </requireReleaseDeps>
    -              </rules>
    -              <fail>true</fail>
    -            </configuration>
    -          </execution>
    -          <execution>
    -            <id>enforce-banned-dependencies</id>
    -            <goals>
    -              <goal>enforce</goal>
    -            </goals>
    -            <configuration>
    -              <rules>
    -                <bannedDependencies>
    -                  <excludes>
    -                    <!--LGPL licenced library-->
    -                    <exclude>com.google.code.findbugs:annotations</exclude>
    -                  </excludes>
    -                </bannedDependencies>
    -              </rules>
    -              <fail>true</fail>
    -            </configuration>
    -          </execution>
    -        </executions>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.apache.maven.plugins</groupId>
    -        <artifactId>maven-surefire-plugin</artifactId>
    -        <configuration>
    -          <excludes>
    -            <exclude>**/TestSerDe.java</exclude>
    -            <exclude>**/TestHiveMetaStore.java</exclude>
    -            <exclude>**/ql/exec/vector/util/*.java</exclude>
    -            <exclude>**/ql/exec/vector/udf/legacy/*.java</exclude>
    -            <exclude>**/ql/exec/vector/udf/generic/*.java</exclude>
    -            <exclude>**/TestHiveServer2Concurrency.java</exclude>
    -            <exclude>${test.excludes.additional}</exclude>
    -            <exclude>${skip.spark.files}</exclude>
    -          </excludes>
    -          <redirectTestOutputToFile>true</redirectTestOutputToFile>
    -          <reuseForks>false</reuseForks>
    -          <failIfNoTests>false</failIfNoTests>
    -          <argLine>${maven.test.jvm.args}</argLine>
    -          <trimStackTrace>false</trimStackTrace>
    -          <additionalClasspathElements>
    -            <additionalClasspathElement>${test.conf.dir}</additionalClasspathElement>
    -            <additionalClasspathElement>${basedir}/${hive.path.to.root}/conf</additionalClasspathElement>
    -          </additionalClasspathElements>
    -          <environmentVariables>
    -            <TZ>US/Pacific</TZ>
    -            <LANG>en_US.UTF-8</LANG>
    -            <HADOOP_CLASSPATH>${test.conf.dir}:${basedir}/${hive.path.to.root}/conf</HADOOP_CLASSPATH>
    -            <HIVE_HADOOP_TEST_CLASSPATH>${test.hive.hadoop.classpath}</HIVE_HADOOP_TEST_CLASSPATH>
    -            <SPARK_SUBMIT_CLASSPATH>${spark.home}/lib/spark-assembly-${spark.version}-hadoop2.4.0.jar:${test.hive.hadoop.classpath}</SPARK_SUBMIT_CLASSPATH>
    -            <SPARK_OSX_TEST_OPTS>-Dorg.xerial.snappy.tempdir=/tmp -Dorg.xerial.snappy.lib.name=libsnappyjava.jnilib</SPARK_OSX_TEST_OPTS>
    -            <SPARK_SCALA_VERSION>2.11</SPARK_SCALA_VERSION>
    -            <SPARK_HOME>${spark.home}</SPARK_HOME>
    -            <PATH>${env.PATH}${test.extra.path}</PATH>
    -          </environmentVariables>
    -          <systemPropertyVariables>
    -            <build.dir>${project.build.directory}</build.dir>
    -            <!-- required by zk test ClientBase -->
    -            <build.test.dir>${test.tmp.dir}</build.test.dir>
    -            <!-- required by a few tests to find the derby jar -->
    -            <derby.version>${derby.version}</derby.version>
    -            <derby.stream.error.file>${test.tmp.dir}/derby.log</derby.stream.error.file>
    -            <hadoop.bin.path>${hadoop.bin.path}</hadoop.bin.path>
    -            <!-- required by Hadoop's JobHistory -->
    -            <hadoop.log.dir>${test.tmp.dir}</hadoop.log.dir>
    -            <hive.root>${basedir}/${hive.path.to.root}/</hive.root>
    -            <hive.version>${project.version}</hive.version>
    -            <!-- required for hive-exec jar path and tests which reference a jar -->
    -            <maven.local.repository>${maven.repo.local}</maven.local.repository>
    -            <mapred.job.tracker>local</mapred.job.tracker>
    -            <log4j.configurationFile>${test.log4j.scheme}${test.conf.dir}/hive-log4j2.properties</log4j.configurationFile>
    -            <hive.test.console.log.level>${test.console.log.level}</hive.test.console.log.level>
    -            <log4j.debug>true</log4j.debug>
    -            <!-- don't diry up /tmp -->
    -            <java.io.tmpdir>${test.tmp.dir}</java.io.tmpdir>
    -            <spark.home>${spark.home}</spark.home>
    -            <!-- Hadoop's minidfs class uses this -->
    -            <test.build.data>${test.tmp.dir}</test.build.data>
    -            <!-- required by QTestUtil -->
    -            <test.data.files>${basedir}/${hive.path.to.root}/data/files</test.data.files>
    -            <test.data.dir>${basedir}/${hive.path.to.root}/data/files</test.data.dir>
    -            <test.tmp.dir>${test.tmp.dir}</test.tmp.dir>
    -            <test.tmp.dir.uri>${test.tmp.dir.uri}</test.tmp.dir.uri>
    -            <test.dfs.mkdir>${test.dfs.mkdir}</test.dfs.mkdir>
    -            <test.output.overwrite>${test.output.overwrite}</test.output.overwrite>
    -            <test.warehouse.dir>${test.warehouse.scheme}${test.warehouse.dir}</test.warehouse.dir>
    -            <java.net.preferIPv4Stack>true</java.net.preferIPv4Stack>
    -            <!-- EnforceReadOnlyTables hook and QTestUtil -->
    -            <test.src.tables>src,src1,srcbucket,srcbucket2,src_json,src_thrift,src_sequencefile,srcpart,alltypesorc,alltypesparquet,src_hbase,cbo_t1,cbo_t2,cbo_t3,src_cbo,part,lineitem</test.src.tables>
    -            <java.security.krb5.conf>${test.conf.dir}/krb5.conf</java.security.krb5.conf>
    -            <!-- Required by spark to work around SPARK-14958 -->
    -            <antlr.version>${antlr.version}</antlr.version>
    -            <qfile>${qfile}</qfile>
    -            <initScript>${initScript}</initScript>
    -            <clustermode>${clustermode}</clustermode>
    -            <qfile_regex>${qfile_regex}</qfile_regex>
    -            <run_disabled>${run_disabled}</run_disabled>
    -          </systemPropertyVariables>
    -        </configuration>
    -      </plugin>
    -      <plugin>
    -	<groupId>org.apache.rat</groupId>
    -	<artifactId>apache-rat-plugin</artifactId>
    -	<version>0.10</version>
    -	<configuration>
    -	  <excludes>
    -	    <exclude>binary-package-licenses/**</exclude>
    -	    <exclude>data/**</exclude>
    -	    <exclude>conf/**</exclude>
    -	    <exclude>checkstyle/**</exclude>
    -	    <exclude>bin/**</exclude>
    -	    <exclude>itests/**</exclude>
    -            <exclude>**/README.md</exclude>
    -            <exclude>**/*.iml</exclude>
    -	    <exclude>**/*.txt</exclude>
    -	    <exclude>**/*.log</exclude>
    -	    <exclude>**/*.arcconfig</exclude>
    -	    <exclude>**/package-info.java</exclude>
    -	    <exclude>**/*.properties</exclude>
    -	    <exclude>**/*.q</exclude>
    -	    <exclude>**/*.q.out</exclude>
    -	    <exclude>**/*.q.out_*</exclude>
    -	    <exclude>**/*.xml</exclude>
    -	    <exclude>**/gen/**</exclude>
    -	    <exclude>**/scripts/**</exclude>
    -	    <exclude>**/resources/**</exclude>
    -	    <exclude>**/*.rc</exclude>
    -	    <exclude>**/*.rcfile</exclude>
    -	    <exclude>**/*.qv</exclude>
    -	    <exclude>**/*.out</exclude>
    -	    <exclude>**/RecordTestObj.java</exclude>
    -	    <exclude>**/*.m</exclude>
    -	    <exclude>**/gen-java/**</exclude>
    -	    <exclude>**/testdata/**</exclude>
    -	    <exclude>**/ptest2/*.md</exclude>
    -	    <exclude>**/test/org/apache/hadoop/hive/hbase/avro/**</exclude>
    -	    <exclude>**/avro_test.avpr</exclude>
    -	    <exclude>**/xmlReport.pl</exclude>
    -	    <exclude>**/*.html</exclude>
    -	    <exclude>**/sit</exclude>
    -      <exclude>**/test/queries/**/*.sql</exclude>
    -        <exclude>**/patchprocess/**</exclude>
    -        <exclude>**/metastore_db/**</exclude>
    -	  </excludes>
    -	</configuration>
    -      </plugin>
    -      <plugin>
    -        <groupId>org.jamon</groupId>
    -        <artifactId>jamon-maven-plugin</artifactId>
    -        <version>${jamon.plugin.version}</version>
    -      </plugin>
    -    </plugins>
    -  </build>
    -
    -  <profiles>
    -    <profile>
    -      <id>thriftif</id>
    -      <build>
    -        <plugins>
    -          <plugin>
    -            <groupId>org.apache.maven.plugins</groupId>
    -            <artifactId>maven-antrun-plugin</artifactId>
    -            <executions>
    -              <execution>
    -                <id>generate-thrift-sources</id>
    -                <phase>generate-sources</phase>
    -                <configuration>
    -                  <target>
    -                    <taskdef name="for" classname="net.sf.antcontrib.logic.ForTask"
    -                      classpathref="maven.plugin.classpath" />
    -                    <property name="thrift.args" value="-I ${thrift.home} --gen java:beans,hashcode,generated_annotations=undated --gen cpp --gen php --gen py --gen rb"/>
    -                    <property name="thrift.gen.dir" value="${basedir}/src/gen/thrift"/>
    -                    <delete dir="${thrift.gen.dir}"/>
    -                    <mkdir dir="${thrift.gen.dir}"/>
    -                    <for param="thrift.file">
    -                      <path>
    -                        <fileset dir="." includes="if/*.thrift,if/test/*.thrift,src/main/thrift/*.thrift" />
    -                      </path>
    -                      <sequential>
    -                        <echo message="Generating Thrift code for @{thrift.file}"/>
    -                        <exec executable="${thrift.home}/bin/thrift"  failonerror="true" dir=".">
    -                          <arg line="${thrift.args} -I ${basedir}/include -I ${basedir}/.. -o ${thrift.gen.dir} @{thrift.file} " />
    -                        </exec>
    -                      </sequential>
    -                    </for>
    -                  </target>
    -                </configuration>
    -                <goals>
    -                  <goal>run</goal>
    -                </goals>
    -              </execution>
    -            </executions>
    -          </plugin>
    -          <plugin>
    -            <groupId>org.apache.maven.plugins</groupId>
    -            <artifactId>maven-enforcer-plugin</artifactId>
    -            <executions>
    -              <execution>
    -                <id>enforce-property</id>
    -                <goals>
    -                  <goal>enforce</goal>
    -                </goals>
    -                <configuration>
    -                  <rules>
    -                    <requireProperty>
    -                      <property>thrift.home</property>
    -                    </requireProperty>
    -                  </rules>
    -                  <fail>true</fail>
    -                </configuration>
    -              </execution>
    -            </executions>
    -          </plugin>
    -        </plugins>
    -      </build>
    -    </profile>
    -
    -    <profile>
    -      <id>sources</id>
    -      <build>
    -        <plugins>
    -          <plugin>
    -            <groupId>org.apache.maven.plugins</groupId>
    -            <artifactId>maven-source-plugin</artifactId>
    -            <executions>
    -              <execution>
    -                <id>attach-sources</id>
    -                <goals>
    -                  <goal>jar</goal>
    -                </goals>
    -              </execution>
    -            </executions>
    -          </plugin>
    -        </plugins>
    -      </build>
    -    </profile>
    -
    -    <profile>
    -      <id>javadoc</id>
    -      <build>
    -        <plugins>
    -          <plugin>
    -            <groupId>org.apache.maven.plugins</groupId>
    -            <artifactId>maven-javadoc-plugin</artifactId>
    -            <configuration>
    -              <failOnError>false</failOnError>
    -            </configuration>
    -            <executions>
    -              <execution>
    -                <id>attach-javadocs</id>
    -                <goals>
    -                  <goal>jar</goal>
    -                </goals>
    -              </execution>
    -            </executions>
    -          </plugin>
    -        </plugins>
    -      </build>
    -    </profile>
    -
    -    <profile>
    -    <id>findbugs</id>
    -      <build>
    -        <plugins>
    -          <plugin>
    -            <groupId>org.codehaus.mojo</groupId>
    -            <artifactId>findbugs-maven-plugin</artifactId>
    -            <version>3.0.0</version>
    -            <configuration>
    -              <fork>true</fork>
    -              <maxHeap>2048</maxHeap>
    -              <jvmArgs>-Djava.awt.headless=true -Xmx2048m -Xms512m</jvmArgs>
    -              <excludeFilterFile>${basedir}/${hive.path.to.root}/findbugs/findbugs-exclude.xml</excludeFilterFile>
    -            </configuration>
    -          </plugin>
    -        </plugins>
    -      </build>
    -      <reporting>
    -        <plugins>
    -          <plugin>
    -            <groupId>org.codehaus.mojo</groupId>
    -            <artifactId>findbugs-maven-plugin</artifactId>
    -            <version>3.0.0</version>
    -            <configuration>
    -              <fork>true</fork>
    -              <maxHeap>2048</maxHeap>
    -              <jvmArgs>-Djava.awt.headless=true -Xmx2048m -Xms512m</jvmArgs>
    -              <excludeFilterFile>${basedir}/${hive.path.to.root}/findbugs/findbugs-exclude.xml</excludeFilterFile>
    -            </configuration>
    -          </plugin>
    -        </plugins>
    -      </reporting>
    -    </profile>
    -    <profile>
    -      <!-- Windows-specific settings to allow unit tests to work -->
    -      <id>windows-test</id>
    -      <activation>
    -        <os>
    -          <family>Windows</family>
    -       </os>
    -      </activation>
    -      <build>
    -        <plugins>
    -          <!-- maven.test.classpath (used for HIVE_HADOOP_TEST_CLASSPATH) exceeds the 8K Windows -->
    -          <!-- command shell limit which causes tests which call hadoop command to fail.         -->
    -          <!-- Workaround is to copy all necessary jars to a single location to shorten the      -->
    -          <!-- the length of the environment variable.                                           -->
    -          <plugin>
    -            <groupId>org.apache.maven.plugins</groupId>
    -            <artifactId>maven-dependency-plugin</artifactId>
    -            <version>2.8</version>
    -            <executions>
    -              <execution>
    -                <id>copy-dependencies</id>
    -                <phase>package</phase>
    -                <goals>
    -                  <goal>copy-dependencies</goal>
    -                </goals>
    -                <configuration>
    -                  <outputDirectory>${project.build.directory}/deplibs/</outputDirectory>
    -                  <overWriteReleases>false</overWriteReleases>
    -                  <overWriteSnapshots>false</overWriteSnapshots>
    -                  <overWriteIfNewer>true</overWriteIfNewer>
    -                </configuration>
    -              </execution>
    -            </executions>
    -          </plugin>
    -        </plugins>
    -      </build>
    -      <properties>
    -        <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop.cmd</hadoop.bin.path>
    -        <test.extra.path>;${env.HADOOP_HOME}/bin</test.extra.path>
    -        <test.hive.hadoop.classpath>${project.build.directory}/deplibs/*</test.hive.hadoop.classpath>
    -        <test.tmp.dir.uri>file:///${test.tmp.dir}</test.tmp.dir.uri>
    -        <test.log4j.scheme>file:/</test.log4j.scheme>
    -      </properties>
    -    </profile>
    -    <profile>
    -      <id>spark-test</id>
    -      <activation>
    -	<property>
    -          <name>!skipSparkTests</name>
    -	</property>
    -      </activation>
    -      <properties>
    -        <skip.spark.files>
    -          **/ql/exec/spark/session/TestSparkSessionManagerImpl.java,**/TestMultiSessionsHS2WithLocalClusterSpark.java,**/TestJdbcWithLocalClusterSpark.java
    -        </skip.spark.files>
    -      </properties>
    -    </profile>
    -    <profile>
    -      <id>itests</id>
    -      <modules>
    -        <module>itests</module>
    -      </modules>
    -    </profile>
    -  </profiles>
    +<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    +         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    +    <modelVersion>4.0.0</modelVersion>
    +    <parent>
    +        <groupId>org.apache</groupId>
    +        <artifactId>apache</artifactId>
    +        <version>18</version>
    +    </parent>
    +    <groupId>org.apache.hive</groupId>
    +    <artifactId>hive</artifactId>
    +    <version>3.1.3</version>
    +    <packaging>pom</packaging>
    +    
    +    <name>Hive</name>
    +    <url>https://hive.apache.org</url>
    +    <prerequisites>
    +        <maven>2.2.1</maven>
    +    </prerequisites>
    +    
    +    <modules>
    +        <module>accumulo-handler</module>
    +        <module>vector-code-gen</module>
    +        <module>beeline</module>
    +        <module>classification</module>
    +        <module>cli</module>
    +        <module>common</module>
    +        <module>contrib</module>
    +        <module>druid-handler</module>
    +        <module>hbase-handler</module>
    +        <module>jdbc-handler</module>
    +        <module>hcatalog</module>
    +        <module>hplsql</module>
    +        <module>jdbc</module>
    +        <module>metastore</module>
    +        <module>ql</module>
    +        <module>serde</module>
    +        <module>service-rpc</module>
    +        <module>service</module>
    +        <module>streaming</module>
    +        <module>llap-common</module>
    +        <module>llap-client</module>
    +        <module>llap-ext-client</module>
    +        <module>llap-tez</module>
    +        <module>llap-server</module>
    +        <module>shims</module>
    +        <module>spark-client</module>
    +        <module>kryo-registrator</module>
    +        <module>testutils</module>
    +        <module>packaging</module>
    +        <module>standalone-metastore</module>
    +        <module>upgrade-acid</module>
    +    </modules>
    +    
    +    <properties>
    +        <hive.version.shortname>3.1.0</hive.version.shortname>
    +        
    +        <!-- Build Properties -->
    +        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    +        <maven.compiler.source>1.8</maven.compiler.source>
    +        <maven.compiler.target>1.8</maven.compiler.target>
    +        <maven.compiler.useIncrementalCompilation>false</maven.compiler.useIncrementalCompilation>
    +        <maven.repo.local>${settings.localRepository}</maven.repo.local>
    +        <hive.path.to.root>.</hive.path.to.root>
    +        <hive.jdbc.driver.classifier>standalone</hive.jdbc.driver.classifier>
    +        <checkstyle.conf.dir>${basedir}/${hive.path.to.root}/checkstyle</checkstyle.conf.dir>
    +        
    +        <!-- Test Properties -->
    +        <test.extra.path></test.extra.path>
    +        <test.hive.hadoop.classpath>${maven.test.classpath}</test.hive.hadoop.classpath>
    +        <test.log4j.scheme>file://</test.log4j.scheme>
    +        <test.tmp.dir>${project.build.directory}/tmp</test.tmp.dir>
    +        <test.conf.dir>${project.build.directory}/testconf</test.conf.dir>
    +        <test.tmp.dir.uri>file://${test.tmp.dir}</test.tmp.dir.uri>
    +        <!-- Determines the log level of the console logger, hive.log is independent of this-->
    +        <test.console.log.level>INFO</test.console.log.level>
    +        <test.warehouse.dir>${project.build.directory}/warehouse</test.warehouse.dir>
    +        <test.warehouse.scheme>pfile://</test.warehouse.scheme>
    +        
    +        <!-- To add additional exclude patterns set this property -->
    +        <test.excludes.additional></test.excludes.additional>
    +        <skip.spark.files></skip.spark.files>
    +        
    +        <!-- Plugin and Plugin Dependency Versions -->
    +        <ant.contrib.version>1.0b3</ant.contrib.version>
    +        <datanucleus.maven.plugin.version>3.3.0-release</datanucleus.maven.plugin.version>
    +        <maven.test.jvm.args>-Xmx2048m</maven.test.jvm.args>
    +        <maven.antrun.plugin.version>1.7</maven.antrun.plugin.version>
    +        <maven.assembly.plugin.version>2.3</maven.assembly.plugin.version>
    +        <maven.checkstyle.plugin.version>2.17</maven.checkstyle.plugin.version>
    +        <maven.compiler.plugin.version>3.6.1</maven.compiler.plugin.version>
    +        <maven.enforcer.plugin.version>1.3.1</maven.enforcer.plugin.version>
    +        <maven.install.plugin.version>2.4</maven.install.plugin.version>
    +        <maven.jar.plugin.version>2.4</maven.jar.plugin.version>
    +        <maven.javadoc.plugin.version>2.4</maven.javadoc.plugin.version>
    +        <maven.shade.plugin.version>3.1.0</maven.shade.plugin.version>
    +        <maven.surefire.plugin.version>2.21.0</maven.surefire.plugin.version>
    +        <maven.war.plugin.version>2.4</maven.war.plugin.version>
    +        <maven.dependency.plugin.version>2.8</maven.dependency.plugin.version>
    +        <maven.eclipse.plugin.version>2.9</maven.eclipse.plugin.version>
    +        <maven.build-helper.plugin.version>1.8</maven.build-helper.plugin.version>
    +        
    +        <!-- Library Dependency Versions -->
    +        <accumulo.version>1.7.3</accumulo.version>
    +        <activemq.version>5.5.0</activemq.version>
    +        <ant.version>1.9.1</ant.version>
    +        <antlr.version>3.5.2</antlr.version>
    +        <apache-directory-server.version>1.5.6</apache-directory-server.version>
    +        <apache-directory-clientapi.version>0.1</apache-directory-clientapi.version>
    +        <!-- Include arrow for LlapOutputFormatService -->
    +        <arrow.version>0.8.0</arrow.version>
    +        <avatica.version>1.11.0</avatica.version>
    +        <avro.version>1.8.2</avro.version>
    +        <bonecp.version>0.8.0.RELEASE</bonecp.version>
    +        <calcite.version>1.16.0</calcite.version>
    +        <datanucleus-api-jdo.version>4.2.4</datanucleus-api-jdo.version>
    +        <datanucleus-core.version>4.1.17</datanucleus-core.version>
    +        <datanucleus-rdbms.version>4.1.19</datanucleus-rdbms.version>
    +        <datanucleus-jdo.version>3.2.0-m3</datanucleus-jdo.version>
    +        <commons-cli.version>1.2</commons-cli.version>
    +        <commons-codec.version>1.15</commons-codec.version>
    +        <commons-collections.version>3.2.2</commons-collections.version>
    +        <commons-compress.version>1.19</commons-compress.version>
    +        <commons-exec.version>1.1</commons-exec.version>
    +        <commons-io.version>2.6</commons-io.version>
    +        <commons-lang.version>2.6</commons-lang.version>
    +        <commons-lang3.version>3.9</commons-lang3.version>
    +        <commons-pool.version>1.5.4</commons-pool.version>
    +        <commons-dbcp.version>1.4</commons-dbcp.version>
    +        <derby.version>10.14.1.0</derby.version>
    +        <dropwizard.version>3.1.0</dropwizard.version>
    +        <dropwizard-metrics-hadoop-metrics2-reporter.version>0.1.2</dropwizard-metrics-hadoop-metrics2-reporter.version>
    +        <druid.version>0.12.0</druid.version>
    +        <flatbuffers.version>1.2.0-3f79e055</flatbuffers.version>
    +        <guava.version>27.0-jre</guava.version>
    +        <groovy.version>2.4.11</groovy.version>
    +        <h2database.version>1.3.166</h2database.version>
    +        <hadoop.version>3.1.0</hadoop.version>
    +        <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop</hadoop.bin.path>
    +        <hamcrest.version>1.3</hamcrest.version>
    +        <hbase.version>2.0.0-alpha4</hbase.version>
    +        <hppc.version>0.7.2</hppc.version>
    +        <!-- required for logging test to avoid including hbase which pulls disruptor transitively -->
    +        <disruptor.version>3.3.7</disruptor.version>
    +        <hikaricp.version>2.6.1</hikaricp.version>
    +        <!-- httpcomponents are not always in version sync -->
    +        <httpcomponents.client.version>4.5.13</httpcomponents.client.version>
    +        <httpcomponents.core.version>4.4.13</httpcomponents.core.version>
    +        <ivy.version>2.4.0</ivy.version>
    +        <jackson.version>2.12.0</jackson.version>
    +        <jamon.plugin.version>2.3.4</jamon.plugin.version>
    +        <jamon-runtime.version>2.3.1</jamon-runtime.version>
    +        <javaewah.version>0.3.2</javaewah.version>
    +        <javax-servlet.version>3.1.0</javax-servlet.version>
    +        <javax-servlet-jsp.version>2.3.1</javax-servlet-jsp.version>
    +        <javolution.version>5.5.1</javolution.version>
    +        <jdo-api.version>3.0.1</jdo-api.version>
    +        <jettison.version>1.1</jettison.version>
    +        <jetty.version>9.3.20.v20170531</jetty.version>
    +        <jersey.version>1.19</jersey.version>
    +        <!-- Glassfish jersey is included for Spark client test only -->
    +        <glassfish.jersey.version>2.22.2</glassfish.jersey.version>
    +        <jline.version>2.12</jline.version>
    +        <jms.version>2.0.2</jms.version>
    +        <joda.version>2.9.9</joda.version>
    +        <jodd.version>3.5.2</jodd.version>
    +        <json.version>1.8</json.version>
    +        <junit.version>4.11</junit.version>
    +        <kryo.version>3.0.3</kryo.version>
    +        <libfb303.version>0.9.3</libfb303.version>
    +        <libthrift.version>0.9.3</libthrift.version>
    +        <log4j2.version>2.17.1</log4j2.version>
    +        <opencsv.version>2.3</opencsv.version>
    +        <orc.version>1.5.8</orc.version>
    +        <mockito-all.version>1.10.19</mockito-all.version>
    +        <mina.version>2.0.0-M5</mina.version>
    +        <netty.version>4.1.17.Final</netty.version>
    +        <netty3.version>3.10.5.Final</netty3.version>
    +        <parquet.version>1.10.0</parquet.version>
    +        <pig.version>0.16.0</pig.version>
    +        <plexus.version>1.5.6</plexus.version>
    +        <protobuf.version>2.5.0</protobuf.version>
    +        <stax.version>1.0.1</stax.version>
    +        <slf4j.version>1.7.10</slf4j.version>
    +        <ST4.version>4.0.4</ST4.version>
    +        <storage-api.version>2.7.0</storage-api.version>
    +        <tez.version>0.9.1</tez.version>
    +        <super-csv.version>2.2.0</super-csv.version>
    +        <spark.version>3.2.3</spark.version>
    +        <scala.binary.version>2.12</scala.binary.version>
    +        <scala.version>2.12.15</scala.version>
    +        <tempus-fugit.version>1.1</tempus-fugit.version>
    +        <snappy.version>1.1.4</snappy.version>
    +        <wadl-resourcedoc-doclet.version>1.4</wadl-resourcedoc-doclet.version>
    +        <velocity.version>2.3</velocity.version>
    +        <xerces.version>2.9.1</xerces.version>
    +        <zookeeper.version>3.4.6</zookeeper.version>
    +        <jpam.version>1.1</jpam.version>
    +        <felix.version>2.4.0</felix.version>
    +        <curator.version>2.12.0</curator.version>
    +        <jsr305.version>3.0.0</jsr305.version>
    +        <tephra.version>0.6.0</tephra.version>
    +        <gson.version>2.2.4</gson.version>
    +    </properties>
    +    
    +    <repositories>
    +        <repository>
    +            <id>central</id>
    +            <name>central</name>
    +            <url>https://repo.maven.apache.org/maven2</url>
    +            <layout>default</layout>
    +            <releases>
    +                <enabled>true</enabled>
    +                <checksumPolicy>warn</checksumPolicy>
    +            </releases>
    +        </repository>
    +    </repositories>
    +    
    +    <dependencyManagement>
    +        <dependencies>
    +            <!-- dependencies are always listed in sorted order by groupId, artifectId -->
    +            <dependency>
    +                <groupId>com.esotericsoftware</groupId>
    +                <artifactId>kryo-shaded</artifactId>
    +                <version>${kryo.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.google.guava</groupId>
    +                <artifactId>guava</artifactId>
    +                <version>${guava.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.google.protobuf</groupId>
    +                <artifactId>protobuf-java</artifactId>
    +                <version>${protobuf.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.google.code.tempus-fugit</groupId>
    +                <artifactId>tempus-fugit</artifactId>
    +                <version>${tempus-fugit.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.hamcrest</groupId>
    +                        <artifactId>hamcrest-core</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.googlecode.javaewah</groupId>
    +                <artifactId>JavaEWAH</artifactId>
    +                <version>${javaewah.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.jolbox</groupId>
    +                <artifactId>bonecp</artifactId>
    +                <version>${bonecp.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.zaxxer</groupId>
    +                <artifactId>HikariCP</artifactId>
    +                <version>${hikaricp.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.parquet</groupId>
    +                <artifactId>parquet</artifactId>
    +                <version>${parquet.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.parquet</groupId>
    +                <artifactId>parquet-column</artifactId>
    +                <version>${parquet.version}</version>
    +                <classifier>tests</classifier>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.sun.jersey</groupId>
    +                <artifactId>jersey-core</artifactId>
    +                <version>${jersey.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.sun.jersey</groupId>
    +                <artifactId>jersey-json</artifactId>
    +                <version>${jersey.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.sun.jersey</groupId>
    +                <artifactId>jersey-server</artifactId>
    +                <version>${jersey.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.sun.jersey.contribs</groupId>
    +                <artifactId>wadl-resourcedoc-doclet</artifactId>
    +                <version>${wadl-resourcedoc-doclet.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.sun.jersey</groupId>
    +                <artifactId>jersey-servlet</artifactId>
    +                <version>${jersey.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-cli</groupId>
    +                <artifactId>commons-cli</artifactId>
    +                <version>${commons-cli.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-codec</groupId>
    +                <artifactId>commons-codec</artifactId>
    +                <version>${commons-codec.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-collections</groupId>
    +                <artifactId>commons-collections</artifactId>
    +                <version>${commons-collections.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-io</groupId>
    +                <artifactId>commons-io</artifactId>
    +                <version>${commons-io.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>commons-lang</groupId>
    +                <artifactId>commons-lang</artifactId>
    +                <version>${commons-lang.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>io.netty</groupId>
    +                <artifactId>netty-all</artifactId>
    +                <version>${netty.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>javax.jdo</groupId>
    +                <artifactId>jdo-api</artifactId>
    +                <version>${jdo-api.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>javax.jms</groupId>
    +                <artifactId>jms</artifactId>
    +                <version>${jms.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>javolution</groupId>
    +                <artifactId>javolution</artifactId>
    +                <version>${javolution.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>jline</groupId>
    +                <artifactId>jline</artifactId>
    +                <version>${jline.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>junit</groupId>
    +                <artifactId>junit</artifactId>
    +                <version>${junit.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.logging.log4j</groupId>
    +                <artifactId>log4j-1.2-api</artifactId>
    +                <version>${log4j2.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.logging.log4j</groupId>
    +                <artifactId>log4j-web</artifactId>
    +                <version>${log4j2.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.logging.log4j</groupId>
    +                <artifactId>log4j-slf4j-impl</artifactId>
    +                <version>${log4j2.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.antlr</groupId>
    +                <artifactId>antlr-runtime</artifactId>
    +                <version>${antlr.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.antlr</groupId>
    +                <artifactId>ST4</artifactId>
    +                <version>${ST4.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.commons</groupId>
    +                <artifactId>commons-compress</artifactId>
    +                <version>${commons-compress.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.commons</groupId>
    +                <artifactId>commons-exec</artifactId>
    +                <version>${commons-exec.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.accumulo</groupId>
    +                <artifactId>accumulo-core</artifactId>
    +                <version>${accumulo.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.accumulo</groupId>
    +                <artifactId>accumulo-fate</artifactId>
    +                <version>${accumulo.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.accumulo</groupId>
    +                <artifactId>accumulo-minicluster</artifactId>
    +                <version>${accumulo.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.accumulo</groupId>
    +                <artifactId>accumulo-start</artifactId>
    +                <version>${accumulo.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.accumulo</groupId>
    +                <artifactId>accumulo-trace</artifactId>
    +                <version>${accumulo.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.activemq</groupId>
    +                <artifactId>activemq-core</artifactId>
    +                <version>${activemq.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.springframework</groupId>
    +                        <artifactId>spring-context</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.activemq</groupId>
    +                <artifactId>kahadb</artifactId>
    +                <version>${activemq.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.avro</groupId>
    +                <artifactId>avro</artifactId>
    +                <version>${avro.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.avro</groupId>
    +                <artifactId>avro-mapred</artifactId>
    +                <classifier>hadoop2</classifier>
    +                <version>${avro.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jetty-util</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jetty</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.derby</groupId>
    +                <artifactId>derby</artifactId>
    +                <version>${derby.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.httpcomponents</groupId>
    +                <artifactId>httpclient</artifactId>
    +                <version>${httpcomponents.client.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.httpcomponents</groupId>
    +                <artifactId>httpcore</artifactId>
    +                <version>${httpcomponents.core.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.velocity</groupId>
    +                <artifactId>velocity-engine-core</artifactId>
    +                <version>${velocity.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>stax</groupId>
    +                <artifactId>stax-api</artifactId>
    +                <version>${stax.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.orc</groupId>
    +                <artifactId>orc-core</artifactId>
    +                <version>${orc.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-common</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.hive</groupId>
    +                        <artifactId>hive-storage-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hive</groupId>
    +                <artifactId>hive-storage-api</artifactId>
    +                <version>${storage-api.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.pig</groupId>
    +                <artifactId>pig</artifactId>
    +                <version>${pig.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jetty-util</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jetty</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.thrift</groupId>
    +                <artifactId>libfb303</artifactId>
    +                <version>${libfb303.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.thrift</groupId>
    +                <artifactId>libthrift</artifactId>
    +                <version>${libthrift.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.zookeeper</groupId>
    +                <artifactId>zookeeper</artifactId>
    +                <version>${zookeeper.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commmons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.httpcomponents</groupId>
    +                        <artifactId>httpcore</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.httpcomponents</groupId>
    +                        <artifactId>httpclient</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.curator</groupId>
    +                <artifactId>curator-client</artifactId>
    +                <version>${curator.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.curator</groupId>
    +                <artifactId>curator-framework</artifactId>
    +                <version>${curator.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.curator</groupId>
    +                <artifactId>curator-recipes</artifactId>
    +                <version>${curator.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.codehaus.groovy</groupId>
    +                <artifactId>groovy-all</artifactId>
    +                <version>${groovy.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.core</groupId>
    +                <artifactId>jackson-annotations</artifactId>
    +                <version>${jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.core</groupId>
    +                <artifactId>jackson-core</artifactId>
    +                <version>${jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.core</groupId>
    +                <artifactId>jackson-databind</artifactId>
    +                <version>${jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.module</groupId>
    +                <artifactId>jackson-module-scala_${scala.binary.version}</artifactId>
    +                <version>${jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.dataformat</groupId>
    +                <artifactId>jackson-dataformat-smile</artifactId>
    +                <version>${jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.jaxrs</groupId>
    +                <artifactId>jackson-jaxrs-json-provider</artifactId>
    +                <version>${jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.fasterxml.jackson.module</groupId>
    +                <artifactId>jackson-module-jaxb-annotations</artifactId>
    +                <version>${jackson.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.codehaus.jettison</groupId>
    +                <artifactId>jettison</artifactId>
    +                <version>${jettison.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>stax</groupId>
    +                        <artifactId>stax-api</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-rewrite</artifactId>
    +                <version>${jetty.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-server</artifactId>
    +                <version>${jetty.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-servlet</artifactId>
    +                <version>${jetty.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.eclipse.jetty</groupId>
    +                <artifactId>jetty-webapp</artifactId>
    +                <version>${jetty.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>javax.servlet</groupId>
    +                <artifactId>javax.servlet-api</artifactId>
    +                <version>${javax-servlet.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.datanucleus</groupId>
    +                <artifactId>datanucleus-api-jdo</artifactId>
    +                <version>${datanucleus-api-jdo.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.datanucleus</groupId>
    +                <artifactId>datanucleus-core</artifactId>
    +                <version>${datanucleus-core.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.datanucleus</groupId>
    +                <artifactId>datanucleus-rdbms</artifactId>
    +                <version>${datanucleus-rdbms.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.datanucleus</groupId>
    +                <artifactId>javax.jdo</artifactId>
    +                <version>${datanucleus-jdo.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>com.tdunning</groupId>
    +                <artifactId>json</artifactId>
    +                <version>${json.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.mockito</groupId>
    +                <artifactId>mockito-all</artifactId>
    +                <version>${mockito-all.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.slf4j</groupId>
    +                <artifactId>slf4j-api</artifactId>
    +                <version>${slf4j.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>xerces</groupId>
    +                <artifactId>xercesImpl</artifactId>
    +                <version>${xerces.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-client</artifactId>
    +                <version>${hadoop.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>commmons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-auth</artifactId>
    +                <version>${hadoop.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>commmons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-common</artifactId>
    +                <version>${hadoop.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jetty-sslengine</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commmons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.httpcomponents</groupId>
    +                        <artifactId>httpcore</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.apache.httpcomponents</groupId>
    +                        <artifactId>httpclient</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-hdfs</artifactId>
    +                <version>${hadoop.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
    +                <version>${hadoop.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commmons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.codahale.metrics</groupId>
    +                        <artifactId>metrics-core</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-mapreduce-client-common</artifactId>
    +                <version>${hadoop.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commmons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-mapreduce-client-core</artifactId>
    +                <version>${hadoop.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.slf4j</groupId>
    +                        <artifactId>slf4j-log4j12</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>commmons-logging</groupId>
    +                        <artifactId>commons-logging</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-minikdc</artifactId>
    +                <version>${hadoop.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-api</artifactId>
    +                <version>${hadoop.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-client</artifactId>
    +                <version>${hadoop.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-common</artifactId>
    +                <version>${hadoop.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-registry</artifactId>
    +                <version>${hadoop.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-server-web-common</artifactId>
    +                <version>${hadoop.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-yarn-server-web-proxy</artifactId>
    +                <version>${hadoop.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hbase</groupId>
    +                <artifactId>hbase-common</artifactId>
    +                <version>${hbase.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hbase</groupId>
    +                <artifactId>hbase-hadoop-compat</artifactId>
    +                <version>${hbase.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hbase</groupId>
    +                <artifactId>hbase-hadoop2-compat</artifactId>
    +                <version>${hbase.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>javax.servlet</groupId>
    +                        <artifactId>servlet-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>javax.servlet.jsp</groupId>
    +                        <artifactId>jsp-api</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jruby</groupId>
    +                        <artifactId>jruby-complete</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.jboss.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>io.netty</groupId>
    +                        <artifactId>netty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jsp-2.1</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jsp-api-2.1</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>servlet-api-2.5</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>servlet-api-2.5</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey</groupId>
    +                        <artifactId>jersey-core</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey</groupId>
    +                        <artifactId>jersey-json</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.sun.jersey</groupId>
    +                        <artifactId>jersey-server</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jetty</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>org.mortbay.jetty</groupId>
    +                        <artifactId>jetty-util</artifactId>
    +                    </exclusion>
    +                    <exclusion>
    +                        <groupId>com.codahale.metrics</groupId>
    +                        <artifactId>metrics-core</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hbase</groupId>
    +                <artifactId>hbase-server</artifactId>
    +                <version>${hbase.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.glassfish</groupId>
    +                        <artifactId>javax.el</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hbase</groupId>
    +                <artifactId>hbase-mapreduce</artifactId>
    +                <version>${hbase.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.hadoop</groupId>
    +                <artifactId>hadoop-minicluster</artifactId>
    +                <version>${hadoop.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.scala-lang</groupId>
    +                <artifactId>scala-library</artifactId>
    +                <version>${scala.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.apache.spark</groupId>
    +                <artifactId>spark-core_${scala.binary.version}</artifactId>
    +                <version>${spark.version}</version>
    +                <exclusions>
    +                    <exclusion>
    +                        <groupId>org.apache.hadoop</groupId>
    +                        <artifactId>hadoop-core</artifactId>
    +                    </exclusion>
    +                </exclusions>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.jamon</groupId>
    +                <artifactId>jamon-runtime</artifactId>
    +                <version>${jamon-runtime.version}</version>
    +            </dependency>
    +            <dependency>
    +                <groupId>org.xerial.snappy</groupId>
    +                <artifactId>snappy-java</artifactId>
    +                <version>${snappy.version}</version>
    +            </dependency>
    +        </dependencies>
    +    </dependencyManagement>
    +    
    +    <dependencies>
    +        <!-- dependencies are always listed in sorted order by groupId, artifectId -->
    +        <!-- global dependencies -->
    +        <dependency>
    +            <groupId>org.slf4j</groupId>
    +            <artifactId>slf4j-api</artifactId>
    +            <version>${slf4j.version}</version>
    +        </dependency>
    +        <dependency>
    +            <groupId>org.apache.hive</groupId>
    +            <artifactId>hive-upgrade-acid</artifactId>
    +            <version>${project.version}</version>
    +        </dependency>
    +        <dependency>
    +            <groupId>org.mockito</groupId>
    +            <artifactId>mockito-all</artifactId>
    +            <scope>test</scope>
    +        </dependency>
    +    </dependencies>
    +    
    +    <build>
    +        <pluginManagement>
    +            <plugins>
    +                <!-- plugins are always listed in sorted order by groupId, artifectId -->
    +                <plugin>
    +                    <groupId>org.antlr</groupId>
    +                    <artifactId>antlr3-maven-plugin</artifactId>
    +                    <version>${antlr.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-compiler-plugin</artifactId>
    +                    <version>${maven.compiler.plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-antrun-plugin</artifactId>
    +                    <version>${maven.antrun.plugin.version}</version>
    +                    <dependencies>
    +                        <dependency>
    +                            <groupId>ant-contrib</groupId>
    +                            <artifactId>ant-contrib</artifactId>
    +                            <version>${ant.contrib.version}</version>
    +                            <exclusions>
    +                                <exclusion>
    +                                    <groupId>ant</groupId>
    +                                    <artifactId>ant</artifactId>
    +                                </exclusion>
    +                            </exclusions>
    +                        </dependency>
    +                    </dependencies>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-assembly-plugin</artifactId>
    +                    <version>${maven.assembly.plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-eclipse-plugin</artifactId>
    +                    <version>${maven.eclipse.plugin.version}</version>
    +                    <configuration>
    +                        <downloadJavadocs>true</downloadJavadocs>
    +                        <downloadSources>true</downloadSources>
    +                        <buildOutputDirectory>target/eclipse/classes</buildOutputDirectory>
    +                        <workspaceActiveCodeStyleProfileName>Hive</workspaceActiveCodeStyleProfileName>
    +                        <workspaceCodeStylesURL>${basedir}/dev-support/eclipse-styles.xml</workspaceCodeStylesURL>
    +                    </configuration>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-checkstyle-plugin</artifactId>
    +                    <version>${maven.checkstyle.plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-enforcer-plugin</artifactId>
    +                    <version>${maven.enforcer.plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-install-plugin</artifactId>
    +                    <version>${maven.install.plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-shade-plugin</artifactId>
    +                    <version>${maven.shade.plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-surefire-plugin</artifactId>
    +                    <version>${maven.surefire.plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-jar-plugin</artifactId>
    +                    <version>${maven.jar.plugin.version}</version>
    +                    <configuration>
    +                        <archive>
    +                            <manifest>
    +                                <addDefaultImplementationEntries>true</addDefaultImplementationEntries>
    +                            </manifest>
    +                        </archive>
    +                    </configuration>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-war-plugin</artifactId>
    +                    <version>${maven.war.plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.datanucleus</groupId>
    +                    <artifactId>datanucleus-maven-plugin</artifactId>
    +                    <version>${datanucleus.maven.plugin.version}</version>
    +                    <dependencies>
    +                        <dependency>
    +                            <groupId>org.datanucleus</groupId>
    +                            <artifactId>datanucleus-core</artifactId>
    +                            <version>${datanucleus-core.version}</version>
    +                        </dependency>
    +                    </dependencies>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.felix</groupId>
    +                    <artifactId>maven-bundle-plugin</artifactId>
    +                    <version>${felix.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.apache.maven.plugins</groupId>
    +                    <artifactId>maven-dependency-plugin</artifactId>
    +                    <version>${maven.dependency.plugin.version}</version>
    +                </plugin>
    +                <plugin>
    +                    <groupId>org.codehaus.mojo</groupId>
    +                    <artifactId>build-helper-maven-plugin</artifactId>
    +                    <version>${maven.build-helper.plugin.version}</version>
    +                </plugin>
    +            </plugins>
    +        </pluginManagement>
    +        
    +        <plugins>
    +            <!-- plugins are always listed in sorted order by groupId, artifectId -->
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-antrun-plugin</artifactId>
    +                <executions>
    +                    <execution>
    +                        <id>define-classpath</id>
    +                        <phase>process-resources</phase>
    +                        <goals>
    +                            <goal>run</goal>
    +                        </goals>
    +                        <configuration>
    +                            <exportAntProperties>true</exportAntProperties>
    +                            <target>
    +                                <property name="maven.test.classpath" refid="maven.test.classpath"/>
    +                            </target>
    +                        </configuration>
    +                    </execution>
    +                    <execution>
    +                        <id>setup-test-dirs</id>
    +                        <phase>process-test-resources</phase>
    +                        <goals>
    +                            <goal>run</goal>
    +                        </goals>
    +                        <configuration>
    +                            <target>
    +                                <delete dir="${test.tmp.dir}"/>
    +                                <delete dir="${test.conf.dir}"/>
    +                                <delete dir="${test.warehouse.dir}"/>
    +                                <mkdir dir="${test.tmp.dir}"/>
    +                                <mkdir dir="${test.warehouse.dir}"/>
    +                                <mkdir dir="${test.conf.dir}"/>
    +                                <!-- copies hive-site.xml so it can be modified -->
    +                                <copy todir="${test.conf.dir}">
    +                                    <fileset dir="${basedir}/${hive.path.to.root}/data/conf/"/>
    +                                </copy>
    +                            </target>
    +                        </configuration>
    +                    </execution>
    +                </executions>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-clean-plugin</artifactId>
    +                <version>2.5</version>
    +                <configuration>
    +                    <filesets>
    +                        <fileset>
    +                            <directory>./</directory>
    +                            <includes>
    +                                <include>datanucleus.log</include>
    +                                <include>derby.log</include>
    +                            </includes>
    +                            <followSymlinks>false</followSymlinks>
    +                        </fileset>
    +                        <fileset>
    +                            <directory>build</directory>
    +                            <followSymlinks>false</followSymlinks>
    +                        </fileset>
    +                    </filesets>
    +                </configuration>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-checkstyle-plugin</artifactId>
    +                <configuration>
    +                    <configLocation>${checkstyle.conf.dir}/checkstyle.xml</configLocation>
    +                    <propertyExpansion>config_loc=${checkstyle.conf.dir}</propertyExpansion>
    +                    <includeTestSourceDirectory>true</includeTestSourceDirectory>
    +                </configuration>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-enforcer-plugin</artifactId>
    +                <executions>
    +                    <execution>
    +                        <id>enforce-no-snapshots</id>
    +                        <goals>
    +                            <goal>enforce</goal>
    +                        </goals>
    +                        <configuration>
    +                            <rules>
    +                                <requireReleaseDeps>
    +                                    <message>Release builds are not allowed to have SNAPSHOT depenendencies</message>
    +                                    <searchTransitive>true</searchTransitive>
    +                                    <onlyWhenRelease>true</onlyWhenRelease>
    +                                </requireReleaseDeps>
    +                            </rules>
    +                            <fail>true</fail>
    +                        </configuration>
    +                    </execution>
    +                    <execution>
    +                        <id>enforce-banned-dependencies</id>
    +                        <goals>
    +                            <goal>enforce</goal>
    +                        </goals>
    +                        <configuration>
    +                            <rules>
    +                                <bannedDependencies>
    +                                    <excludes>
    +                                        <!--LGPL licenced library-->
    +                                        <exclude>com.google.code.findbugs:annotations</exclude>
    +                                    </excludes>
    +                                </bannedDependencies>
    +                            </rules>
    +                            <fail>true</fail>
    +                        </configuration>
    +                    </execution>
    +                </executions>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.apache.maven.plugins</groupId>
    +                <artifactId>maven-surefire-plugin</artifactId>
    +                <configuration>
    +                    <excludes>
    +                        <exclude>**/TestSerDe.java</exclude>
    +                        <exclude>**/TestHiveMetaStore.java</exclude>
    +                        <exclude>**/ql/exec/vector/util/*.java</exclude>
    +                        <exclude>**/ql/exec/vector/udf/legacy/*.java</exclude>
    +                        <exclude>**/ql/exec/vector/udf/generic/*.java</exclude>
    +                        <exclude>**/TestHiveServer2Concurrency.java</exclude>
    +                        <exclude>${test.excludes.additional}</exclude>
    +                        <exclude>${skip.spark.files}</exclude>
    +                    </excludes>
    +                    <redirectTestOutputToFile>true</redirectTestOutputToFile>
    +                    <reuseForks>false</reuseForks>
    +                    <failIfNoTests>false</failIfNoTests>
    +                    <argLine>${maven.test.jvm.args}</argLine>
    +                    <trimStackTrace>false</trimStackTrace>
    +                    <additionalClasspathElements>
    +                        <additionalClasspathElement>${test.conf.dir}</additionalClasspathElement>
    +                        <additionalClasspathElement>${basedir}/${hive.path.to.root}/conf</additionalClasspathElement>
    +                    </additionalClasspathElements>
    +                    <environmentVariables>
    +                        <TZ>US/Pacific</TZ>
    +                        <LANG>en_US.UTF-8</LANG>
    +                        <HADOOP_CLASSPATH>${test.conf.dir}:${basedir}/${hive.path.to.root}/conf</HADOOP_CLASSPATH>
    +                        <HIVE_HADOOP_TEST_CLASSPATH>${test.hive.hadoop.classpath}</HIVE_HADOOP_TEST_CLASSPATH>
    +                        <SPARK_SUBMIT_CLASSPATH>
    +                            ${spark.home}/lib/spark-assembly-${spark.version}-hadoop2.4.0.jar:${test.hive.hadoop.classpath}
    +                        </SPARK_SUBMIT_CLASSPATH>
    +                        <SPARK_OSX_TEST_OPTS>-Dorg.xerial.snappy.tempdir=/tmp
    +                            -Dorg.xerial.snappy.lib.name=libsnappyjava.jnilib
    +                        </SPARK_OSX_TEST_OPTS>
    +                        <SPARK_SCALA_VERSION>2.12</SPARK_SCALA_VERSION>
    +                        <SPARK_HOME>${spark.home}</SPARK_HOME>
    +                        <PATH>${env.PATH}${test.extra.path}</PATH>
    +                    </environmentVariables>
    +                    <systemPropertyVariables>
    +                        <build.dir>${project.build.directory}</build.dir>
    +                        <!-- required by zk test ClientBase -->
    +                        <build.test.dir>${test.tmp.dir}</build.test.dir>x`
    +                        <!-- required by a few tests to find the derby jar -->
    +                        <derby.version>${derby.version}</derby.version>
    +                        <derby.stream.error.file>${test.tmp.dir}/derby.log</derby.stream.error.file>
    +                        <hadoop.bin.path>${hadoop.bin.path}</hadoop.bin.path>
    +                        <!-- required by Hadoop's JobHistory -->
    +                        <hadoop.log.dir>${test.tmp.dir}</hadoop.log.dir>
    +                        <hive.root>${basedir}/${hive.path.to.root}/</hive.root>
    +                        <hive.version>${project.version}</hive.version>
    +                        <!-- required for hive-exec jar path and tests which reference a jar -->
    +                        <maven.local.repository>${maven.repo.local}</maven.local.repository>
    +                        <mapred.job.tracker>local</mapred.job.tracker>
    +                        <log4j.configurationFile>${test.log4j.scheme}${test.conf.dir}/hive-log4j2.properties
    +                        </log4j.configurationFile>
    +                        <hive.test.console.log.level>${test.console.log.level}</hive.test.console.log.level>
    +                        <log4j.debug>true</log4j.debug>
    +                        <!-- don't diry up /tmp -->
    +                        <java.io.tmpdir>${test.tmp.dir}</java.io.tmpdir>
    +                        <spark.home>${spark.home}</spark.home>
    +                        <!-- Hadoop's minidfs class uses this -->
    +                        <test.build.data>${test.tmp.dir}</test.build.data>
    +                        <!-- required by QTestUtil -->
    +                        <test.data.files>${basedir}/${hive.path.to.root}/data/files</test.data.files>
    +                        <test.data.dir>${basedir}/${hive.path.to.root}/data/files</test.data.dir>
    +                        <test.tmp.dir>${test.tmp.dir}</test.tmp.dir>
    +                        <test.tmp.dir.uri>${test.tmp.dir.uri}</test.tmp.dir.uri>
    +                        <test.dfs.mkdir>${test.dfs.mkdir}</test.dfs.mkdir>
    +                        <test.output.overwrite>${test.output.overwrite}</test.output.overwrite>
    +                        <test.warehouse.dir>${test.warehouse.scheme}${test.warehouse.dir}</test.warehouse.dir>
    +                        <java.net.preferIPv4Stack>true</java.net.preferIPv4Stack>
    +                        <!-- EnforceReadOnlyTables hook and QTestUtil -->
    +                        <test.src.tables>
    +                            src,src1,srcbucket,srcbucket2,src_json,src_thrift,src_sequencefile,srcpart,alltypesorc,alltypesparquet,src_hbase,cbo_t1,cbo_t2,cbo_t3,src_cbo,part,lineitem
    +                        </test.src.tables>
    +                        <java.security.krb5.conf>${test.conf.dir}/krb5.conf</java.security.krb5.conf>
    +                        <!-- Required by spark to work around SPARK-14958 -->
    +                        <antlr.version>${antlr.version}</antlr.version>
    +                        <qfile>${qfile}</qfile>
    +                        <initScript>${initScript}</initScript>
    +                        <clustermode>${clustermode}</clustermode>
    +                        <qfile_regex>${qfile_regex}</qfile_regex>
    +                        <run_disabled>${run_disabled}</run_disabled>
    +                    </systemPropertyVariables>
    +                </configuration>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.apache.rat</groupId>
    +                <artifactId>apache-rat-plugin</artifactId>
    +                <version>0.10</version>
    +                <configuration>
    +                    <excludes>
    +                        <exclude>binary-package-licenses/**</exclude>
    +                        <exclude>data/**</exclude>
    +                        <exclude>conf/**</exclude>
    +                        <exclude>checkstyle/**</exclude>
    +                        <exclude>bin/**</exclude>
    +                        <exclude>itests/**</exclude>
    +                        <exclude>**/README.md</exclude>
    +                        <exclude>**/*.iml</exclude>
    +                        <exclude>**/*.txt</exclude>
    +                        <exclude>**/*.log</exclude>
    +                        <exclude>**/*.arcconfig</exclude>
    +                        <exclude>**/package-info.java</exclude>
    +                        <exclude>**/*.properties</exclude>
    +                        <exclude>**/*.q</exclude>
    +                        <exclude>**/*.q.out</exclude>
    +                        <exclude>**/*.q.out_*</exclude>
    +                        <exclude>**/*.xml</exclude>
    +                        <exclude>**/gen/**</exclude>
    +                        <exclude>**/scripts/**</exclude>
    +                        <exclude>**/resources/**</exclude>
    +                        <exclude>**/*.rc</exclude>
    +                        <exclude>**/*.rcfile</exclude>
    +                        <exclude>**/*.qv</exclude>
    +                        <exclude>**/*.out</exclude>
    +                        <exclude>**/RecordTestObj.java</exclude>
    +                        <exclude>**/*.m</exclude>
    +                        <exclude>**/gen-java/**</exclude>
    +                        <exclude>**/testdata/**</exclude>
    +                        <exclude>**/ptest2/*.md</exclude>
    +                        <exclude>**/test/org/apache/hadoop/hive/hbase/avro/**</exclude>
    +                        <exclude>**/avro_test.avpr</exclude>
    +                        <exclude>**/xmlReport.pl</exclude>
    +                        <exclude>**/*.html</exclude>
    +                        <exclude>**/sit</exclude>
    +                        <exclude>**/test/queries/**/*.sql</exclude>
    +                        <exclude>**/patchprocess/**</exclude>
    +                        <exclude>**/metastore_db/**</exclude>
    +                    </excludes>
    +                </configuration>
    +            </plugin>
    +            <plugin>
    +                <groupId>org.jamon</groupId>
    +                <artifactId>jamon-maven-plugin</artifactId>
    +                <version>${jamon.plugin.version}</version>
    +            </plugin>
    +        </plugins>
    +    </build>
    +    
    +    <profiles>
    +        <profile>
    +            <id>thriftif</id>
    +            <build>
    +                <plugins>
    +                    <plugin>
    +                        <groupId>org.apache.maven.plugins</groupId>
    +                        <artifactId>maven-antrun-plugin</artifactId>
    +                        <executions>
    +                            <execution>
    +                                <id>generate-thrift-sources</id>
    +                                <phase>generate-sources</phase>
    +                                <configuration>
    +                                    <target>
    +                                        <taskdef name="for" classname="net.sf.antcontrib.logic.ForTask"
    +                                                 classpathref="maven.plugin.classpath"/>
    +                                        <property name="thrift.args"
    +                                                  value="-I ${thrift.home} --gen java:beans,hashcode,generated_annotations=undated --gen cpp --gen php --gen py --gen rb"/>
    +                                        <property name="thrift.gen.dir" value="${basedir}/src/gen/thrift"/>
    +                                        <delete dir="${thrift.gen.dir}"/>
    +                                        <mkdir dir="${thrift.gen.dir}"/>
    +                                        <for param="thrift.file">
    +                                            <path>
    +                                                <fileset dir="."
    +                                                         includes="if/*.thrift,if/test/*.thrift,src/main/thrift/*.thrift"/>
    +                                            </path>
    +                                            <sequential>
    +                                                <echo message="Generating Thrift code for @{thrift.file}"/>
    +                                                <exec executable="${thrift.home}/bin/thrift" failonerror="true" dir=".">
    +                                                    <arg line="${thrift.args} -I ${basedir}/include -I ${basedir}/.. -o ${thrift.gen.dir} @{thrift.file} "/>
    +                                                </exec>
    +                                            </sequential>
    +                                        </for>
    +                                    </target>
    +                                </configuration>
    +                                <goals>
    +                                    <goal>run</goal>
    +                                </goals>
    +                            </execution>
    +                        </executions>
    +                    </plugin>
    +                    <plugin>
    +                        <groupId>org.apache.maven.plugins</groupId>
    +                        <artifactId>maven-enforcer-plugin</artifactId>
    +                        <executions>
    +                            <execution>
    +                                <id>enforce-property</id>
    +                                <goals>
    +                                    <goal>enforce</goal>
    +                                </goals>
    +                                <configuration>
    +                                    <rules>
    +                                        <requireProperty>
    +                                            <property>thrift.home</property>
    +                                        </requireProperty>
    +                                    </rules>
    +                                    <fail>true</fail>
    +                                </configuration>
    +                            </execution>
    +                        </executions>
    +                    </plugin>
    +                </plugins>
    +            </build>
    +        </profile>
    +        
    +        <profile>
    +            <id>sources</id>
    +            <build>
    +                <plugins>
    +                    <plugin>
    +                        <groupId>org.apache.maven.plugins</groupId>
    +                        <artifactId>maven-source-plugin</artifactId>
    +                        <executions>
    +                            <execution>
    +                                <id>attach-sources</id>
    +                                <goals>
    +                                    <goal>jar</goal>
    +                                </goals>
    +                            </execution>
    +                        </executions>
    +                    </plugin>
    +                </plugins>
    +            </build>
    +        </profile>
    +        
    +        <profile>
    +            <id>javadoc</id>
    +            <build>
    +                <plugins>
    +                    <plugin>
    +                        <groupId>org.apache.maven.plugins</groupId>
    +                        <artifactId>maven-javadoc-plugin</artifactId>
    +                        <configuration>
    +                            <failOnError>false</failOnError>
    +                        </configuration>
    +                        <executions>
    +                            <execution>
    +                                <id>attach-javadocs</id>
    +                                <goals>
    +                                    <goal>jar</goal>
    +                                </goals>
    +                            </execution>
    +                        </executions>
    +                    </plugin>
    +                </plugins>
    +            </build>
    +        </profile>
    +        
    +        <profile>
    +            <id>findbugs</id>
    +            <build>
    +                <plugins>
    +                    <plugin>
    +                        <groupId>org.codehaus.mojo</groupId>
    +                        <artifactId>findbugs-maven-plugin</artifactId>
    +                        <version>3.0.0</version>
    +                        <configuration>
    +                            <fork>true</fork>
    +                            <maxHeap>2048</maxHeap>
    +                            <jvmArgs>-Djava.awt.headless=true -Xmx2048m -Xms512m</jvmArgs>
    +                            <excludeFilterFile>${basedir}/${hive.path.to.root}/findbugs/findbugs-exclude.xml
    +                            </excludeFilterFile>
    +                        </configuration>
    +                    </plugin>
    +                </plugins>
    +            </build>
    +            <reporting>
    +                <plugins>
    +                    <plugin>
    +                        <groupId>org.codehaus.mojo</groupId>
    +                        <artifactId>findbugs-maven-plugin</artifactId>
    +                        <version>3.0.0</version>
    +                        <configuration>
    +                            <fork>true</fork>
    +                            <maxHeap>2048</maxHeap>
    +                            <jvmArgs>-Djava.awt.headless=true -Xmx2048m -Xms512m</jvmArgs>
    +                            <excludeFilterFile>${basedir}/${hive.path.to.root}/findbugs/findbugs-exclude.xml
    +                            </excludeFilterFile>
    +                        </configuration>
    +                    </plugin>
    +                </plugins>
    +            </reporting>
    +        </profile>
    +        <profile>
    +            <!-- Windows-specific settings to allow unit tests to work -->
    +            <id>windows-test</id>
    +            <activation>
    +                <os>
    +                    <family>Windows</family>
    +                </os>
    +            </activation>
    +            <build>
    +                <plugins>
    +                    <!-- maven.test.classpath (used for HIVE_HADOOP_TEST_CLASSPATH) exceeds the 8K Windows -->
    +                    <!-- command shell limit which causes tests which call hadoop command to fail.         -->
    +                    <!-- Workaround is to copy all necessary jars to a single location to shorten the      -->
    +                    <!-- the length of the environment variable.                                           -->
    +                    <plugin>
    +                        <groupId>org.apache.maven.plugins</groupId>
    +                        <artifactId>maven-dependency-plugin</artifactId>
    +                        <version>2.8</version>
    +                        <executions>
    +                            <execution>
    +                                <id>copy-dependencies</id>
    +                                <phase>package</phase>
    +                                <goals>
    +                                    <goal>copy-dependencies</goal>
    +                                </goals>
    +                                <configuration>
    +                                    <outputDirectory>${project.build.directory}/deplibs/</outputDirectory>
    +                                    <overWriteReleases>false</overWriteReleases>
    +                                    <overWriteSnapshots>false</overWriteSnapshots>
    +                                    <overWriteIfNewer>true</overWriteIfNewer>
    +                                </configuration>
    +                            </execution>
    +                        </executions>
    +                    </plugin>
    +                </plugins>
    +            </build>
    +            <properties>
    +                <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop.cmd</hadoop.bin.path>
    +                <test.extra.path>;${env.HADOOP_HOME}/bin</test.extra.path>
    +                <test.hive.hadoop.classpath>${project.build.directory}/deplibs/*</test.hive.hadoop.classpath>
    +                <test.tmp.dir.uri>file:///${test.tmp.dir}</test.tmp.dir.uri>
    +                <test.log4j.scheme>file:/</test.log4j.scheme>
    +            </properties>
    +        </profile>
    +        <profile>
    +            <id>spark-test</id>
    +            <activation>
    +                <property>
    +                    <name>!skipSparkTests</name>
    +                </property>
    +            </activation>
    +            <properties>
    +                <skip.spark.files>
    +                    **/ql/exec/spark/session/TestSparkSessionManagerImpl.java,**/TestMultiSessionsHS2WithLocalClusterSpark.java,**/TestJdbcWithLocalClusterSpark.java
    +                </skip.spark.files>
    +            </properties>
    +        </profile>
    +        <profile>
    +            <id>itests</id>
    +            <modules>
    +                <module>itests</module>
    +            </modules>
    +        </profile>
    +    </profiles>
     </project>
    Index: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Main.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Main.java b/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Main.java
    --- a/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Main.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Main.java	(date 1666966315719)
    @@ -255,7 +255,8 @@
       // Configure the AuthFilter with the Kerberos params iff security
       // is enabled.
       public FilterHolder makeAuthFilter() throws IOException {
    -    FilterHolder authFilter = new FilterHolder(AuthFilter.class);
    +    // FilterHolder authFilter = new FilterHolder(AuthFilter.class);
    +    FilterHolder authFilter = new FilterHolder();
         UserNameHandler.allowAnonymous(authFilter);
         if (UserGroupInformation.isSecurityEnabled()) {
           //http://hadoop.apache.org/docs/r1.1.1/api/org/apache/hadoop/security/authentication/server/AuthenticationFilter.html
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/LongColumnStatsAggregator.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/LongColumnStatsAggregator.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/LongColumnStatsAggregator.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/LongColumnStatsAggregator.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/LongColumnStatsAggregator.java	(date 1666966498503)
    @@ -38,6 +38,8 @@
     import org.slf4j.Logger;
     import org.slf4j.LoggerFactory;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.longInspectorFromStats;
    +
     public class LongColumnStatsAggregator extends ColumnStatsAggregator implements
         IExtrapolatePartStatus {
     
    @@ -63,8 +65,7 @@
             LOG.trace("doAllPartitionContainStats for column: {} is: {}", colName,
                 doAllPartitionContainStats);
           }
    -      LongColumnStatsDataInspector longColumnStatsData =
    -          (LongColumnStatsDataInspector) cso.getStatsData().getLongStats();
    +      LongColumnStatsDataInspector longColumnStatsData = longInspectorFromStats(cso);
           if (longColumnStatsData.getNdvEstimator() == null) {
             ndvEstimator = null;
             break;
    @@ -96,8 +97,7 @@
           double densityAvgSum = 0.0;
           for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {
             ColumnStatisticsObj cso = csp.getColStatsObj();
    -        LongColumnStatsDataInspector newData =
    -            (LongColumnStatsDataInspector) cso.getStatsData().getLongStats();
    +        LongColumnStatsDataInspector newData = longInspectorFromStats(cso);
             lowerBound = Math.max(lowerBound, newData.getNumDVs());
             higherBound += newData.getNumDVs();
             densityAvgSum += (newData.getHighValue() - newData.getLowValue()) / newData.getNumDVs();
    @@ -174,8 +174,7 @@
             for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {
               ColumnStatisticsObj cso = csp.getColStatsObj();
               String partName = csp.getPartName();
    -          LongColumnStatsDataInspector newData =
    -              (LongColumnStatsDataInspector) cso.getStatsData().getLongStats();
    +          LongColumnStatsDataInspector newData = longInspectorFromStats(cso);
               // newData.isSetBitVectors() should be true for sure because we
               // already checked it before.
               if (indexMap.get(partName) != curIndex) {
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DecimalColumnStatsAggregator.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DecimalColumnStatsAggregator.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DecimalColumnStatsAggregator.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DecimalColumnStatsAggregator.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DecimalColumnStatsAggregator.java	(date 1666966498503)
    @@ -40,6 +40,8 @@
     import org.slf4j.Logger;
     import org.slf4j.LoggerFactory;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.decimalInspectorFromStats;
    +
     public class DecimalColumnStatsAggregator extends ColumnStatsAggregator implements
         IExtrapolatePartStatus {
     
    @@ -65,8 +67,8 @@
             LOG.trace("doAllPartitionContainStats for column: {} is: {}", colName,
                 doAllPartitionContainStats);
           }
    -      DecimalColumnStatsDataInspector decimalColumnStatsData =
    -          (DecimalColumnStatsDataInspector) cso.getStatsData().getDecimalStats();
    +      DecimalColumnStatsDataInspector decimalColumnStatsData = decimalInspectorFromStats(cso);
    +
           if (decimalColumnStatsData.getNdvEstimator() == null) {
             ndvEstimator = null;
             break;
    @@ -98,8 +100,7 @@
           double densityAvgSum = 0.0;
           for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {
             ColumnStatisticsObj cso = csp.getColStatsObj();
    -        DecimalColumnStatsDataInspector newData =
    -            (DecimalColumnStatsDataInspector) cso.getStatsData().getDecimalStats();
    +        DecimalColumnStatsDataInspector newData = decimalInspectorFromStats(cso);
             lowerBound = Math.max(lowerBound, newData.getNumDVs());
             higherBound += newData.getNumDVs();
             densityAvgSum += (MetaStoreUtils.decimalToDouble(newData.getHighValue()) - MetaStoreUtils
    @@ -187,8 +188,7 @@
             for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {
               ColumnStatisticsObj cso = csp.getColStatsObj();
               String partName = csp.getPartName();
    -          DecimalColumnStatsDataInspector newData =
    -              (DecimalColumnStatsDataInspector) cso.getStatsData().getDecimalStats();
    +          DecimalColumnStatsDataInspector newData = decimalInspectorFromStats(cso);
               // newData.isSetBitVectors() should be true for sure because we
               // already checked it before.
               if (indexMap.get(partName) != curIndex) {
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DecimalColumnStatsDataInspector.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DecimalColumnStatsDataInspector.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DecimalColumnStatsDataInspector.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DecimalColumnStatsDataInspector.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DecimalColumnStatsDataInspector.java	(date 1666966498503)
    @@ -43,6 +43,10 @@
         }
       }
     
    +  public DecimalColumnStatsDataInspector(DecimalColumnStatsData other) {
    +    super(other);
    +  }
    +
       @Override
       public DecimalColumnStatsDataInspector deepCopy() {
         return new DecimalColumnStatsDataInspector(this);
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DoubleColumnStatsDataInspector.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DoubleColumnStatsDataInspector.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DoubleColumnStatsDataInspector.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DoubleColumnStatsDataInspector.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DoubleColumnStatsDataInspector.java	(date 1666966498503)
    @@ -43,6 +43,10 @@
         }
       }
     
    +  public DoubleColumnStatsDataInspector(DoubleColumnStatsData other) {
    +    super(other);
    +  }
    +
       @Override
       public DoubleColumnStatsDataInspector deepCopy() {
         return new DoubleColumnStatsDataInspector(this);
    Index: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java
    --- a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java	(date 1666964078043)
    @@ -175,7 +175,7 @@
         executionCompletionExecutorService = MoreExecutors.listeningDecorator(
             executionCompletionExecutorServiceRaw);
         ListenableFuture<?> future = waitQueueExecutorService.submit(new WaitQueueWorker());
    -    Futures.addCallback(future, new WaitQueueWorkerCallback());
    +    Futures.addCallback(future, new WaitQueueWorkerCallback(), MoreExecutors.directExecutor());
       }
     
       private LlapQueueComparatorBase createComparator(
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java	(date 1666966498503)
    @@ -38,6 +38,8 @@
     import org.slf4j.Logger;
     import org.slf4j.LoggerFactory;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.stringInspectorFromStats;
    +
     public class StringColumnStatsAggregator extends ColumnStatsAggregator implements
         IExtrapolatePartStatus {
     
    @@ -63,8 +65,7 @@
             LOG.trace("doAllPartitionContainStats for column: {} is: {}", colName,
                 doAllPartitionContainStats);
           }
    -      StringColumnStatsDataInspector stringColumnStatsData =
    -          (StringColumnStatsDataInspector) cso.getStatsData().getStringStats();
    +      StringColumnStatsDataInspector stringColumnStatsData = stringInspectorFromStats(cso);
           if (stringColumnStatsData.getNdvEstimator() == null) {
             ndvEstimator = null;
             break;
    @@ -93,8 +94,7 @@
           StringColumnStatsDataInspector aggregateData = null;
           for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {
             ColumnStatisticsObj cso = csp.getColStatsObj();
    -        StringColumnStatsDataInspector newData =
    -            (StringColumnStatsDataInspector) cso.getStatsData().getStringStats();
    +        StringColumnStatsDataInspector newData = stringInspectorFromStats(cso);
             if (ndvEstimator != null) {
               ndvEstimator.mergeEstimators(newData.getNdvEstimator());
             }
    @@ -149,7 +149,7 @@
               ColumnStatisticsObj cso = csp.getColStatsObj();
               String partName = csp.getPartName();
               StringColumnStatsDataInspector newData =
    -              (StringColumnStatsDataInspector) cso.getStatsData().getStringStats();
    +              stringInspectorFromStats(cso);
               // newData.isSetBitVectors() should be true for sure because we
               // already checked it before.
               if (indexMap.get(partName) != curIndex) {
    @@ -211,7 +211,8 @@
           int numPartsWithStats, Map<String, Double> adjustedIndexMap,
           Map<String, ColumnStatisticsData> adjustedStatsMap, double densityAvg) {
         int rightBorderInd = numParts;
    -    StringColumnStatsDataInspector extrapolateStringData = new StringColumnStatsDataInspector();
    +    StringColumnStatsDataInspector extrapolateStringData =
    +        new StringColumnStatsDataInspector();
         Map<String, StringColumnStatsData> extractedAdjustedStatsMap = new HashMap<>();
         for (Map.Entry<String, ColumnStatisticsData> entry : adjustedStatsMap.entrySet()) {
           extractedAdjustedStatsMap.put(entry.getKey(), entry.getValue().getStringStats());
    Index: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java
    --- a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java	(date 1666964053719)
    @@ -128,7 +128,7 @@
             sendCounterInterval, maxEventsToGet, requestCounter, containerIdStr, initialEvent,
             fragmentRequestId, wmCounters);
         ListenableFuture<Boolean> future = heartbeatExecutor.submit(currentCallable);
    -    Futures.addCallback(future, new HeartbeatCallback(errorReporter));
    +    Futures.addCallback(future, new HeartbeatCallback(errorReporter), MoreExecutors.directExecutor());
       }
     
       /**
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DateColumnStatsAggregator.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DateColumnStatsAggregator.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DateColumnStatsAggregator.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DateColumnStatsAggregator.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DateColumnStatsAggregator.java	(date 1666966498503)
    @@ -38,6 +38,8 @@
     import org.slf4j.Logger;
     import org.slf4j.LoggerFactory;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.dateInspectorFromStats;
    +
     public class DateColumnStatsAggregator extends ColumnStatsAggregator implements
         IExtrapolatePartStatus {
     
    @@ -62,8 +64,8 @@
                 cso.getStatsData().getSetField());
             LOG.trace("doAllPartitionContainStats for column: {} is: {}", colName, doAllPartitionContainStats);
           }
    -      DateColumnStatsDataInspector dateColumnStats =
    -          (DateColumnStatsDataInspector) cso.getStatsData().getDateStats();
    +      DateColumnStatsDataInspector dateColumnStats = dateInspectorFromStats(cso);
    +
           if (dateColumnStats.getNdvEstimator() == null) {
             ndvEstimator = null;
             break;
    @@ -95,9 +97,7 @@
           double densityAvgSum = 0.0;
           for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {
             ColumnStatisticsObj cso = csp.getColStatsObj();
    -        DateColumnStatsDataInspector newData =
    -            (DateColumnStatsDataInspector) cso.getStatsData().getDateStats();
    -        lowerBound = Math.max(lowerBound, newData.getNumDVs());
    +        DateColumnStatsDataInspector newData = dateInspectorFromStats(cso);
             higherBound += newData.getNumDVs();
             densityAvgSum += (diff(newData.getHighValue(), newData.getLowValue()))
                 / newData.getNumDVs();
    @@ -174,8 +174,7 @@
             for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {
               ColumnStatisticsObj cso = csp.getColStatsObj();
               String partName = csp.getPartName();
    -          DateColumnStatsDataInspector newData =
    -              (DateColumnStatsDataInspector) cso.getStatsData().getDateStats();
    +          DateColumnStatsDataInspector newData = dateInspectorFromStats(cso);
               // newData.isSetBitVectors() should be true for sure because we
               // already checked it before.
               if (indexMap.get(partName) != curIndex) {
    Index: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java
    --- a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java	(date 1666964006667)
    @@ -174,7 +174,7 @@
               Thread.getDefaultUncaughtExceptionHandler().uncaughtException(Thread.currentThread(), t);
             }
           }
    -    });
    +    }, MoreExecutors.directExecutor());
         // TODO: why is this needed? we could just save the host and port?
         nodeId = LlapNodeId.getInstance(localAddress.get().getHostName(), localAddress.get().getPort());
         LOG.info("AMReporter running with DaemonId: {}, NodeId: {}", daemonId, nodeId);
    @@ -274,7 +274,7 @@
             LOG.warn("Failed to send taskKilled for {}. The attempt will likely time out.",
                 taskAttemptId);
           }
    -    });
    +    }, MoreExecutors.directExecutor());
       }
     
       public void queryComplete(QueryIdentifier queryIdentifier) {
    @@ -342,7 +342,7 @@
                         amNodeInfo.amNodeId, currentQueryIdentifier, t);
                       queryFailedHandler.queryFailed(currentQueryIdentifier);
                     }
    -              });
    +              }, MoreExecutors.directExecutor());
                 }
               }
             } catch (InterruptedException e) {
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java	(date 1666966498503)
    @@ -24,13 +24,15 @@
     import org.apache.hadoop.hive.metastore.api.Decimal;
     import org.apache.hadoop.hive.metastore.columnstats.cache.DecimalColumnStatsDataInspector;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.decimalInspectorFromStats;
    +
     public class DecimalColumnStatsMerger extends ColumnStatsMerger {
       @Override
       public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj newColStats) {
         DecimalColumnStatsDataInspector aggregateData =
    -        (DecimalColumnStatsDataInspector) aggregateColStats.getStatsData().getDecimalStats();
    +        decimalInspectorFromStats(aggregateColStats);
         DecimalColumnStatsDataInspector newData =
    -        (DecimalColumnStatsDataInspector) newColStats.getStatsData().getDecimalStats();
    +        decimalInspectorFromStats(newColStats);
     
         Decimal lowValue = getMin(aggregateData.getLowValue(), newData.getLowValue());
         aggregateData.setLowValue(lowValue);
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/StringColumnStatsDataInspector.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/StringColumnStatsDataInspector.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/StringColumnStatsDataInspector.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/StringColumnStatsDataInspector.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/StringColumnStatsDataInspector.java	(date 1666966498503)
    @@ -44,6 +44,10 @@
         }
       }
     
    +  public StringColumnStatsDataInspector(StringColumnStatsData other) {
    +    super(other);
    +  }
    +
       @Override
       public StringColumnStatsDataInspector deepCopy() {
         return new StringColumnStatsDataInspector(this);
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/LongColumnStatsDataInspector.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/LongColumnStatsDataInspector.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/LongColumnStatsDataInspector.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/LongColumnStatsDataInspector.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/LongColumnStatsDataInspector.java	(date 1666966498503)
    @@ -43,6 +43,10 @@
         }
       }
     
    +  public LongColumnStatsDataInspector(LongColumnStatsData other) {
    +    super(other);
    +  }
    +
       @Override
       public LongColumnStatsDataInspector deepCopy() {
         return new LongColumnStatsDataInspector(this);
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/ColumnsStatsUtils.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/ColumnsStatsUtils.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/ColumnsStatsUtils.java
    new file mode 100644
    --- /dev/null	(date 1666966498503)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/ColumnsStatsUtils.java	(date 1666966498503)
    @@ -0,0 +1,117 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.hadoop.hive.metastore.columnstats;
    +
    +import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;
    +import org.apache.hadoop.hive.metastore.columnstats.cache.DateColumnStatsDataInspector;
    +import org.apache.hadoop.hive.metastore.columnstats.cache.DecimalColumnStatsDataInspector;
    +import org.apache.hadoop.hive.metastore.columnstats.cache.DoubleColumnStatsDataInspector;
    +import org.apache.hadoop.hive.metastore.columnstats.cache.LongColumnStatsDataInspector;
    +import org.apache.hadoop.hive.metastore.columnstats.cache.StringColumnStatsDataInspector;
    +
    +/**
    + * Utils class for columnstats package.
    + */
    +public final class ColumnsStatsUtils {
    +
    +  private ColumnsStatsUtils(){}
    +
    +  /**
    +   * Convertes to DateColumnStatsDataInspector if it's a DateColumnStatsData.
    +   * @param cso ColumnStatisticsObj
    +   * @return DateColumnStatsDataInspector
    +   */
    +  public static DateColumnStatsDataInspector dateInspectorFromStats(ColumnStatisticsObj cso) {
    +    DateColumnStatsDataInspector dateColumnStats;
    +    if (cso.getStatsData().getDateStats() instanceof DateColumnStatsDataInspector) {
    +      dateColumnStats =
    +          (DateColumnStatsDataInspector)(cso.getStatsData().getDateStats());
    +    } else {
    +      dateColumnStats = new DateColumnStatsDataInspector(cso.getStatsData().getDateStats());
    +    }
    +    return dateColumnStats;
    +  }
    +
    +  /**
    +   * Convertes to StringColumnStatsDataInspector
    +   * if it's a StringColumnStatsData.
    +   * @param cso ColumnStatisticsObj
    +   * @return StringColumnStatsDataInspector
    +   */
    +  public static StringColumnStatsDataInspector stringInspectorFromStats(ColumnStatisticsObj cso) {
    +    StringColumnStatsDataInspector columnStats;
    +    if (cso.getStatsData().getStringStats() instanceof StringColumnStatsDataInspector) {
    +      columnStats =
    +          (StringColumnStatsDataInspector)(cso.getStatsData().getStringStats());
    +    } else {
    +      columnStats = new StringColumnStatsDataInspector(cso.getStatsData().getStringStats());
    +    }
    +    return columnStats;
    +  }
    +
    +  /**
    +   * Convertes to LongColumnStatsDataInspector if it's a LongColumnStatsData.
    +   * @param cso ColumnStatisticsObj
    +   * @return LongColumnStatsDataInspector
    +   */
    +  public static LongColumnStatsDataInspector longInspectorFromStats(ColumnStatisticsObj cso) {
    +    LongColumnStatsDataInspector columnStats;
    +    if (cso.getStatsData().getLongStats() instanceof LongColumnStatsDataInspector) {
    +      columnStats =
    +          (LongColumnStatsDataInspector)(cso.getStatsData().getLongStats());
    +    } else {
    +      columnStats = new LongColumnStatsDataInspector(cso.getStatsData().getLongStats());
    +    }
    +    return columnStats;
    +  }
    +
    +  /**
    +   * Convertes to DoubleColumnStatsDataInspector
    +   * if it's a DoubleColumnStatsData.
    +   * @param cso ColumnStatisticsObj
    +   * @return DoubleColumnStatsDataInspector
    +   */
    +  public static DoubleColumnStatsDataInspector doubleInspectorFromStats(ColumnStatisticsObj cso) {
    +    DoubleColumnStatsDataInspector columnStats;
    +    if (cso.getStatsData().getDoubleStats() instanceof DoubleColumnStatsDataInspector) {
    +      columnStats =
    +          (DoubleColumnStatsDataInspector)(cso.getStatsData().getDoubleStats());
    +    } else {
    +      columnStats = new DoubleColumnStatsDataInspector(cso.getStatsData().getDoubleStats());
    +    }
    +    return columnStats;
    +  }
    +
    +  /**
    +   * Convertes to DecimalColumnStatsDataInspector
    +   * if it's a DecimalColumnStatsData.
    +   * @param cso ColumnStatisticsObj
    +   * @return DecimalColumnStatsDataInspector
    +   */
    +  public static DecimalColumnStatsDataInspector decimalInspectorFromStats(ColumnStatisticsObj cso) {
    +    DecimalColumnStatsDataInspector columnStats;
    +    if (cso.getStatsData().getDecimalStats() instanceof DecimalColumnStatsDataInspector) {
    +      columnStats =
    +          (DecimalColumnStatsDataInspector)(cso.getStatsData().getDecimalStats());
    +    } else {
    +      columnStats = new DecimalColumnStatsDataInspector(cso.getStatsData().getDecimalStats());
    +    }
    +    return columnStats;
    +  }
    +}
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DateColumnStatsDataInspector.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DateColumnStatsDataInspector.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DateColumnStatsDataInspector.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DateColumnStatsDataInspector.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/cache/DateColumnStatsDataInspector.java	(date 1666966498503)
    @@ -43,6 +43,10 @@
         }
       }
     
    +  public DateColumnStatsDataInspector(DateColumnStatsData other) {
    +    super(other);
    +  }
    +
       @Override
       public DateColumnStatsDataInspector deepCopy() {
         return new DateColumnStatsDataInspector(this);
    Index: ql/src/test/org/apache/hadoop/hive/ql/stats/TestStatsUtils.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/ql/src/test/org/apache/hadoop/hive/ql/stats/TestStatsUtils.java b/ql/src/test/org/apache/hadoop/hive/ql/stats/TestStatsUtils.java
    --- a/ql/src/test/org/apache/hadoop/hive/ql/stats/TestStatsUtils.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/ql/src/test/org/apache/hadoop/hive/ql/stats/TestStatsUtils.java	(date 1666964794855)
    @@ -31,7 +31,7 @@
     import org.apache.hadoop.hive.ql.plan.ColStatistics.Range;
     import org.apache.hadoop.hive.serde.serdeConstants;
     import org.junit.Test;
    -import org.spark_project.guava.collect.Sets;
    +import org.sparkproject.guava.collect.Sets;
     
     public class TestStatsUtils {
     
    Index: spark-client/src/main/java/org/apache/hive/spark/counter/SparkCounter.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/spark-client/src/main/java/org/apache/hive/spark/counter/SparkCounter.java b/spark-client/src/main/java/org/apache/hive/spark/counter/SparkCounter.java
    --- a/spark-client/src/main/java/org/apache/hive/spark/counter/SparkCounter.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/spark-client/src/main/java/org/apache/hive/spark/counter/SparkCounter.java	(date 1666965003163)
    @@ -19,15 +19,14 @@
     
     import java.io.Serializable;
     
    -import org.apache.spark.Accumulator;
    -import org.apache.spark.AccumulatorParam;
     import org.apache.spark.api.java.JavaSparkContext;
    +import org.apache.spark.util.LongAccumulator;
     
     public class SparkCounter implements Serializable {
     
       private String name;
       private String displayName;
    -  private Accumulator<Long> accumulator;
    +  private LongAccumulator accumulator;
     
       // Values of accumulators can only be read on the SparkContext side. This field is used when
       // creating a snapshot to be sent to the RSC client.
    @@ -55,9 +54,9 @@
     
         this.name = name;
         this.displayName = displayName;
    -    LongAccumulatorParam longParam = new LongAccumulatorParam();
    +    // LongAccumulatorParam longParam = new LongAccumulatorParam();
         String accumulatorName = groupName + "_" + name;
    -    this.accumulator = sparkContext.accumulator(initValue, accumulatorName, longParam);
    +    this.accumulator = sparkContext.sc().longAccumulator(accumulatorName);
       }
     
       public long getValue() {
    @@ -88,22 +87,21 @@
         return new SparkCounter(name, displayName, accumulator.value());
       }
     
    -  class LongAccumulatorParam implements AccumulatorParam<Long> {
    -
    -    @Override
    -    public Long addAccumulator(Long t1, Long t2) {
    -      return t1 + t2;
    -    }
    -
    -    @Override
    -    public Long addInPlace(Long r1, Long r2) {
    -      return r1 + r2;
    -    }
    -
    -    @Override
    -    public Long zero(Long initialValue) {
    -      return 0L;
    -    }
    -  }
    -
    +  // class LongAccumulatorParam implements AccumulatorParam<Long> {
    +  //
    +  //   @Override
    +  //   public Long addAccumulator(Long t1, Long t2) {
    +  //     return t1 + t2;
    +  //   }
    +  //
    +  //   @Override
    +  //   public Long addInPlace(Long r1, Long r2) {
    +  //     return r1 + r2;
    +  //   }
    +  //
    +  //   @Override
    +  //   public Long zero(Long initialValue) {
    +  //     return 0L;
    +  //   }
    +  // }
     }
    Index: spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java b/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java
    --- a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java	(date 1666964853331)
    @@ -47,8 +47,8 @@
       }
     
       public ShuffleWriteMetrics(TaskMetrics metrics) {
    -    this(metrics.shuffleWriteMetrics().shuffleBytesWritten(),
    -      metrics.shuffleWriteMetrics().shuffleWriteTime());
    +    this(metrics.shuffleWriteMetrics().bytesWritten(),
    +      metrics.shuffleWriteMetrics().bytesWritten());
       }
     
     }
    Index: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
    --- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java	(date 1666964377279)
    @@ -17,6 +17,7 @@
      */
     package org.apache.hadoop.hive.ql.exec.tez;
     
    +import com.google.common.util.concurrent.MoreExecutors;
     import org.apache.hadoop.hive.metastore.api.WMPoolSchedulingPolicy;
     import org.apache.hadoop.hive.metastore.utils.MetaStoreUtils;
     
    @@ -1092,7 +1093,7 @@
       }
     
       private void failOnFutureFailure(ListenableFuture<?> future) {
    -    Futures.addCallback(future, FATAL_ERROR_CALLBACK);
    +    Futures.addCallback(future, FATAL_ERROR_CALLBACK, MoreExecutors.directExecutor());
       }
     
       private void queueGetRequestOnMasterThread(
    @@ -1925,7 +1926,7 @@
     
         public void start() throws Exception {
           ListenableFuture<WmTezSession> getFuture = tezAmPool.getSessionAsync();
    -      Futures.addCallback(getFuture, this);
    +      Futures.addCallback(getFuture, this, MoreExecutors.directExecutor());
         }
     
         @Override
    @@ -1979,7 +1980,7 @@
           case GETTING: {
             ListenableFuture<WmTezSession> waitFuture = session.waitForAmRegistryAsync(
                 amRegistryTimeoutMs, timeoutPool);
    -        Futures.addCallback(waitFuture, this);
    +        Futures.addCallback(waitFuture, this, MoreExecutors.directExecutor());
             break;
           }
           case WAITING_FOR_REGISTRY: {
    Index: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidScanQueryRecordReader.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidScanQueryRecordReader.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidScanQueryRecordReader.java
    --- a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidScanQueryRecordReader.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidScanQueryRecordReader.java	(date 1666964400751)
    @@ -28,6 +28,7 @@
     import com.google.common.collect.Iterators;
     
     import java.io.IOException;
    +import java.util.Collections;
     import java.util.Iterator;
     import java.util.List;
     
    @@ -43,7 +44,7 @@
     
       private ScanResultValue current;
     
    -  private Iterator<List<Object>> compactedValues = Iterators.emptyIterator();
    +  private Iterator<List<Object>> compactedValues = Collections.emptyIterator();
     
       @Override
       protected JavaType getResultTypeDef() {
    Index: ql/src/test/org/apache/hadoop/hive/ql/exec/tez/SampleTezSessionState.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/SampleTezSessionState.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/SampleTezSessionState.java
    --- a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/SampleTezSessionState.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/SampleTezSessionState.java	(date 1666964106343)
    @@ -22,6 +22,7 @@
     import com.google.common.util.concurrent.Futures;
     import com.google.common.util.concurrent.FutureCallback;
     import com.google.common.util.concurrent.ListenableFuture;
    +import com.google.common.util.concurrent.MoreExecutors;
     import com.google.common.util.concurrent.SettableFuture;
     import java.io.IOException;
     import java.util.concurrent.ScheduledExecutorService;
    @@ -128,7 +129,7 @@
           public void onFailure(Throwable t) {
             future.setException(t);
           }
    -    });
    +    }, MoreExecutors.directExecutor());
         return future;
       }
     
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DateColumnStatsMerger.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DateColumnStatsMerger.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DateColumnStatsMerger.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DateColumnStatsMerger.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DateColumnStatsMerger.java	(date 1666966498503)
    @@ -24,13 +24,13 @@
     import org.apache.hadoop.hive.metastore.api.Date;
     import org.apache.hadoop.hive.metastore.columnstats.cache.DateColumnStatsDataInspector;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.dateInspectorFromStats;
    +
     public class DateColumnStatsMerger extends ColumnStatsMerger {
       @Override
       public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj newColStats) {
    -    DateColumnStatsDataInspector aggregateData =
    -        (DateColumnStatsDataInspector) aggregateColStats.getStatsData().getDateStats();
    -    DateColumnStatsDataInspector newData =
    -        (DateColumnStatsDataInspector) newColStats.getStatsData().getDateStats();
    +    DateColumnStatsDataInspector aggregateData = dateInspectorFromStats(aggregateColStats);
    +    DateColumnStatsDataInspector newData = dateInspectorFromStats(newColStats);
         Date lowValue = aggregateData.getLowValue().compareTo(newData.getLowValue()) < 0 ? aggregateData
             .getLowValue() : newData.getLowValue();
         aggregateData.setLowValue(lowValue);
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DoubleColumnStatsMerger.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DoubleColumnStatsMerger.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DoubleColumnStatsMerger.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DoubleColumnStatsMerger.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DoubleColumnStatsMerger.java	(date 1666966498503)
    @@ -23,13 +23,13 @@
     import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;
     import org.apache.hadoop.hive.metastore.columnstats.cache.DoubleColumnStatsDataInspector;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.doubleInspectorFromStats;
    +
     public class DoubleColumnStatsMerger extends ColumnStatsMerger {
       @Override
       public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj newColStats) {
    -    DoubleColumnStatsDataInspector aggregateData =
    -        (DoubleColumnStatsDataInspector) aggregateColStats.getStatsData().getDoubleStats();
    -    DoubleColumnStatsDataInspector newData =
    -        (DoubleColumnStatsDataInspector) newColStats.getStatsData().getDoubleStats();
    +    DoubleColumnStatsDataInspector aggregateData = doubleInspectorFromStats(aggregateColStats);
    +    DoubleColumnStatsDataInspector newData = doubleInspectorFromStats(newColStats);
         aggregateData.setLowValue(Math.min(aggregateData.getLowValue(), newData.getLowValue()));
         aggregateData.setHighValue(Math.max(aggregateData.getHighValue(), newData.getHighValue()));
         aggregateData.setNumNulls(aggregateData.getNumNulls() + newData.getNumNulls());
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/LongColumnStatsMerger.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/LongColumnStatsMerger.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/LongColumnStatsMerger.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/LongColumnStatsMerger.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/LongColumnStatsMerger.java	(date 1666966498503)
    @@ -23,13 +23,13 @@
     import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;
     import org.apache.hadoop.hive.metastore.columnstats.cache.LongColumnStatsDataInspector;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.longInspectorFromStats;
    +
     public class LongColumnStatsMerger extends ColumnStatsMerger {
       @Override
       public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj newColStats) {
    -    LongColumnStatsDataInspector aggregateData =
    -        (LongColumnStatsDataInspector) aggregateColStats.getStatsData().getLongStats();
    -    LongColumnStatsDataInspector newData =
    -        (LongColumnStatsDataInspector) newColStats.getStatsData().getLongStats();
    +    LongColumnStatsDataInspector aggregateData = longInspectorFromStats(aggregateColStats);
    +    LongColumnStatsDataInspector newData = longInspectorFromStats(newColStats);
         aggregateData.setLowValue(Math.min(aggregateData.getLowValue(), newData.getLowValue()));
         aggregateData.setHighValue(Math.max(aggregateData.getHighValue(), newData.getHighValue()));
         aggregateData.setNumNulls(aggregateData.getNumNulls() + newData.getNumNulls());
    Index: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/StringColumnStatsMerger.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/StringColumnStatsMerger.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/StringColumnStatsMerger.java
    --- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/StringColumnStatsMerger.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/StringColumnStatsMerger.java	(date 1666966498503)
    @@ -23,13 +23,13 @@
     import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;
     import org.apache.hadoop.hive.metastore.columnstats.cache.StringColumnStatsDataInspector;
     
    +import static org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.stringInspectorFromStats;
    +
     public class StringColumnStatsMerger extends ColumnStatsMerger {
       @Override
       public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj newColStats) {
    -    StringColumnStatsDataInspector aggregateData =
    -        (StringColumnStatsDataInspector) aggregateColStats.getStatsData().getStringStats();
    -    StringColumnStatsDataInspector newData =
    -        (StringColumnStatsDataInspector) newColStats.getStatsData().getStringStats();
    +    StringColumnStatsDataInspector aggregateData = stringInspectorFromStats(aggregateColStats);
    +    StringColumnStatsDataInspector newData = stringInspectorFromStats(newColStats);
         aggregateData.setMaxColLen(Math.max(aggregateData.getMaxColLen(), newData.getMaxColLen()));
         aggregateData.setAvgColLen(Math.max(aggregateData.getAvgColLen(), newData.getAvgColLen()));
         aggregateData.setNumNulls(aggregateData.getNumNulls() + newData.getNumNulls());
    Index: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java b/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
    --- a/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java	(date 1666964214403)
    @@ -744,15 +744,16 @@
           }, 10000L, TimeUnit.MILLISECONDS);
     
           nodeEnablerFuture = nodeEnabledExecutor.submit(nodeEnablerCallable);
    -      Futures.addCallback(nodeEnablerFuture, new LoggingFutureCallback("NodeEnablerThread", LOG));
    +      Futures.addCallback(nodeEnablerFuture, new LoggingFutureCallback("NodeEnablerThread", LOG), MoreExecutors.directExecutor());
     
           delayedTaskSchedulerFuture =
               delayedTaskSchedulerExecutor.submit(delayedTaskSchedulerCallable);
           Futures.addCallback(delayedTaskSchedulerFuture,
    -          new LoggingFutureCallback("DelayedTaskSchedulerThread", LOG));
    +          new LoggingFutureCallback("DelayedTaskSchedulerThread", LOG),
    +          MoreExecutors.directExecutor());
     
           schedulerFuture = schedulerExecutor.submit(schedulerCallable);
    -      Futures.addCallback(schedulerFuture, new LoggingFutureCallback("SchedulerThread", LOG));
    +      Futures.addCallback(schedulerFuture, new LoggingFutureCallback("SchedulerThread", LOG), MoreExecutors.directExecutor());
     
           registry.start();
           registry.registerStateChangeListener(new NodeStateChangeListener());
    Index: llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java
    IDEA additional info:
    Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
    <+>UTF-8
    ===================================================================
    diff --git a/llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java b/llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java
    --- a/llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java	(revision 4df4d75bf1e16fe0af75aad0b4179c34c07fc975)
    +++ b/llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java	(date 1666964261603)
    @@ -171,7 +171,7 @@
               CallableRequest<T, U> request, LlapNodeId nodeId) {
             ListenableFuture<U> future = executor.submit(request);
             Futures.addCallback(future, new ResponseCallback<U>(
    -            request.getCallback(), nodeId, this));
    +            request.getCallback(), nodeId, this), MoreExecutors.directExecutor());
           }
     
           @VisibleForTesting
    @@ -283,7 +283,7 @@
               LOG.warn("RequestManager shutdown with error", t);
             }
           }
    -    });
    +    }, MoreExecutors.directExecutor());
       }
     
       @Override

```

### 6.3 解压安装

```bash
    cd packaging/target || exit                                                # 进入编译后的打包路径
    tar -zxvf apache-hive-3.1.3-bin.tar.gz -C /opt/apache/                     # 解压编译后的压缩包
    mv /opt/apache/apache-hive-3.1.3-bin /opt/apache/hive                      # 修改目录名称
    mkdir -p /opt/apache/hive/logs/                                            # 创建日志目录
```

### 6.4 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Hive 3.1.3 ====================================== #
        export HIVE_HOME=/opt/apache/zookeeper
        export PATH=${PATH}:${HIVE_HOME}/bin
        
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 6.5 修改配置文件

#### 6.5.1 创建配置文件

```bash
    cd /opt/apache/hive || exit                                                # 切换到 hive 安装目录
    touch ${HIVE_HOME}/conf/hive-env.sh                                        # hive 环境参数配置
    touch ${HIVE_HOME}/conf/hive-site.xml                                      # hive 主要配置文件
    touch ${HIVE_HOME}/conf/beeline-site.xml                                   # beeline 客户端配置文件
    mv ${HIVE_HOME}/conf/hive-log4j2.properties.template ${HIVE_HOME}/conf/hive-log4j2.properties
```

#### 6.5.2 修改 ${HIVE_HOME}/conf/hive-env.sh

```bash
    export HADOOP_HEAPSIZE=2048
    
    # Hadoop 安装路径
    export HADOOP_HOME=/opt/apache/hadoop
    
    # Hive 配置文件路径
    export HIVE_CONF_DIR=/opt/apache/hive/conf
    
    # Hive jar 包路径
    export HIVE_AUX_JARS_PATH=/opt/apache/hive/lib
```

#### 6.5.3 修改 ${HIVE_HOME}/conf/hive-site.xml

```xml
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <!-- 记录 Hive 中的元数据信息在 mysql 中 -->
        <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://master:3306/hive?serverTimezone=UTC&amp;createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;allowPublicKeyRetrieval=true</value>
            <description>连接数据库用户名称</description>
        </property>
        <!-- jdbc mysql驱动 -->
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>com.mysql.cj.jdbc.Driver</value>
            <description>连接数据库驱动</description>
        </property>
        <!-- mysql 的用户名和密码 -->
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>issac</value>
            <description>连接数据库用户名称</description>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>111111</value>
            <description>连接数据库用户密码</description>
        </property>
        
        <!-- Hive 元数据存储版本的验证
        <property>
            <name>hive.metastore.schema.verification</name>
            <value>false</value>
        </property>    
        -->
    
        <!-- 元数据读取不到  -->    
        <property>
            <name>metastore.storage.schema.reader.impl</name>
            <value>org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader</value>
        </property>    
         <!-- 元数据存储授权  -->
        <property>
            <name>hive.metastore.event.db.notification.api.auth</name>
            <value>false</value>
        </property>
    
        <!-- 自动创建相关数据 -->
        <property>
            <name>datanucleus.fixedDatastore</name>
            <value>false</value>
        </property>
        <property>
            <name>datanucleus.readOnlyDatastore</name>
            <value>false</value>
        </property>
        <property>
            <name>datanucleus.schema.autoCreateAll</name>
            <value>true</value>
        </property>
        <property>
            <name>datanucleus.autoCreateSchema</name>
            <value>true</value>
        </property>
        <property>
            <name>datanucleus.autoCreateTables</name>
            <value>true</value>
        </property>
        <property>
            <name>datanucleus.autoCreateColumns</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.metastore.local</name>
            <value>true</value>
        </property>
        <!-- 显示表的列名 -->
        <property>
            <name>hive.cli.print.header</name>
            <value>true</value>
            <description>客户端显示当前查询表的头信息</description>
        </property>
        <!-- 显示数据库名称 -->
        <property>
            <name>hive.cli.print.current.db</name>
            <value>true</value>
            <description>客户端显示当前数据库名称信息</description>
        </property>
        <!-- hdfs 位置 -->
        <property>
            <name>hive.metastore.warehouse.dir</name>
            <value>/hive/data</value>
            <description>hdfs 上 hive 数据存放位置</description>
        </property>
        
        <property>
            <name>hive.exec.scratchdir</name>
            <value>/hive/tmp</value>
            <description>Hive 作业的 HDFS 根目录位置</description>
        </property>
            
        <property>
            <name>hive.scratch.dir.permission</name>
            <value>777</value>
            <description>Hive 作业的 HDFS 根目录创建写权限</description>
        </property>
        
        <!-- 日志目录
        <property>
            <name>hive.querylog.location</name>
            <value>/hive/log</value>
        </property>
         -->
        <!-- 设置 metastore 的节点信息 -->
        <property>
            <name>hive.metastore.uris</name>
            <value>thrift://master:9083</value>
            <description>主节点元数据服务</description>
        </property>
        
        <!-- 客户端远程连接的端口 -->
        <property> 
            <name>hive.server2.thrift.port</name> 
            <value>10000</value>
        </property>
        <property> 
            <name>hive.server2.thrift.bind.host</name> 
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>hive.server2.webui.host</name>
            <value>0.0.0.0</value>
        </property>
        
        <!-- hive 服务的页面的端口 -->
        <property>
            <name>hive.server2.webui.port</name>
            <value>10002</value>
        </property>
         
        <property> 
            <name>hive.server2.long.polling.timeout</name> 
            <value>5000</value>
        </property>
        
        <property>
            <name>hive.server2.enable.doAs</name>
            <value>true</value>
        </property>
        
        <property>
            <name>datanucleus.autoCreateSchema</name>
            <value>false</value>
        </property>
        
        <property>
            <name>datanucleus.fixedDatastore</name>
            <value>true</value>
        </property>
        
        <property>
            <name>hive.server2.thrift.client.user</name>
            <value>issac</value>
            <description>Username to use against thrift client</description>
        </property>
        <property>
            <name>hive.server2.thrift.client.password</name>
            <value>111111</value>
            <description>Password to use against thrift client</description>
        </property>
        
        <!-- Spark 依赖位置（注意：端口号 9000 必须和 namenode 的端口号一致） -->
        <property>
            <name>spark.yarn.jars</name>
            <value>hdfs://master:9000/spark/jars/*</value>
        </property>
        
        <!-- Hive 执行引擎 -->
        <property>
            <name>hive.execution.engine</name>
            <value>spark</value>
        </property>
        <property>
            <name>hive.enable.spark.execution.engine</name>
            <value>true</value>
        </property>
        <!--
        <property>
            <name>spark.master</name>
            <value>yarn-client</value>
        </property>  
        -->
        <!-- 连接超时问题 -->
        <property>
            <name>hive.spark.client.connect.timeout</name>
            <value>900000</value>
        </property>
        <property>
            <name>hive.spark.client.server.connect.timeout</name>
            <value>900000</value>
        </property>
        <property>
            <name>hive.fetch.task.conversion</name>
            <value>more</value>
            <description>
                0. none    : disable hive.fetch.task.conversion
                1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
                2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)
            </description>
        </property>
    </configuration>
```

#### 6.5.4 修改 ${HIVE_HOME}/conf/beeline-site.xml

```xml
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>beeline.hs2.jdbc.url.tcpUrl</name>
            <value>jdbc:hive2://master:10000/default;user=issac;password=111111</value>
        </property>
        <property>
            <name>beeline.hs2.jdbc.url.httpUrl</name>
            <value>jdbc:hive2://master:10000/default;user=issac;password=111111;transportMode=http;httpPath=cliservice</value>
        </property>
        <property>
            <name>beeline.hs2.jdbc.url.default</name>
            <value>tcpUrl</value>
        </property>
    </configuration>
```

#### 6.5.4 修改 ${HIVE_HOME}/conf/hive-log4j2.properties

```properties
    property.hive.log.dir = /opt/apache/hive/logs                              # 修改日志存储路径
```

#### 6.5.5 修改 ${HIVE_HOME}/conf/hive-site.xml、${HIVE_HOME}/conf/spark-defaults.conf

```bash
    cp ${HIVE_HOME}/conf/hive-site.xml ${SPARK_HOME}/conf/                     # 复制 hive 配置到 spark 配置目录，用于搭建 hive on spark 
    cp ${SPARK_HOME}/conf/spark-defaults.conf ${HIVE_HOME}/conf/               # 复制 spark 配置到 hive 配置目录，用于搭建 hive on spark
```

### 6.5 初始化元数据

### 6.5.1 Mysql 连接数据库驱动下载  

在 **[Maven 仓库](https://mvnrepository.com/)** 搜索并下载 **[Mysql jdbc 驱动](https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.31/mysql-connector-java-8.0.31.jar)** 到本地

### 6.5.2 复制数据库驱动到 hive 依赖

```bash
    cp mysql-connector-java-8.0.31.jar ${HIVE_HOME}/lib/
```

#### 6.5.3 启动 Mysql 创建相关数据库

```bash
    /opt/mysql/bin/mysql.sh start                                              # 启动数据库
    mycli -h issac -P 3306 -u root -p 111111 -D mysql                          # 连接数据库
    
    create database if not exists hive;                                        # 创建 hive  数据库
    grant all privileges on hive.*  to 'issac'@'%';                            # 将数据库 hive 的所有权限给用户 issac
```

#### 6.5.4 初始化元数据

```bash
    cd /opt/apache/hive/ || exit                                               # 切换到 hive 安装目录
    ${HIVE_HOME}/bin/schematool -dbType mysql -initSchema -verbose             # 初始化元数据
```

#### 6.5.5 解决 hive 中文乱码

```mysql
    -- 登录 mysql，切换到 hive 数据库
    use hive;

    -- 修改字段注释字符集
    alter table COLUMNS_V2       modify column COMMENT      varchar(256)   character set utf8mb4;

    -- 修改表注释字符集
    alter table TABLE_PARAMS     modify column PARAM_VALUE  varchar(10000)  character set utf8mb4;

    -- 修改分区参数，支持分区建用中文表示
    alter table PARTITION_PARAMS modify column PARAM_VALUE  varchar(10000) character set utf8mb4;
    alter table PARTITION_KEYS   modify column PKEY_COMMENT varchar(10000) character set utf8mb4;

    -- 修改索引名注释，支持中文表示
    alter table INDEX_PARAMS     modify column PARAM_VALUE  varchar(4000)  character set utf8mb4;

    -- 修改视图，支持视图中文
    alter table TBLS modify COLUMN VIEW_EXPANDED_TEXT       mediumtext     character set utf8mb4;
    alter table TBLS modify COLUMN VIEW_ORIGINAL_TEXT       mediumtext     character set utf8mb4;

    -- 刷新权限
    flush privileges;
```

### 6.6 Hive 启停脚本：${HIVE_HOME}/bin/hive.sh 

```bash
    #!/usr/bin/env bash
    
    SERVICE_DIR=$(cd "$(dirname "$0")/../" || exit; pwd)       # 服务安装路径
    SERVICE_NAME=Hive                                          # 服务名称
    JUDGE_NAME=org.apache.hadoop.util.RunJar                   # 运行 jar 
    
    HiveServer2_PORT=10002                                     # HiveServer2 端口号
    BEELIN_PORT=10000                                          # Beelin 端口号
    LOG_FILE="$(date +%F-%H-%M-%S).log"                        # 操作日志
    
    
    # beeline -u jdbc:hive2://issac:10000 -n issac
    printf "\n=========================================================================\n"
    #  匹配输入参数
    case "$1" in
        #  1. 运行程序
        start)
            # 1.1 统计正在运行程序的 pid 的个数
            pid_list=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | grep -v "$0" | awk '{print $2}' | awk -F "_" '{print $1}')
            
            #  1.2 若 pid 个数为 0，则运行程序，否则打印程序正在运行
            if [ ! "${pid_list}" ]; then
                echo "    程序（${SERVICE_NAME}）正在加载中 ......"
                nohup "${SERVICE_DIR}/bin/hive" --service metastore >> "${SERVICE_DIR}/logs/${LOG_FILE}" 2>&1 &
                
                sleep 3
                echo "    程序（${SERVICE_NAME}）启动验证中 ......"
                
                nohup "${SERVICE_DIR}/bin/hiveserver2" >> "${SERVICE_DIR}/logs/${LOG_FILE}" 2>&1 &
                sleep 15
                
                # 1.3 判断程序启动是否成功
                # 1.5 判断所有程序启动是否成功
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | grep -v "$0" | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${pid_count}" -ge 2 ]; then
                    echo "    程序（${SERVICE_NAME}）启动成功 ...... "
                else
                    echo "    程序（${SERVICE_NAME}）启动失败 ...... "
                fi
            else
                echo "    程序（${SERVICE_NAME}）正在运行当中 ...... "
            fi
        ;;
        
        #  2. 停止    
        stop)
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | grep -v "$0" | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            if [ "${pid_count}" -eq 0 ]; then
                echo "    ${SERVICE_NAME} 的进程不存在，程序没有运行 ...... "
            elif [ "${pid_count}" -eq 2 ]; then
                # 2.2 杀死进程，关闭程序
                echo "    程序（${SERVICE_NAME}）正在停止 ......"    
                pid_list=program_count=$(ps -aux | grep $SERVICE_NAME | grep -v "$0" | grep -v grep | awk '{print $2}' | xargs kill -term)
                sleep 5
                
                # 2.3 若还未关闭，则强制杀死进程，关闭程序
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | wc -l)
                if [ "${pid_count}" -ge 1 ]; then
                    temp=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | xargs kill -9)
                fi
                            
                echo "    程序（${SERVICE_NAME}）已经停止成功 ......"            
            else
                echo "    程序（${SERVICE_NAME}）运行出现问题 ......"
            fi
        ;;
        #  3. 状态查询
        status)
            # 3.1 查看正在运行程序的 pid
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | grep -v "$0" | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            #  3.2 判断 ES 运行状态
            if [ "${pid_count}" -eq 0 ]; then
                echo "    程序（${SERVICE_NAME}）已经停止 ...... "
            elif [ "${pid_count}" -eq 2 ]; then
                echo "    程序（${SERVICE_NAME}）正在运行中 ...... "
            else
                echo "    程序（${SERVICE_NAME}）运行出现问题 ...... "
            fi
        ;;
        
        
        #  4. 重启程序
        restart)
            "$0" stop
            sleep 2
            "$0" start
        ;;
        
        
        #  5. 其它情况
        *)
            echo "    脚本可传入一个参数，如下所示：              "
            echo "        +-----------------------------------+ "
            echo "        |  start | stop | restart | status  | "
            echo "        +-----------------------------------+ "
            echo "        |        start    ：  启动服务      | "
            echo "        |        stop     ：  关闭服务      | "
            echo "        |        restart  ：  重启服务      | "
            echo "        |        status   ：  查看状态      | "
            echo "        +-----------------------------------+ "
        ;;
    esac
    printf "=========================================================================\n\n"
```

### 6.7 同步 hive 安装路径，分发到其他节点 

```bash
    cd /opt/apache/                                                            # 切换到 hive 安装父路径
    ~/shell/xync.sh hive                                                       # 同步 hive 到其它节点
````

### 6.8 启动 Hive 

```bash
    # 切换到 hive 安装目录
    cd /opt/apache/hive/ || exit
    
    # 单个服务一次启动，分别启动 metastore 和 hiveserver2 
    nohup ${HIVE_HOME}/bin/hive --service metastore >> ${HIVE_HOME}/logs/${LOG_FILE} 2>&1 &
    nohup ${HIVE_HOME}/bin/hiveserver2              >> ${HIVE_HOME}/logs/${LOG_FILE} 2>&1 &
    
    # 自定义脚本启动
    ${HIVE_HOME}/bin/hive.sh start

    # 查看 hive 启动状态
    jps -l 
    http://issac:10002/
```

### 6.9 测试 Hive

```mysql
    -- 使用 datagrip 连接上 hive 
    
    show databases;                                                            -- 查看所有数据库
    create database if not exists test;                                        -- 创建 test 数据库
    use test;                                                                  -- 切换到 test 数据库
    create table if not exists student                                        -- 创建 test 数据库
    (
        id     int           comment '主键 ID',
        name   varchar(64)   comment '姓名',
        age    int           comment '年龄',
        gender int           comment '性别：-1，未知；0，女；1：男',
        hight  float         comment '身高：厘米',
        wight  float         comment '体重：千克',
        email  varchar(128)  comment '电子邮件',
        remark varchar(1024) comment '备注'
    ) comment '学生测试表';

    set mapreduce.map.java.opts='-Xmx4096m';                                   -- 设置 map 堆内存
    set mapreduce.reduce.java.opts='-Xms4096m';                                -- 设置 reduce 堆内存
    
    -- 测试 mr 引擎
    set hive.execution.engine=mr;
    insert into student (id, name, age, gender, hight, wight, email, remark) values (1, '张三', 33, 1, 172.1, 48.9, 'zhangsan@qq.com', '学生');
    
    -- 测试 spark 引擎
    set hive.execution.engine=spark;
    insert into student (id, name, age, gender, hight, wight, email, remark) values (2, '李四', 23, 0, 165.1, 53.9, 'lisi@qq.com', '学生');
    insert into student (id, name, age, gender, hight, wight, email, remark) values (3, '王五', 28, 1, 168.3, 52.7, 'wangwu@qq.com', '学生');

    -- 测试执行计划
    explain formatted select * from student;
    select * from student limit 10;
```

## 7. HBase 安装

### 7.1 HBase-2.4.15 下载

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[hbase-2.4.16](https://mirrors.aliyun.com/apache/hbase/2.4.15/hbase-2.4.16-bin.tar.gz)** 安装包到本地

### 7.2 解压安装 hbase

```bash
    tar -zxvf hbase-2.4.15-bin.tar.gz -C /opt/apache/                          # 解压 压缩包
    mv /opt/apache/hbase-2.4.15-bin /opt/apache/hbase                          # 修改目录名称
    mkdir -p /opt/apache/hbase/data/                                           # 创建数据存储目录
    mkdir -p /opt/apache/hbase/logs/                                           # 创建日志目录
```

### 7.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Hbase 3.2.3 ====================================== #
        export HBASE_HOME=/opt/apache/hbase
        export PATH=${PATH}:${HBASE_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 7.4 配置文件修改

#### 7.4.1 修改 ${HBASE_HOME}/conf/hbase-env.sh

```bash
    export JAVA_HOME=/opt/java/jdk-08
    export HBASE_HEAPSIZE=2G
    export HBASE_OFFHEAPSIZE=2G
    export HBASE_PID_DIR=/opt/apache/hbase/data
    export HBASE_MANAGES_ZK=false
```

#### 7.4.2 修改 ${HBASE_HOME}/conf/hbase-site.xml

```xml
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    
    <!-- See also https://hbase.apache.org/book.html#standalone_dist -->
    <configuration>
        <!-- 指定 HBase 在 HDFS 上存储的路径 -->
        <property>
            <name>hbase.rootdir</name>
            <value>hdfs://master:9000/hbase</value>
        </property>
        <!-- 指定 HBase 是否分布式运行 -->
        <property>
            <name>hbase.cluster.distributed</name>
            <value>true</value>
        </property>
        <!-- 在分布式的情况下一定要设置，不然容易出现 Hmaster 起不来的情况 -->
        <property>
            <name>hbase.unsafe.stream.capability.enforce</name>
            <value>false</value>
        </property>
        <!-- 指定 zookeeper 的地址，多个用 "," 分割 -->
        <property>
            <name>hbase.zookeeper.quorum</name>
            <value>slaver1,slaver2,slaver3</value>
        </property>
        <!-- 指定在 zookeeper 存储路径 -->
        <!--
        <property>
            <name>hbase.zookeeper.property.dataDir</name>
            <value>/opt/apache/zookeeper/data</value>
        </property>
        -->
        <!-- 连接 zookeeper 的端口号 -->
        <property>
            <name>hbase.zookeeper.property.clientPort</name>
            <value>2181</value>
        </property>
        <!-- 指定 HBase 管理页面 -->
        <property>
            <name>hbase.master.info.port</name>
            <value>16010</value>
        </property>
    
        <!--  建立二级索引，将业务需要的数据联立建立索引，方便查询 -->
        <property>
            <name>hbase.regionserver.wal.codec</name>
            <value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
        </property>
    
        <!-- 如果使用了 hbase 中的自定义 namespace，不仅仅使用 default -->
        <!-- 那么在 phoenix 中与之对应的是 schema 的概念，但是默认并没有开启，需要在增加以下配置项 -->
        <property>
            <name>phoenix.schema.isNamespaceMappingEnabled</name>
            <value>true</value>
        </property>
    
        <property>
            <name>phoenix.schema.mapSystemTablesToNamespace</name>
            <value>true</value>
        </property>
    
        <!-- 用户可以创建临时或永久的用户自定义函数。 -->
        <!-- 这些用户自定义函数可以像内置的 create、upsert、delete 一样被调用 -->
        <property>
            <name>phoenix.functions.allowUserDefinedFunctions</name>
            <value>true</value>
            <description>enable UDF functions</description>
        </property>
    </configuration>
```

#### 7.4.3 修改 ${HBASE_HOME}/conf/regionservers

```bash
    cd /opt/apache/hbase/ || exit                                              # 切换的 hbase 安装路径
    echo "slaver1" >  ${HBASE_HOME}/conf/regionservers                         # 将主机 slaver1 添加到 RegionServer
    echo "slaver2" >> ${HBASE_HOME}/conf/regionservers                         # 将主机 slaver2 添加到 RegionServer
    echo "slaver3" >> ${HBASE_HOME}/conf/regionservers                         # 将主机 slaver3 添加到 RegionServer
```

### 7.5 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 hbase 安装父路径
    ~/shell/xync.sh hbase                                                      # 同步 hbase 到其它节点
````

### 7.6 编写 hbase 启停脚本：${HBASE_HOME}/bin/hbase.sh

```bashpro shell script
    #!/usr/bin/env bash
    
    SERVICE_DIR=$(cd "$(dirname "$0")/../" || exit; pwd)
    SERVICE_NAME=HBase
    JUDGE_NAME=org.apache.hadoop.hbase
    
    HBASE_MASTER_PORT=60010
    Region_Server_PORT=16030
    
    HBASE_MASTER=org.apache.hadoop.hbase.master.HMaster
    Region_Server=org.apache.hadoop.hbase.regionserver.HRegionServer
    
    
    printf "\n=========================================================================\n"
    #  匹配输入参数
    case "$1" in
        #  1. 运行程序
        start)
            # 1.1 查找程序的 pid
            pid_list=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}')
            
            #  1.2 若 pid 不存在，则运行程序，否则打印程序运行状态
            if [ ! "${pid_list}" ]; then
                echo "    程序 ${SERVICE_NAME} 正在加载中 ......"
                "${SERVICE_DIR}/bin/start-hbase.sh" > /dev/null 2>&1
                sleep 2
                
                # 1.3 判断程序 HMaster 启动是否成功
                master_pid=$(ps -aux | grep -i ${HBASE_MASTER} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}')
                if [ ! "${master_pid}" ]; then
                    echo "    程序 HMaster 启动失败 ...... "
                fi
                
                # 1.4 判断程序 HRegionServer 启动是否成功
                region_pid=$(ps -aux | grep -i ${Region_Server} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}')
                if [ ! "${region_pid}" ]; then
                    echo "    程序 HRegionServer 启动失败 ...... "
                fi
                
                # 1.5 判断所有程序启动是否成功
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
                if [ "${pid_count}" -ge 2 ]; then
                    echo "    程序 ${SERVICE_NAME} 启动成功 ...... "
                else
                    echo "    程序 ${SERVICE_NAME} 启动失败 ...... "
                fi
                
            else
                echo "    程序 ${SERVICE_NAME} 正在运行当中 ...... "
            fi
        ;;
        
        
        #  2. 停止
        stop)
            # 2.1 根据程序的 pid 查询程序运行状态
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            if [ "${pid_count}" -eq 0 ]; then
                echo "    ${SERVICE_NAME} 的进程不存在，程序没有运行 ...... "
            elif [ "${pid_count}" -eq 2 ]; then
                # 2.2 杀死进程，关闭程序
                echo "    程序 ${SERVICE_NAME} 正在停止 ......"
                
                "${SERVICE_DIR}/bin/stop-hbase.sh" > /dev/null 2>&1
                sleep 2
    
                # 2.3 若还未关闭，则强制杀死进程，关闭程序
                pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | wc -l)
                if [ "${pid_count}" -ge 1 ]; then
                    temp=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | xargs kill -9)
                fi
                
                echo "    程序 ${SERVICE_NAME} 已经停止成功 ......"            
            else
                echo "    程序 ${SERVICE_NAME} 运行出现问题 ......"
            fi
        ;;
        
        
        #  3. 状态查询
        status)
            # 3.1 查看正在运行程序的 pid
            pid_count=$(ps -aux | grep -i ${JUDGE_NAME} | grep -v grep | awk '{print $2}' | awk -F "_" '{print $1}' | wc -l)
            #  3.2 判断 ES 运行状态
            if [ "${pid_count}" -eq 0 ]; then
                echo "    程序 ${SERVICE_NAME} 已经停止 ...... "
            elif [ "${pid_count}" -eq 2 ]; then
                echo "    程序 ${SERVICE_NAME} 正在运行中 ...... "
            else
                echo "    程序 ${SERVICE_NAME} 运行出现问题 ...... "
            fi
        ;;
        
        
        #  4. 重启程序
        restart)
            "$0" stop
        sleep 3
            "$0" start
        ;;
        
        
        #  5. 其它情况
        *)
            echo "    脚本可传入一个参数，如下所示：              "
            echo "        +-----------------------------------+ "
            echo "        |  start | stop | restart | status  | "
            echo "        +-----------------------------------+ "
            echo "        |        start    ：  启动服务      | "
            echo "        |        stop     ：  关闭服务      | "
            echo "        |        restart  ：  重启服务      | "
            echo "        |        status   ：  查看状态      | "
            echo "        +-----------------------------------+ "
        ;;
    esac
    printf "=========================================================================\n\n"
```

### 7.7 启动 HBase

```bash
    ${ZOOKEEPER_HOME}/bin/zookeeper.sh start                                   # 启动 zookeeper
    ${HADOOP_HOME}/bin/hadoop.sh start                                         # 启动 hadoop
    
    cd /opt/apache/hbase/ || exit                                              # 切换的 hbase 安装路径
    
    # 每个节点单独启动
    ${HBASE_HOME}/bin/hbase-daemon.sh start master                             # 主节点启动 HMaster
    ${HBASE_HOME}/bin/hbase-daemon.sh start regionserver                       # 每个 slaver 节点启动 HRegionServer
    
    # 启动集群的组件
    ${HBASE_HOME}/bin/hbase-daemons.sh start master                            # 启动集群的 HMaster
    ${HBASE_HOME}/bin/hbase-daemons.sh start regionserver                      # 启动集群所有的 HRegionServer
    
    # 启动集群所有组件
    ${HBASE_HOME}/bin/start-habse.sh                                           # 启动集群的 HMaster 和所有的 HRegionServer
    ${HBASE_HOME}/bin/hbase.sh start                                           # 启动集群的 HMaster 和所有的 HRegionServer，并检查启动状况
```

### 7.8 查看 HBase 启动结果并测试

```bash
    # 查看各节点状态
    jps -l 
    ${HBASE_HOME}/bin/hbase.sh status                                          # 在每个节点执行查看
    ${ISSAC_HOME}/shell/xcall.sh ${HBASE_HOME}/bin/hbase.sh status             # 在主节点 master 执行查看
    
    http://issac:16010/master-status                                           # HMaster 状态
    http://issac:16030/rs-status                                               # HRegionServer 状态
```


## 8. Phoenix 安装

### 8.1 Phoenix 下载

从 [**阿里云镜像网站**](https://mirrors.aliyun.com/apache/) 下载 **[phoenix-5.1.3](https://mirrors.aliyun.com/apache/phoenix/phoenix-5.1.3/phoenix-hbase-2.4-5.1.3-bin.tar.gz)** 安装包到本地


### 8.2 解压安装 phoenix

```bash
    tar -zxvf phoenix-hbase-2.4-5.1.3-bin.tar.gz -C /opt/apache/               # 解压 压缩包
    mv /opt/apache/phoenix-hbase-2.4-5.1.3-bin /opt/apache/phoenix             # 修改目录名称
    mkdir -p /opt/apache/phoenix/logs                                          # 创建 日志存储 路径
```

### 8.3 配置环境变量

```bash
    # 切换 root 账户，配置系统的环境变量
    su - root                                                                  # 切换到 root 账户，或者使用 sudo 
    
    # 使用 vim 编辑器修改系统配置文件
    vim /etc/profile                                                           # 或使用： sudo vim /etc/profile 
    
    # 添加如下内容：
        # ===================================== Phoenix 3.2.3 ====================================== #
        export PHOENIX_HOME=/opt/apache/phoenix
        export PATH=${PATH}:${PHOENIX_HOME}/bin
            
    # 使系统变量生效
    source /etc/profile                                                        # 或者使用： . /etc/profile 
```

### 8.4 配置文件修改

#### 8.4.1 复制驱动并修改配置

```bash
    cd /opt/apache/phoenix/ || exit                                            # 切换的 phoenix 安装路径
    cp ${PHOENIX_HOME}/phoenix-server-hbase-2.4-5.1.2.jar ${HBASE_HOME}/lib/   # 复制 驱动
    vim ${HBASE_HOME}/conf/hbase-site.xml                                      # hbase 的配置文件添加 8.4.2 内容
    cp ${HBASE_HOME}/conf/hbase-site.xml ${PHOENIX_HOME}/bin/                  # 复制 hbase 配置文件到 phoenix 
    cp ${HADOOP_HOME}/etc/hadoop/core-site.xml ${PHOENIX_HOME}/bin/            # 复制 hadoop 的 core 配置文件到 phoenix 
    cp ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml ${PHOENIX_HOME}/bin/            # 复制 hadoop 的 hdfs 配置文件到 phoenix    
    cp ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml ${PHOENIX_HOME}/bin/            # 复制 hadoop 的 hdfs 配置文件到 phoenix    
```

#### 8.4.2 修改 ${HBASE_HOME}/conf/hbase-site.xml   

````xml
    <!-- 如果使用了 hbase 中的自定义 namespace，不仅仅使用 default -->
    <!-- 那么在 phoenix 中与之对应的是 schema 的概念，但是默认并没有开启，需要在增加以下配置项 -->
    <property>
        <name>phoenix.schema.isNamespaceMappingEnabled</name>
        <value>true</value>
    </property>
    
    <property>
        <name>phoenix.schema.mapSystemTablesToNamespace</name>
        <value>true</value>
    </property>
    
    <!-- 用户可以创建临时或永久的用户自定义函数 -->
    <property>
        <name>phoenix.functions.allowUserDefinedFunctions</name>
        <value>true</value>
    </property>
````

### 8.5 分发到其它节点

```bash
    cd /opt/apache/                                                            # 切换到 phoenix 安装父路径
    ~/shell/xync.sh phoenix                                                    # 同步 phoenix 到其它节点
    ~/shell/xync.sh hbase/conf/hbase-site.xml                                  # 同步 hbase 配置到其它节点
````

### 8.6 编写 phoenix 启停脚本：${PHOENIX_HOME}/bin/phoenix.sh

```bashpro shell script
    
```

### 8.7 启动 HBase
```bash
    ${ZOOKEEPER_HOME}/bin/zookeeper.sh start                                   # 启动 zookeeper
    ${HADOOP_HOME}/bin/hadoop.sh start                                         # 启动 hadoop
    ${HBASE_HOME}/bin/hbase.sh start                                           # 启动 hbase
    cd /opt/apache/phoenix/ || exit                                            # 切换的 phoenix 安装路径
    ${PHOENIX_HOME}/bin/sqlline.py slaver1,slaver2,slaver3:2181                # 启动 sqlline 客户端
    nohup ${PHOENIX_HOME}/bin/queryserver.py >> ${PHOENIX_HOME}/logs/queryserver.log 2>&1 &   # 后台启动查询
    jps -l                                                                     # 查看 phoenix 和 客户端 状态
```

### 8.4 测试 phoenix

```mysql
    create schema if not exists test;                                          -- 创建 schema（数据库），就是 hbase 中的 namespace
    use test;                                                                  -- 切换到 test schema
    !tables                                                                    -- 查看所有表格
    
    create table if not exists student                                         -- 创建 test 表
    (
        id     int           primary key comment '主键 ID',
        name   varchar(64)               comment '姓名',
        age    int                       comment '年龄',
        gender int                       comment '性别：-1，未知；0，女；1：男'
    ) comment '学生测试表';

    !describe student;                                                         -- 查看表的结构
    upsert into student (id, name, age, gender) values(1001, '张三', 25, 0);   -- 添加 
    upsert into student (id, name, age, gender) values(1001, '张三', 36, 0);   -- 修改 
    select * from student;                                                     -- 查询    
    delete from student where id = 1001;                                       -- 删除
    
    select * from SYSTEM.CATALOG;                                               -- phoenix 中的表信息都存在 SYSTEM.CATALOG 表
    !exit                                                                       -- 退出 sqlline 客户端，或者：!quit
```



## 9. Flink 安装